1. [Equivariant Subgraph Aggregation Networks](https://www.aminer.cn/pub/615e657b5244ab9dcbf22024/equivariant-subgraph-aggregation-networks?conf=ICLR%202022):
   
   * 图神经网络模型 表达能力提升 突破消息传递理论限制 子图采样方式
   
   * 将图表示为采样的子图的集合，使用等变结构处理

2. [Graph Neural Networks with Learnable Structural and Positional Representations](https://www.aminer.cn/pub/616ce5a15244ab9dcbacfc10/graph-neural-networks-with-learnable-structural-and-positional-representations?conf=ICLR%202022)：
   
   * 图神经网络模型 表达能力提升
   
   * 分离结构和位置表征学习，为节点学习位置嵌入

3. [Discovering Invariant Rationales for Graph Neural Networks](https://www.aminer.cn/pub/61f8a4c35aee126c0fee0426/discovering-invariant-rationales-for-graph-neural-networks?conf=ICLR%202022)：
   
   * 图神经网络  可解释性
   
   * 解决可解释性模型由于依赖数据偏差导致的在训练数据外表现差的现象。通过在训练分布上加入不同扰动生成扰动后分布，然后通过不同扰动分布找到其中不变的因果关系来解释GNN
   
   * （原来自己想过，但是觉得没有道理就没跟，不过跟了也跟不出什么来）

4. [Graph Condensation for Graph Neural Networks](https://www.aminer.cn/pub/6168f1a35244ab9dcbe2ffa3/graph-condensation-for-graph-neural-networks?conf=ICLR%202022)：
   
   * 图神经网络  大规模GNN训练  图简化
   
   * 把大网络简化成一个小网络，在小网络上训练的GNN可以应用于大网络

5. [Topological Graph Neural Networks](https://www.aminer.cn/pub/602cdfaf91e011c3e8f669fa/topological-graph-neural-networks?conf=ICLR%202022):
   
   * 图神经网络  表达能力提升
   
   * 解决GNN无法识别环等图内常见结构的问题，加了一个用于引入全局结构信息的网络层，提升WL测试能力

6. [Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design](https://www.aminer.cn/pub/6164fcc15244ab9dcb24d012/iterative-refinement-graph-neural-network-for-antibody-sequence-structure-co-design?conf=ICLR%202022):
   
   * 图生成 图神经网络
   
   * 将cdr序列和3d结构共同设计为图，在逐渐改进预测的全局结构的同时自回归的解开一个残基序列，预测的全局结构改进序列的选择

7. [Automated Self-Supervised Learning for Graphs](https://www.aminer.cn/pub/60c567e691e011368ce8c0f2/automated-self-supervised-learning-for-graphs?conf=ICLR%202022):
   
   * 图神经网络 自动机器学习 自监督学习
   
   * 解决自监督学习下游任务表现对上游任务选择敏感的问题，提出一个自动机器学习框架，自动选择上游任务的组合

8. [Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction](https://www.aminer.cn/pub/6180ac435244ab9dcb793a8f/node-feature-extraction-by-self-supervised-multi-scale-neighborhood-prediction?conf=ICLR%202022):
   
   * 图神经网络 数据预处理（节点特征提取）超多标签分类
   
   * 解决提取特征时没有考虑图结构的问题，提出一个自监督框架，将邻接矩阵作为标签输入，对bert进行微调

9. [Query Embedding on Hyper-relational Knowledge Graphs](https://www.aminer.cn/pub/60cd36a491e011329faa202e/query-embedding-on-hyper-relational-knowledge-graphs?conf=ICLR%202022):
   
   * 知识图谱 多跳逻辑推理 超关系
   
   * 超关系：知识图谱的边上有键值对提供额外信息
   
   * 解决超关系上的修饰信息没有充分利用的问题，利用GNN和查询嵌入，将超关系链接查询嵌入并搜索答案

10. [Chemical-Reaction-Aware Molecule Representation Learning](https://www.aminer.cn/pub/614a9eca5244ab9dcbc38b04/chemical-reaction-aware-molecule-representation-learning?conf=ICLR%202022):
    
    * 图神经网络 分子表示学习
    
    * 解决分子表示学习方法中要么不关注结构信息，要么过于强调结构而忽视泛化的问题。提出使用化学反应提升学习效果，使反应物的嵌入的和与生成物的嵌入的和相同。

11. [Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation](https://www.aminer.cn/pub/616e37435244ab9dcbd1a8b1/graph-less-neural-networks-teaching-old-mlps-new-tricks-via-distillation?conf=ICLR%202022):
    
    * 图神经模型  可伸缩性  知识蒸馏
    
    * 解决大规模网络时邻域爆炸的问题，使用知识蒸馏结合GNN和MLP，解决图结构依赖问题

12. [Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks](https://www.aminer.cn/pub/619472d35244ab9dcbd2dc0b/learn-locally-correct-globally-a-distributed-algorithm-for-training-graph-neural-networks?conf=ICLR%202022):
    
    * 图神经网络 训练方法 分布式（联邦？）学习
    
    * 解决图神经网络训练时依赖大量通信消耗和内存消耗问题，首先在本地训练一个GNN，然后将模型送到服务器上做周期平均。在服务器上做全局矫正对模型进行矫正

13. [Graph-Guided Network for Irregularly Sampled Multivariate Time Series](https://www.aminer.cn/pub/6164fcc75244ab9dcb24d947/graph-guided-network-for-irregularly-sampled-multivariate-time-series?conf=ICLR%202022):
    
    * 时间序列数据 数据矫正 时间序列分类
    
    * 解决时间序列数据由于传感器原因不连续和不规律等 噪声问题。使用基于消息传递机制和注意力机制的图方法对时间序列进行分类（识别提供序列的传感器间的关系），并找出误指定的序列

14. [Neural Link Prediction with Walk Pooling](https://www.aminer.cn/pub/6164fcc15244ab9dcb24ce9c/neural-link-prediction-with-walk-pooling?conf=ICLR%202022):
    
    * 图神经网络 链接预测
    
    * 解决链接预测模型使用图池化方式无法利用环和motif等结构信息的问题，提出了一种新的池化机制

15. [IGLU: Efficient GCN Training via Lazy Updates](https://www.aminer.cn/pub/61552aed5244ab9dcb23eaae/iglu-efficient-gcn-training-via-lazy-updates?conf=ICLR%202022):
    
    * 图神经网络 训练方法
    
    * 解决基于随机梯度下降的方法因为下降步骤要更新大部分节点导致的性能不佳，次级采样方法会导致偏差梯度的问题。方法捕捉前向传递的嵌入，并进行懒惰更新。

16. [Crystal Diffusion Variational Autoencoder for Periodic Material Generation](https://www.aminer.cn/pub/61664e625244ab9dcb455908/crystal-diffusion-variational-autoencoder-for-periodic-material-generation?conf=ICLR%202022):
    
    * 图生成 图神经网络 材料生成
    
    * 生成具有周期性结构的稳定材料

17. [From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness](https://www.aminer.cn/pub/6163ab255244ab9dcbf95d72/from-stars-to-subgraphs-uplifting-any-gnn-with-local-structure-awareness?conf=ICLR%202022):
    
    * 图神经网络  表达能力
    
    * 新图神经网络模型，解决传统图神经网络模型表达能力不超过1-WL测试，超过的可伸缩性和泛化能力受限的问题。使用一个GNN以子图为单位学习节点的嵌入，远离类似将卷积核替换为GNN的CNN

18. [Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods](https://www.aminer.cn/pub/618b38575244ab9dcb710ce2/cold-brew-distilling-graph-node-representations-with-incomplete-or-missing-neighborhoods?conf=ICLR%202022):
    
    * 图神经网络  严格冷启动场景
    
    * 解决孤立节点和结构强噪声节点的嵌入和分类问题。使用特征分布比例衡量GNN来解决严格冷启动问题的可能性，并选择最佳的模型

19. [Understanding over-squashing and bottlenecks on graphs via curvature](https://www.aminer.cn/pub/61a596635244ab9dcbdfe47f/understanding-over-squashing-and-bottlenecks-on-graphs-via-curvature?conf=ICLR%202022):
    
    * 图神经网络  过拥挤问题
    
    * 过拥挤：相邻节点的失真信息会导致依赖长距离交互的任务效果下降
    
    * 提出了基于边的组合弯曲方法，证明负向弯曲会造成过拥挤问题

20. [Predicting Physics in Mesh-reduced Space with Temporal Attention](https://www.aminer.cn/pub/61ef6ad35244ab9dcb67db22/predicting-physics-in-mesh-reduced-space-with-temporal-attention?conf=ICLR%202022):
    
    * 图神经网络 下一步预测 动态图？
    
    * 解决下一步预测模型短期时间注意力机制的限制导致的首误差累积和传播影响的问题。使用编码解码结构学习简单的相互协调的表示，用类transformer的注意力机制捕捉长时间依赖

21. [Graph Neural Network Guided Local Search for the Traveling Salesperson Problem](https://www.aminer.cn/pub/6164fcc75244ab9dcb24d8e8/graph-neural-network-guided-local-search-for-the-traveling-salesperson-problem?conf=ICLR%202022):
    
    * 图神经网络 旅行商问题
    
    * 解决已有方法在大型旅行商问题上效率和效果无法兼顾的问题
    
    * 将问题转化为答案中保留图里哪些边的问题，并预测每条边的后悔值

22. [Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery](https://www.aminer.cn/pub/60bec3c291e0118491817773/spatial-graph-attention-and-curiosity-driven-policy-for-antiviral-drug-discovery?conf=ICLR%202022):
    
    * 图生成 强化学习 空间注意力机制
    
    * 通过探索物理受限区域生成可以优化用户指定的对象的图结构的化学表征
    
    * 提出强化学习框架，一个空间图注意力网络，编码节点河边的属性以及图空间结构信息

23. [Convergent Graph Solvers](https://www.aminer.cn/pub/60bafaa291e01102e59b6ce1/convergent-graph-solvers?conf=ICLR%202022):
    
    * 图系统？图生成？图分类？
    
    * 预测图系统在稳定状态时的特性，没太搞懂问题是什么

24. [Understanding and Improving Graph Injection Attack by Promoting Unnoticeability](https://www.aminer.cn/pub/620dbcfa5aee126c0f5db47f/understanding-and-improving-graph-injection-attack-by-promoting-unnoticeability?conf=ICLR%202022):
    
    * 图攻击 图注射攻击
    
    * 图注射攻击：加几个点，图修改攻击：修改已有节点
    
    * 解决图注射攻击对同质性分布的破坏导致的容易被基于同质性的保护方法防御的问题。提出了同质性不易察觉度指标，并设计了协调攻击机制实现。

25. [Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions](https://www.aminer.cn/pub/6164fcc15244ab9dcb24d265/ab-initio-potential-energy-surfaces-by-pairing-gnns-with-neural-wave-functions?conf=ICLR%202022):
    
    * 薛定谔方程 图神经网络
    
    * 解决已有方法只能处理单电子系统问题。使用GNN结合神经波函数通过变分蒙特卡洛框架同时解多几何薛定谔方程。

26. [Equivariant Graph Mechanics Networks with Constraints](https://www.aminer.cn/pub/623004305aee126c0f9b358b/equivariant-graph-mechanics-networks-with-constraints?conf=ICLR%202022):
    
    * 多目标运动推理 图神经网络
    
    * 利用图神经网络生成目标物的运动学信息（位置，速度），然后预测符合几何约束的运动方式

27. [Graph Auto-Encoder Via Neighborhood Wasserstein Reconstruction](https://www.aminer.cn/pub/6213029a5aee126c0f42a3c4/graph-auto-encoder-via-neighborhood-wasserstein-reconstruction?conf=ICLR%202022):
    
    * 图自编码器 解码器
    
    * 解决解码器只重构直接相连的边导致只适合考虑直接相连关系的下游任务的问题。设计新解码器，重构节点度数和邻居特征分布

28. [Why Propagate Alone? Parallel Use of Labels and Features on Graphs](https://www.aminer.cn/pub/6168f19c5244ab9dcbe2f9db/why-propagate-alone-parallel-use-of-labels-and-features-on-graphs?conf=ICLR%202022):
    
    * 图神经网络 标签传播
    
    * 将一部分标签作为GNN输入，和节点特征拼接后一起训练预测剩余节点的标签。分析标签技巧给训练过程带来的特性。解决潜在的标签泄露问题和适应图的规模和连接性问题。

29. [Large-Scale Representation Learning on Graphs via Bootstrapping](https://www.aminer.cn/pub/619e189e6750f82b1e8c7102/large-scale-representation-learning-on-graphs-via-bootstrapping?conf=ICLR%202022):
    
    * 图神经网络 大规模训练 负采样 集成学习
    
    * 通过预测其他输入增强方式进行学习？

30. [Graph-Relational Domain Adaptation](https://www.aminer.cn/pub/620330d55aee126c0f0c1f9a/graph-relational-domain-adaptation?conf=ICLR%202022):
    
    * 域适应 迁移学习
    
    * 解决域适应方法将所有域同等对待导致的忽略域间结构信息的问题，构建域图来编码域间的相邻关系

31. [GNN is a Counter? Revisiting GNN for Question Answering](https://www.aminer.cn/pub/615fb6ef5244ab9dcb9c3c26/gnn-is-a-counter-revisiting-gnn-for-question-answering?conf=ICLR%202022):
    
    * 

32. [GNN-LM: Language Modeling based on Global Contexts via GNN](https://www.aminer.cn/pub/616e37435244ab9dcbd1a8c6/gnn-lm-language-modeling-based-on-global-contexts-via-gnn?conf=ICLR%202022):
    
    * 

33. [How Attentive are Graph Attention Networks](https://www.aminer.cn/pub/60b6e7b191e011903fc2b99a/how-attentive-are-graph-attention-networks?conf=ICLR%202022):
    
    * 

34. 
