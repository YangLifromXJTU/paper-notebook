# UNENIX Security 2023

## PROGRAPHER: an anomaly detection system based on Provenance Graph Embedding

- 研究问题：对抗高级持续攻击（APT）的数据溯源（data provenance）技术可以捕获计算机系统或网络实体间的复杂关系，然后利用这些信息检测精确的APT攻击。但目前的方法在效率，精确度和粒度上没有做到很好的平衡。基于溯源图的方法会存在“依赖爆炸”的现象。
- 相关工作：基于签名，启发式或则已知攻击模式的溯源系统可以被绕过；使用单一溯源图的系统难以处理大量的日志和警告数据；基于时序快照的系统识别粒度过于粗糙，需要人工分析异常快照中的所有实体和交互。
- 解决方案：提出了一种溯源图上的新异常检测方法PROGRAPHER，在日志上提取时序快照并在其上构建溯源图进行检测。使用图嵌入学习整图嵌入,然后利用序列学习方法来获取图上的结构信息，最后从异常的快照上提取指标并汇报给分析员。
- 实验效果：在五个真实世界数据集上进行实验，结果证明可以高精确度检测标准攻击和APT攻击，超越现有最好的检测系统

## PrivateFL: accurate, differentially private federated learning via personalized data transformation

- 研究问题：联邦学习支持多个client在一个中心服务器的组织下联合训练模型，攻击者（不可信的服务器）可以通过多种推理攻击方式危害client本地训练数据的隐私性。差分隐私方法用于解决此问题，但会牺牲模型的精确程度。
- 相关工作：通过调整数据规模和激活函数的方法无法处理client引入的不均匀性；控制数据质量的方法处理训练数据，但不处理差分隐私噪声
- 解决方案：首先提出精确度衰减的部分原因是差分隐私在加入随机噪声时为client引入了额外的不均匀性。随后设计了PrivateFL来学习去除不均匀性的精确的差分隐私模型。主要思路是为每个client同时训练一个差分隐私个性化的数据转换器，用于移动数据分布来补偿引入的不均匀性，从而提升精确度。
- 实验效果：与8个sota方法在7个数据集上进行比较，结果说明PrivateFL可以学到较精确模型

## No more Reviewer 2: subverting automatic paper-reviewer assignment using adversarial learning

- 研究问题：论文审核分配系统，根据统计学的语言模型容易被手动操纵的问题。
- 相关工作：以往工作主要关注控制审核与评委碰撞问题，当前工作主要关注对抗学习方法
- 解决方案：提出一种攻击方式，通过修改给定论文来误导分配系统，从而选择它自己指定的审稿人。攻击基于一种新的优化策略，通过交替特征空间和问题空间来向论文添加不明显的变更。
- 实验效果：在真实的会议场景中进行分配模拟，结果说明可以在不接触分配系统的前提下选择和移除审稿人。修改的论文保持了合理性，并且难以人为分辨。

## NeuroPots: realtime proactive defense against bit-flip attacks in neural networks

- 研究问题：位翻转攻击的防御问题，位翻转攻击通过将量化模型参数进行少量位翻转来降低模型精确度或者误分类特定输入，但因为可以发生在任何权重参数上，检测面过大，所以难以检测到。
- 相关工作：已有工作主要试图弥补DNN模型的脆弱性来抵抗位翻转攻击。
- 解决方案：提出了一种新的防御概念NeuroPots，通过将部分人工设计的脆弱神经引入DNN模型的方式来引诱攻击者进行攻击，从而缩小检测面，实现高效检测和模型恢复。设计了一种脆弱神经选择策略，并提出了两种将陷阱门嵌入DNN模型的方法，设计了一种基于校验和的方法来有效检测位翻转攻击，然后通过刷新陷阱门的方法来挽救模型精确度。
- 实验效果：实验证明方法可以有效检测攻击并低成本恢复被攻击模型。

## The space of adversarial Strategies

- 研究问题：对于对抗学习的理解问题，目前的对抗学习方法在威胁模型的假设和优化的定义上都各不相同。
- 相关工作：没有提到对抗的理论性工作，本文探索是否存在最优攻击方式
- 解决方案：提出了系统方法来描述最差情况的对抗学习。通过将攻击组成原子化为surface和travelers，对对抗机器学习的攻击进行可扩展的分解。在此基础上枚举组成部分创建了576种攻击方式，最后提出了帕累托组合攻击，一种理论上表现最优的攻击方式。
- 实验效果：在多个数据集和不同的模型构成的对抗策略空间（根据给定模型和领域时的攻击集合）上进行测试，实验结果表明攻击表现与环境信息（领域，模型稳定性）高度相关，攻击模型对攻击效率有较大影响。

## Learning Normality is Enough: a software-based mitigation against inaudible voice attacks

- 研究问题：针对语音助手的无声攻击，使用听不到的声音将有害信息打入目标的声控设备，存在攻击模式多样，在不同设备上不同的问题。
- 相关工作：基于硬件的方法可以修复硬件问题，但需要定制周期较长。基于软件的方法通常是基于攻击信号的声学特性的有监督学习方法，需要受攻击设备且费时。不同设备上的声学特性不同，需要方法轻量化可迁移。
- 解决方案：根据异常检测设计无监督学习方法。根据良性声音模式相似的特点，通过学习良性模式来检测恶性声音（异常）。NormDetect将频谱特征映射到低维空间，进行相似度查血并将它们替换为标准特征嵌入以进行频谱重建，从而得到良性声音和攻击之间明显的重建误差。
- 实验效果：在24个设备上的383320个测试样本上的结果显示有平均99.48%的AUC和2.23%的EER，说明了方法的有效性。

## Squint Hard Enough: attacking perceptual hashing with adversarial machine learning

- 研究问题：对实时内容扫描任务的感知哈希技术的对抗攻击问题。实时内容扫描用于识别违法内容，感知哈希技术根据文件生成摘要来与数据库中的违法内容进行模糊比对。将感知哈希匹配纳入客户端和端到端加密系统中的方式对感知哈希技术的对抗鲁棒性提出了更高要求（服务器端完全无法查看原内容）
- 相关工作：对感知哈希技术的研究很少从隐私和安全角度出发，服务器端的感知哈希仅在严格的机密性协议下有效。
- 解决方案：设计了对抗设定下针对哈希感知算法的威胁模型，并对两种算法进行了攻击。进一步按久了满足检测回避攻击的图像生成方法。
- 实验效果：可以有效生成目标的二次前向攻击，现有的感知哈希方法在检测回避攻击下并不鲁棒。

## PrivTrace: differentially private trajectory synthesis by adaptive markov models

- 研究问题：轨迹信息的隐私问题。
- 相关工作：差分隐私方法主要研究为具体场景设计算法而不是设计满足差分隐私的数据集，数据生成方法要么只用一阶马尔可夫链模型，难以保留足够的转换信息，要么用高阶马可夫链模型，引入额外的噪音。
- 解决方案：结合差分隐私和马可夫链模型来生成虚假轨迹，提出了一个框架来分析马可夫链模型在轨迹生成问题上的作用，在此基础上提出了PrivTrace算法，自适应地利用一阶和二阶马可夫模型
- 实验效果：在人工和真实数据集上测试了方法的效果

## UnGANable: defending against GAN-based face manipulation

- 研究问题：基于生成对抗网络的面部操纵攻击的防御问题。GAN逆映射将目标图像映射到模型隐空间，以便生成指定方向的图像。
- 相关工作：检测方法只能被动检测面部图片是否被修改，扰乱方法模仿骨干网络，只对基于图片转换的操纵方式有效
- 解决方案：针对GAN逆映射的防御问题提出UnGANable防御系统，通过搜索目标图像附近的其他图像（遮罩图）来在发布后削弱GAN的逆映射过程。
- 实验效果：为两种sota逆映射技术设计了五种不同的防御措施，在两个基准数据集上的实验说明了有效性，并进一步探索了可能的绕过UnGANable的对抗方法。

## Improving real-world password guessing attacks via bi-directional transformers

- 研究问题：密码猜测攻击问题。密码猜测攻击可以概念化为估计文本token的概率分布。双向transformer因为可以利用两个方向的文本来捕捉文本的细微差别而表现突出。
- 相关工作：已有方法简单将transformer引入猜测攻击，并没有超过其他对比方法，使用trasformer的方式并不直观，并且需要针对任务进行设计。
- 解决方案：提出了一种基于双向transformer的猜测框架PassBERT，在密码猜测攻击中使用预训练范式。首先准备一个预训练的密码模型，然后为三种攻击场景（有条件密码猜测，目标密码猜测，基于自适应规则的密码猜测）设计特定的微调方法。
- 实验效果：微调模型可以在三种攻击场景下超过有超过SOTA 14.53%，21.82%和4.86%的表现。，最后提出了一种混合密码强度指标来缓和三种攻击效果。

## GAP: differentially private graph neural networks with aggregation perturbation

- 研究问题：学习具备差分隐私能力的GNN模型问题，目标是保护敏感的图结构信息和其他相关数据。
- 相关工作：已有方法无法同时实现节点级和边级的隐私保护，或需要公开数据支持，以及无法应用于复杂图神经网络。
- 解决方案：设计了一种新的基于聚合扰动的差分隐私GNN模型，为GNN的聚合函数添加随机噪声，从而混淆一条边的存在（边级隐私）或者一个点以及所有相连的边（点级隐私）。模型首先用编码器模块学习节点嵌入，然后使用聚合模块得到含噪声的节点聚合嵌入，最后训练分类器完成节点分类任务。GAP可以利用多跳信息，并且使用训练时的隐私成本就可以在训练和推理过程中都实现差分隐私
- 实验效果：在三个数据集上的实现表明GAP相较其他DP-GNN方法有更好的精确度和效率的平衡。

## FreeEagle: detecting complex neural trojans in data-free cases

- 研究问题：复杂网络上的后门攻击问题。
- 相关工作：大多数现有方法都假设防御着可以获得一部分干净的验证样本或者有触发词的样本,在某些真实场景中并不成立
- 解决方案：提出了第一个不用数据的后门检测攻击FreeEagele，可以在不接触任何样本的前提下从DNN中检测复杂后门攻击
- 实验效果：实验证明可以有效抵御多种后门攻击，甚至超过某些SOTA的需要数据的后门检测方法。

# Every Vote Counts: ranking-based training of federated learning to resist poisoning attacks

- 研究问题：联邦学习的无目标中毒攻击问题
- 相关工作：已有方法主要通过限制可接受更新的范围来防御中毒攻击，只考虑安全范围内的梯度更新从而限制对抗样本的选择。
- 解决方案：提出问题关键时client端的可选择的更新空间过大，因此提出了FRL框架，通过将更新空间从模型参数更新限制到模型参数级别的方式来限制选择空间。FRL clients根据本地训练数据对神经网络参数进行评分，server使用投票机制来聚合评分进行更新。
- 实验效果：实验证明了FRL的稳定性以及沟通高效性。

## DiffSmooth: certifiably robust learning via diffusion models and local smoothing

- 研究问题：使用diffusion model在随机防御中获得更好更高效的光滑模型，提升光滑模型可验证鲁棒性的问题。包括利用扩散模型的对抗春华来提高模型在面对大的扰动半径时的可验证鲁棒性，以及通过增加预测步骤来高效提升可验证鲁棒性。
- 相关工作：
- 解决方案：理论证明从对抗中恢复的实例将以高概率位于相应原始实例的有界邻域内，去噪扩散概率模型（DDPM）的一次性去噪可以逼近连续时间扩散模型生成的后验分布的均值，从而在温和条件下逼近原始实例。在理论指导下设计了通用的可验证鲁棒对抗精华管道DiffSmooth，首先为输入加入随机高斯噪声方便验证，然后通过预训练扩散模型的逆步骤生成输入的去噪净化样本，最后为净化样本加入另一组噪声来生成局部光滑实例，并在平均置信度基础上进行预测。为所有输入重复以上步骤并选用主要投票作为最终的光滑预测。
- 实验效果：在不同数据集上验证了DiffSmooth相对于SOTA更高的可验证稳定性

## AutoFR: automated filter rule generation for adblocking

- 研究问题：广告阻挡的过滤列表生成问题。
- 相关工作：已有基于ML的工作需要人工反馈或现有过滤列表的监督来训练，且并不考虑屏蔽广告语避免破损之间的权衡。
- 解决方案：提出了完全自动话过滤规则生成和评估的强化学习框架AutoFR。设计了基于多臂老虎机的方法来生成过滤归责并控制过滤广告与避免视觉破损（visual breakage）之间的权衡。
- 实验效果：在上千个网站上进行了测试，可以生成屏蔽86%广告的规律规则，相较于屏蔽了87%的EasyList有更好的视觉连贯性。生成的规则可以有效泛化到新网站。

## An input-agnostic hierarchical deep learning framework for traffic fingerprinting

- 研究问题：基于深度学习的流量指纹识别应用
- 相关工作：已有方法面临流量个性化和层级不敏感两个问题，需要手动为每个任务设计方法将流量特征转换为神经网络的固定维度输入，同时无法捕捉不同流量包之间的层级关系和相关性。
- 解决方案：设计了一种输入无关的层次化深度学习框架，可以层次化的将全面的易购流量特征抽象为可消化的同构响亮，以支持现有的神经网络进行进一步分类。
- 实验效果：实验表明该框架仅需一个范例，支持易购流量输入，在流量识别任务中与SOTA相比表现了相当的性能。

## AIRS: explanation for deep reinforcement learning based security applications

- 研究问题：安全应用中的深度强化学习模型可解释性问题。
- 相关工作：已有的深度强化学习解释方法通常关注寻找重要特征，不适用于安全应用，在解释的精确性，有效性和模型调试能力上有所欠缺
- 解决方案：提出了一种解释基于深度强化学习的安全应用的通用框架AIRS。框架通过建模最终输出和DRL agent采用的关键步骤之间的关系来找到对于最终输出最重要的步骤。
- 实验效果：对AIRS的解释能力，精确性，稳定性和有效性进行测试，结果超过其他可解释方法，同时证明方法可以促进DRL模型抵消错误，帮助用于建立对模型决策的信任。

## Aegis: mitigating targeted bit-flip attacks against deep neural networks

- 研究问题：DNN上的位翻转攻击防御问题
- 相关工作：现有关注无目标场景的防御方法或者需要额外的可信应用，或者让模型在面对有目标攻击时更加脆弱。针对有目标攻击的防御方法需要更加隐蔽也更有意义，还有所欠缺。
- 解决方案：提出了一种新的缓和有目标BFA攻击的防御方法Aegis。根据已有攻击关注在特定关键层上进行翻转的观察结果，设计了一种动态退出机制来为隐层增加额外的内部分类器。此机制允许输入样本从不同层提前离开，有效打乱对抗攻击计划。同时通过在推理时随机选择分类起来增加自适应攻击的成本。进一步提出了一种鲁棒训练机制来在分类器训练过程中通过模拟BFA让分类器适应不同的攻击场景。
- 实验效果：在四个数据集和两个DNN结构上测试表明Agis可以有效缓解不同的SOTA有目标攻击，可以缓解5-10倍的成功率。

## Adversarial Training for Row-Binary Malware Classifiers

- 研究问题：可执行文件危害检测的机器学习模型的对抗训练问题。
- 相关工作：已有方法通过标准化可执行文件、随机遮罩一部分字节来进行识别，但标准化需要知道所有的IPR转换且计算成本巨大，随机遮罩会同时导致更多的误分类。
- 解决方案：首先大幅度提升了对抗样本生成的效率和规模，然后分析了不同的训练长度和针对不同攻击类型的效果。
- 实验效果：实验发现数据增强不会抵御SOTA攻击，但使用其他领域普通的梯度指导的方法会提升鲁棒性。在大多数情况下可以通过用同类型但低投入版本的攻击对抗训练来提升鲁棒性。通过使用某些类型的攻击可以提升针对其他类型攻击的鲁棒性。

## A Plot is Worth a Thousand Words: model information stealing attacks via scientific plots

- 研究问题：针对模型窃取攻击的侧信道攻击方法。
- 相关工作：
- 解决方案：发现了一种新的侧信道攻击方法，利用科学画图进行攻击。使用浅层模型训练技术生成训练数据来攻击图片分类任务的目标模型。
- 实验效果：实验结果证明可以有效推理出基于CNN的模型结构或超参数。发现攻击成功主要是由于科学图片的形状，说明攻击在多种场景下鲁棒，最后提出了多种防御机制来降低精度。

## A Data-free Backdoor Injection Approach in Neural Networks

- 研究问题：DNN模型上的后门攻击问题
- 相关工作：现有方法基本都需要解除原始的训练测试数据或者与主要任务相关的数据，以便植入后门
- 解决方案：提出了一种新的无需数据的后门植入攻击，使用与主要任务无关的数据，设计新的损失函数来用这些数据微调模型，并优化后门植入与在煮任务表现上的平衡。
- 实验效果：在多个深度学习场景和模型上进行了实验，结果证明了方法可以接近100%的成功率植入后门，并在下游主要任务上保持可接受的表现。

## PCAT: functionality and data stealing from split learning by pseudo-client attack

- 研究问题：联邦学习的半诚实服务器上的数据和模型窃取攻击问题
- 相关工作：已有攻击手段往往需要知道关于client的网络结构，并且在模型结构加深时效果会明显减弱。没有考虑客户端模型对服务器透明且更加复杂和深度的情况
- 解决方案：从服务端模型角度探索内部隐私泄露问题，发现客户模型和数据信息可以从服务端获得，并且一系列中间服务器模型可以导致更多的泄露。基于以上观察提出第一个从半诚实服务器端窃取client模型权重，重建输入并推理标签的攻击手段Pseudo-Client ATtack（PCAT），攻击手段对客户端透明，服务端可以在客户端无感情况下获取客户端数据。
- 实验效果：在包括更加复杂的模型和学习任务，甚至不满足iid条件的任务等场景上明显超出SOTA攻击方法，并且能有效适应目前的防御机制。

## Gradient Obfuscation Gives a False Sense of Security in Federated Learning

- 研究问题：联邦学习中的客户端隐私保护问题，针对数据重建攻击。
- 相关工作：已有方法（梯度量化，梯度稀疏，梯度扰动）可能会对联邦学习的安全造成错误理解。
- 解决方案：提出了一种针对联邦学习的图像分类任务的新重建攻击框架，不将隐私强化作为梯度压缩的副产品。在框架下设计了一种新方法来在语义层面重建图像。
- 实验效果：将语义隐私泄露和传统图像相似度分数做比较，说明了重新设计隐私保护机制的重要性。

## xNIDS: explaining deep learning-based network intrusion detection systems for active intrusion responses

- 研究问题：基于深度学习的网络入侵检测系统的可解释性问题。
- 相关工作：已有工作的检测结果与可执行操作之间有差距，无法主动对检测到的入侵采取行动。高错误成本也导致不能完全根据检测结果相应。根本问题是已有系统的可解释性不足。已有的可解释性方法无法处理历史输入和结构数据复杂的特征相关性，在检测系统上表现不佳。
- 解决方案：提出了通过解释入侵检测系统来促进入侵相应的新框架XNIDS，框架通过近似和采样历史输入以及捕获结构数据的特征相关性来实现高精度解释。随后基于解释结果生成可执行的防御规则。
- 实验效果：在四个SOTA检测系统上进行试验，结果说明XNIDS在精确度，稀疏性，完整性和稳定性上都超过现有方法，并且可以有效生成防御归责。
