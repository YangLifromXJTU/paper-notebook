# S&P2022 大模型安全相关

## WTAGRAPH: web tracking and advertising detection using GNN

+ 目标问题：网络流量追踪和广告检测问题(WTA)，针对原有的基于黑白名单或机器学习方法的屏蔽方法有限的精确度和效率问题(误分类和没有识别到的WTA请求)
+ 解决方案：提出了一种基于GNN的网络流量追踪和广告检测框架。首先构建表示HTTP网络流量的属性同构多图，将检测问题构建成图上的边表示学习和分类问题。收集http网络流量，DOM和javascript数据构建多图，使用直推式学习训练模型，并用归纳式进行WTA检测
+ 实验效果：使用Alexa Top 10K网站搜集的数据进行实验，手动验证可以发现原来方法发现不了和误分类的WTA请求。可以在更灵活的选项下实现可竞争的的效果。

## Model Stealing Attacks against inductive GNN

+ 目标问题：填补没有针对GNN的模型提取攻击的空白。模型窃取攻击目的是根据与目标模型的交互逆向还原出一个替代模型。
+ 解决方案：提出第一个针对归纳式GNN的模型窃取攻击方法，系统性的定义了威胁模型，并提出了六种基于窃取者的背景知识和目标模型反馈的攻击方法
+ 实验效果：在六个基准数据集上显示提出的攻击方法有乐观的效果

## DeepSteal: advanced Model Extractions leveraging efficient weight stealing in memories

+ 目标问题：模型提取攻击，逆向还原替代模型，从DNN模型中窃取敏感信息
+ 解决方案：首次提出使用内存侧信道攻击的模型提取框架，用于提取DNN模型权重参数。首先使用基于rowhammer的攻击技术来提取权重位的信息，然后提出一种新的使用平均聚类权重惩罚的子模型训练方法来获取部分泄露的位信息并生成目标模型的子模型原型。
+ 实验效果：在三个图像数据集和四个DNN架构上进行试验，提取的子模型可以获得超过90%的测试准确率，还可以有效生成足以欺骗目标模型的攻击输入样例。

## Reconstructing Training Data with Informed Adversaries

+ 目标问题：训练数据重建问题，在知道除了一个之外的训练数据时，根据模型交互重建剩余的这个未知的数据
+ 解决方案：通过实例化攻击，说明可以在给定模型下重建剩余训练数据。针对通用模型，提出了一种基于训练以目标模型权重位输入，训练数据为输出的重建网络的攻击策略。
+ 实验效果：在MNIST和CIFAR-10上验证了攻击策略的有效性，系统分析了影响重建的机器学习训练环节，并理论分析了足以缓解重建攻击的差分隐私程度。在接触到模型存储时可以有效进行重建攻击，标准的模型已经包含足够的信息来进行高精确度的重建，在utility degradation较小的参数域进行隐私差分就可以有效缓解此类攻击

## Phishing in Organizations: findings from a large-scale and long-term study

+ 目标问题：网络诈骗的检测和防止问题，社会工程学攻击
+ 解决方案：与第三方公司合作，向员工发送不同的模拟诈骗邮件，并允许员工上报收到的可疑邮件
+ 实验效果：第一，已有的部分方法可以有效防止网络诈骗，比如邮件中的提醒信息；第二，与常识不同，网络诈骗提醒教育并不会避免员工遭到诈骗，反而更有可能遭到诈骗；第三，首次发现将员工纳入诈骗检测机制在大型组织中是可行的，可以更快地发现新型诈骗，组织的运转投入可以接受，员工可以在更长时间内保持活跃。

## Copy, Right? a testing framework for copyright protection of Deep Learning Models

+ 目标问题：未授权的模型复制操作导致的模型泄露问题。针对目前基于水印的方法存在的扩散性问题（水印会跟随训练过程变更，会影响模型效果或引入新的安全问题）、易受水印移除替换攻击和水印溯源屏蔽攻击的影响、易受模型提取攻击影响等问题。基于深度学习的水印技术不易扩散，但还是易受攻击。
+ 解决方案：提出了一种测试框架，同时测试目标模型和可疑模型，使用多种测试标准和测试数据生成方法来提供判断是否为复制模型的证据链。模型就不用加水印了，可以直接进行测试比较。
+ 实验效果：在模型微调，修剪和提取等三个复制权违反的场景下进行测试。提出的测试框架不易扩散，高效，灵活并且在面对模型提取和模型自适应攻击时表现稳定。

## SoK: how robust is image classification deep neural network watermarking

+ 目标问题：DNN水印用于对DNN模型进行溯源，水印应当对水印移除攻击（生成替代模型来逃避水印溯源）鲁棒。目前的方法都尽在一个小的攻击集合内验证稳定性，还没有进行过系统全面的针对水印移除攻击的测试，导致在实际应用中无法信任部署。
+ 解决方案：从已知的移除攻击/生成替代模型但没有被作为移除攻击测试/新的移除攻击的文章中搜集水印方法，在权重漂移和平滑重训练两个攻击方式上进行测试。
+ 实验效果：在图像识别数据集上对不同参数进行了验证，发现没有水印方法可以抵御所有的移除攻击，没有攻击移除所有的水印；攻击可以叠加，并且找到了可以移除所有水印的组合攻击。水印算法需要在一个更加真实的攻击模型上，使用更全面的移除攻击场景进行测试。

## Spining Language Models: risks of propaganda-as-a-service and countermeasures

+ 目标问题：发现了一种新的seq2seq模型的模型旋转威胁：可以导致模型修改输出倾向来在触发某些特定词支持选定的语义或观点的训练时间攻击。模型旋转将元后门引入模型，在保持上下文和标准精确度指标表现的前提下支持特定的元任务。模型旋转支持宣传即服务（此处宣传定义为有偏向的言论）。攻击者可以使用模型旋转定制模型来产生虚假信息或者将模型放入机器学习管道来污染下游任务。
+ 解决方案：提出新的后门攻击技术，将元任务安插至seq2seq模型中，将需要的元任务输出反向传播至词嵌入空间作为点（“伪词”），然后使用伪词来偏移模型的整体输出分布。随后提出了一种黑盒且元任务独立的防御措施，给定候选触发词列表后可以检测出使用这些词进行了旋转的模型。
+ 实验效果：在语言生成，总结和翻译三种模型上使用不同出发词和元任务测试了这种攻击的效果。结果证明模型旋转可以在保持精确度指标表现的情况下满足元任务要求，并且在供应链攻击场景下旋转的效果可以传播至下游任务。

## BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning

+ 目标问题：自监督学习模型上的后门攻击问题，让使用自监督学习编码器搭建的下游分类器继承后门的行为。
+ 解决方案：提出了第一个针对自监督学习的后门攻击方法BadEncoder，将后门攻击描述为优化问题，并使用梯度递减方法解决，利用干净的编码器生成受污染的编码器。
+ 实验效果：BadEncoder获得了很高的攻击成功率，同时保留了下游分类器的精确度。

## Piccolo: exposing complex backdoors in NLP transformer models

+ 目标问题：现有的后门检测方法在面对较复杂的触发词和模型时效果不佳的问题。后门检测问题由于管道不连续和搜索空间大等NLP任务的特点而难以解决。
+ 解决方案：提出了一种新的后门扫描技术，将模型转换为等价但可微的形式，然后使用优化方法来逆推出表示触发词相关性的词分布。方法使用新的词判别性分析来判断模型是否可以分辨类似的触发词的存在。
+ 实验效果：使用大量NLP模型，复杂网络结构和不同的攻击类型进行测试，实验结果证明了方法的有效性。

## LINKTELLER: recovering private edges from GNN via influence analysis

+ 目标问题：联合训练下的边隐私问题，在向远端发送节点特征信息进行训练后，利用远端的API预测测试节点间的边信息场景。
+ 解决方案：提出隐私攻击方法LINKTELLER，通过为本地设计查询语句，使用影响分析发现远端持有的边信息.
+ 实验效果：在生成式和直推式数据集上LINKTELLER都可以发现大量隐私边。使用不同的差分隐私图卷积网络测试后发现LINKTELLER依然可以发现隐私边。

## Bad Characters: imperceptible NLP Attacks

+ 目标问题：针对基于文本的对抗样本攻击存在的保留语义信息和不可分辨性能力欠缺问题。
+ 解决方案：探索了大量的可用于黑盒攻击文本模型的对抗样本。使用人肉眼无法分辨的针对编码的置换来操纵大量的NLP系统的输出。
+ 实验效果：使用单一的不可分辨的编码注入（不可见字符的表示，同形异意，重新排序，删除），攻击者就可以显著降低被攻击模型的表现。使用超过三个注入时大部分模型会直接失效。说明攻击者可以在不对底层模型做出任何假设的情况下使用特定方法影响系统。基于文本的NLP系统需要细心的输入净化。
