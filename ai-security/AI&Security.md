# AI&Security

[TOC]

## 网页追踪和广告检测

### S&P 2022

#### WTAGRAPH: web tracking and advertising detection using GNN

- 目标问题：网页追踪和广告检测问题(WTA)
- 相关工作：原有的基于黑白名单或机器学习方法的屏蔽方法有限的精确度和效率问题(误分类和没有识别到的WTA请求)
- 解决方案：提出了一种基于GNN的网络流量追踪和广告检测框架。首先构建表示HTTP网络流量的属性同构多图，将检测问题构建成图上的边表示学习和分类问题。收集http网络流量，DOM和javascript数据构建多图，使用直推式学习训练模型，并用归纳式进行WTA检测
- 实验效果：使用Alexa Top 10K网站搜集的数据进行实验，手动验证可以发现原来方法发现不了和误分类的WTA请求。可以在更灵活的选项下实现可竞争的的效果。

### NDSS 2023

#### DOITRUST: dissecting on-chain compromised internet domains via Graph Learning

- 目标问题：网站访问黑白名单的不完全和欠反应问题。
- 解决方案：首先引入一种扩展图，通过爬取超链接，依据信任的传递性来创造有机增长的互联网域名允许列表，然后使用拓展图来凸显检测节点的缺失，恶意节点隐藏在沿被污染网站的路径上，称为“链上妥协”。为解决链上妥协问题，设计了两步的整体方案DoITrust，利用个体节点特征和拓扑结构分析。设计了一种半监督的嫌疑预测方案，来预测一个节点与妥协目标（被拒绝的节点）相关的概率。方案包括一个新的节点排名方法来使用拓扑信息，以及一种图学习方案来分离全局传播和局部预测模型的训练。DoITrust还基于嫌疑预测结果提出有效的修剪策略来从抓取的途中删除高度可疑的节点并分析潜在的妥协指标。
- 实验效果：使用少于1%的标记节点进行嫌疑预测时准确率达到了90%，超过了现有的基于节点和结构的方法。还证明DoITrust时可移植的，手动检查已发现的被污染节点表明至少有94.55%具有可疑内容。

### USENIX security 2023

#### AutoFR: automated filter rule generation for adblocking

- 研究问题：广告阻挡的过滤列表生成问题。
- 相关工作：已有基于ML的工作需要人工反馈或现有过滤列表的监督来训练，且并不考虑屏蔽广告语避免破损之间的权衡。
- 解决方案：提出了完全自动化过滤规则生成和评估的强化学习框架AutoFR。设计了基于多臂老虎机的方法来生成过滤归责并控制过滤广告与避免视觉破损（visual breakage）之间的权衡。
- 实验效果：在上千个网站上进行了测试，可以生成屏蔽86%广告的规律规则，相较于屏蔽了87%的EasyList有更好的视觉连贯性。生成的规则可以有效泛化到新网站。

## 模型提取攻击

### S&P 2022

#### [read] Model Stealing Attacks against inductive GNN

- 目标问题：针对GNN的模型提取攻击，根据与目标模型的交互逆向还原出一个替代模型。
- 相关工作：已有工作主要集中在图片和文本等任务的模型上，对图数据上的模型没有探索
- 解决方案：提出第一个针对归纳式GNN的模型窃取攻击方法，系统性的定义了威胁模型，并提出了六种基于提取者的背景知识和目标模型反馈的攻击方法
- 实验效果：在六个基准数据集上显示提出的攻击方法有较好效果

#### [read] DeepSteal: advanced Model Extractions leveraging efficient weight stealing in memories

- 目标问题：模型提取攻击，逆向还原替代模型
- 相关工作：
- 解决方案：首次提出使用内存侧信道攻击的模型提取框架，用于提取DNN模型权重参数。首先使用基于rowhammer的攻击技术来提取权重位的信息，然后提出一种新的使用平均聚类权重惩罚的子模型训练方法来获取部分泄露的位信息并生成目标模型的子模型原型。
- 实验效果：在三个图像数据集和四个DNN架构上进行试验，提取的子模型可以获得超过90%的测试准确率，还可以有效生成足以欺骗目标模型的攻击输入样例。

### USENIX security 2023

#### [read] A Plot is Worth a Thousand Words: model information stealing attacks via scientific plots

- 研究问题：针对模型提取攻击任务的侧信道攻击方法。
- 相关工作：
- 解决方案：发现了一种新的侧信道攻击方法，利用科学图像进行攻击。使用浅层模型训练技术生成训练数据来攻击图片分类任务的目标模型。
- 实验效果：实验结果证明可以有效推理出基于CNN的模型结构或超参数。发现攻击成功主要是由于科学图片的形状，说明攻击在多种场景下鲁棒，最后提出了多种防御机制来降低精度。

#### PCAT: functionality and data stealing from split learning by pseudo-client attack

- 研究问题：联邦学习的半诚实服务器上的数据和模型窃取攻击问题
- 相关工作：已有攻击手段往往需要知道关于client的网络结构，并且在模型结构加深时效果会明显减弱。没有考虑客户端模型对服务器透明且更加复杂和深度的情况
- 解决方案：从服务端模型角度探索内部隐私泄露问题，发现客户模型和数据信息可以从服务端获得，并且一系列中间服务器模型可以导致更多的泄露。基于以上观察提出第一个从半诚实服务器端窃取client模型权重，重建输入并推理标签的攻击手段Pseudo-Client ATtack（PCAT），攻击手段对客户端透明，服务端可以在客户端无感情况下获取客户端数据。
- 实验效果：在包括更加复杂的模型和学习任务，甚至不满足iid条件的任务等场景上明显超出SOTA攻击方法，并且能有效适应目前的防御机制。

### CCS 2022

#### SSLGuard: a watermarking scheme for self-supervised learning pre-trained encoders

- 研究问题：预训练编码器的版权保护问题，攻击者会训练一个模型来模仿目标模型的行为。自学习编码器对模型窃取攻击非常脆弱。
- 相关工作：目前的版权保护工作主要集中在分类器上，对编码器的保护工作研究不多
- 解决方案：提出第一个面向预训练编码器的水印方法SSLGuard，为给定的预训练编码器植入水印并输出有水印的版本，浅层训练技术也用于保护水印免受模型窃取攻击的影响。
- 实验效果：实验证明SSLGuard在水印植入和验证上的有效性，以及在模型窃取和其他水印移除攻击（输入噪音，输出扰动，充血，模型修剪和微调）上的鲁棒性

#### [read] On the Privacy Risks of Cell-Based NAS Architecture

- 研究问题：神经网络结构搜索的隐私保护问题
- 相关工作：已有工作主要关注如何搜索性能更好的网络结构，忽视网络结构对于隐私攻击的鲁棒性。
- 解决方案：系统性的度量了NAS结构的隐私风险，并探索了基于结构单元搜索的架构的微观模式，衡量了结构单元模式如何影响架构的隐私风险
- 实验效果：根据实验结果解释了如何设计鲁棒的NAS架构，并且提供了一个理解NAS架构和其他隐私风险隐形联系的通用方法

#### [read] StolenEncoder: stealing pre-trained encoders in self-supervised learning

- 研究问题：预训练编码器的版权保护问题。
- 相关工作：
- 解决方案：提出了第一个窃取预训练图像编码器的攻击方法StolenEncoder。
- 实验效果：在多个数据现实世界的预训练编码器上测试，证明窃取的编码器与目标编码器有着类似的功能。

## 数据提取攻击（包含隐私保护问题）

### S&P 2022

#### [read] Reconstructing Training Data with Informed Adversaries

- 目标问题：训练数据重建问题，在知道除了一个之外的训练数据时，根据模型交互重建剩余的这个未知的数据
- 相关工作：
- 解决方案：通过实例化攻击，说明可以在给定模型下重建剩余训练数据。针对通用模型，提出了一种基于训练以目标模型权重位输入，训练数据为输出的重建网络的攻击策略。
- 实验效果：在MNIST和CIFAR-10上验证了攻击策略的有效性，系统分析了影响重建的机器学习训练环节，并理论分析了足以缓解重建攻击的差分隐私程度。在接触到模型存储时可以有效进行重建攻击，标准的模型已经包含足够的信息来进行高精确度的重建，在utility degradation较小的参数域进行隐私差分就可以有效缓解此类攻击

#### [read] LINKTELLER: recovering private edges from GNN via influence analysis

- 目标问题：联邦学习下的边隐私问题，在向远端发送节点特征信息进行训练后，利用远端的API预测测试节点间的边信息。
- 相关工作：
- 解决方案：提出隐私攻击方法LINKTELLER，通过为本地设计查询语句，使用影响分析发现远端持有的边信息.
- 实验效果：在生成式和直推式数据集上LINKTELLER都可以发现大量隐私边。使用不同的差分隐私图卷积网络测试后发现LINKTELLER依然可以发现隐私边。

### USENIX security 2023

#### PrivateFL: accurate, differentially private federated learning via personalized data transformation

- 研究问题：联邦学习支持多个client在一个中心服务器的组织下联合训练模型，攻击者（不可信的服务器）可以通过多种推理攻击方式危害client本地训练数据的隐私性。差分隐私方法用于解决此问题，但会牺牲模型的精确程度。
- 相关工作：通过调整数据规模和激活函数的方法无法处理client引入的不均匀性；控制数据质量的方法处理训练数据，但不处理差分隐私噪声
- 解决方案：首先提出精确度衰减的部分原因是差分隐私在加入随机噪声时为client引入了额外的不均匀性。随后设计了PrivateFL来学习去除不均匀性的精确的差分隐私模型。主要思路是为每个client同时训练一个差分隐私个性化的数据转换器，用于移动数据分布来补偿引入的不均匀性，从而提升精确度。
- 实验效果：与8个sota方法在7个数据集上进行比较，结果说明PrivateFL可以学到较精确模型

#### PrivTrace: differentially private trajectory synthesis by adaptive markov models

- 研究问题：轨迹信息的隐私问题。
- 相关工作：差分隐私方法主要研究为具体场景设计算法而不是设计满足差分隐私的数据集，数据生成方法要么只用一阶马尔可夫链模型，难以保留足够的转换信息，要么用高阶马可夫链模型，引入额外的噪音。
- 解决方案：结合差分隐私和马可夫链模型来生成虚假轨迹，提出了一个框架来分析马可夫链模型在轨迹生成问题上的作用，在此基础上提出了PrivTrace算法，自适应地利用一阶和二阶马可夫模型
- 实验效果：在人工和真实数据集上测试了方法的效果

#### Improving real-world password guessing attacks via bi-directional transformers

- 研究问题：密码猜测攻击问题。密码猜测攻击可以概念化为估计文本token的概率分布。双向transformer因为可以利用两个方向的文本来捕捉文本的细微差别而表现突出。
- 相关工作：已有方法简单将transformer引入猜测攻击，并没有超过其他对比方法，使用trasformer的方式并不直观，并且需要针对任务进行设计。
- 解决方案：提出了一种基于双向transformer的猜测框架PassBERT，在密码猜测攻击中使用预训练范式。首先准备一个预训练的密码模型，然后为三种攻击场景（有条件密码猜测，目标密码猜测，基于自适应规则的密码猜测）设计特定的微调方法。
- 实验效果：微调模型可以在三种攻击场景下超过有超过SOTA 14.53%，21.82%和4.86%的表现。，最后提出了一种混合密码强度指标来缓和三种攻击效果。

#### [read] GAP: differentially private graph neural networks with aggregation perturbation

- 研究问题：学习具备差分隐私能力的GNN模型问题，目标是保护敏感的图结构信息和其他相关数据。
- 相关工作：已有方法无法同时实现节点级和边级的隐私保护，或需要公开数据支持，以及无法应用于复杂图神经网络。
- 解决方案：设计了一种新的基于聚合扰动的差分隐私GNN模型，为GNN的聚合函数添加随机噪声，从而混淆一条边的存在（边级隐私）或者一个点以及所有相连的边（点级隐私）。模型首先用编码器模块学习节点嵌入，然后使用聚合模块得到含噪声的节点聚合嵌入，最后训练分类器完成节点分类任务。GAP可以利用多跳信息，并且使用训练时的隐私成本就可以在训练和推理过程中都实现差分隐私
- 实验效果：在三个数据集上的实现表明GAP相较其他DP-GNN方法有更好的精确度和效率的平衡。

#### Gradient Obfuscation Gives a False Sense of Security in Federated Learning

- 研究问题：联邦学习中的客户端隐私保护问题，针对数据提取攻击。
- 相关工作：已有方法（梯度量化，梯度稀疏，梯度扰动）可能会对联邦学习的安全造成错误理解。
- 解决方案：提出了一种针对联邦学习的图像分类任务的新重建攻击框架，不将隐私强化作为梯度压缩的副产品。在框架下设计了一种新方法来在语义层面重建图像。
- 实验效果：将语义隐私泄露和传统图像相似度分数做比较，说明了重新设计隐私保护机制的重要性。

### NDSS 2023

#### Focusing on Pinocchio's Nose: a gradients scrutinizer to thwart split-learning hijacking attacks using intrinsic attributes

- 目标问题：联邦学习上的数据提取攻击问题。攻击者可以劫持client发向server的结果（可以视作输入数据的隐层编码），训练一个自编码器来还原输入数据，从而完成劫持目标。
- 相关工作：SplitGuard是目前唯一抵御此类攻击的方法，但实验证明对SplitSpy自适应劫持攻击表现不佳。
- 解决方案：提出了一种新的被动检测方法Gradients Scrutinizer，根据目标模型和可以模型之间的区别进行识别。目标模型中对于同一标签样本的梯度相似程度与不同标签样本的不同，而在可疑模型中两种相似程度类似。
- 实验效果：实验结果证明提出的方法可以有效抵御联邦学习的劫持攻击和自适应对抗攻击。

### CCS 2022

#### Are Attribute Inference Attacks Just Imputation?

- 研究问题：敏感值推理，一种数据推理攻击问题。攻击者知道部分训练数据并可接触由其训练出的模型，最终推理出这些训练数据未知的敏感信息，比一般属性推理攻击要求精度更高，属性值的敏感度也更高。
- 相关工作：
- 解决方案：将属性推理与数据归因在多种攻击者可获得训练数据的假设下进行了详尽的对比实验。
- 实验效果：目前的属性推理方法并不比只知道训练数据分布而不能接触模型的对抗方法好，黑盒属性推理难以学到任何不用模型就学不到的知识，但白盒攻击可以有效的识别某些不接触模型就识别不到的敏感属性值。

#### [read] LPGNet: link private graph networks for node classification

- 研究问题：图神经网络上的数据窃取攻击，边窃取攻击。在不接触网络模型的前提下推理出现在训练图中的边是可行的。
- 相关工作：
- 解决方案：提出了新的网络架构LPGNet在有隐私敏感的边的图上训练图神经网络模型，通过新的图结构使用方法为边的使用提供差分隐私保证。
- 实验效果：实验结果说明LPGNet相较不使用边信息的隐私保护架构有更好的可用性，相较使用全图结构的GCN在面对边窃取攻击时更鲁棒。相较SOTA隐私保护GCN而言有更好的可用性和隐私保护的平衡。

#### [read] Finding MNEMON: reviving memories of node embeddings

- 研究问题：图神经网络模型的数据窃取攻击问题
- 相关工作：当前工作主要关注隐藏图上信息或者理解图神经网络的安全性和隐私问题，没有关注上游模型和下游任务pipeline整合时的隐私风险。
- 解决方案：提出了新的模型未知时的图复原攻击，探索保存在图节点嵌入中的图结构信息。
- 实验效果：对抗攻击可以在仅接触节点嵌入矩阵而不接触图模型的情况下复原边的信息。

#### Truth Serum: poisoning machine learning models to reveal their secrets

- 研究问题：数据窃取攻击问题
- 相关工作：
- 解决方案：提出了一类新的攻击方法，说明可以毒害数据的对抗攻击可以让在此数据上训练的模型泄露属于第三方的训练数据中的隐私信息。方法将完整性攻击和隐私攻击两者联合了起来。
- 实验效果：攻击方法可以在只毒害<0.1%的数据下将推理攻击的效果增大1到2个数量级。掌握了显著比例的训练数据的对抗攻击方可以使用无目标攻击，向所有其他用户的数据进行8x精度的推理。

#### Group Peoperty Inference Attacks Against Graph Neural Networks

- 研究问题：图神经网络模型上的特性推理攻击，在与GNN接触过程中推理训练图上的敏感特性。
- 相关工作：已有工作主要探索图级别的特性（节点度数，图密度），不关注节点和边的群组的特性。
- 解决方案：首先根据对手知识类型将攻击模型分类，然后设计了六种攻击方式，。然后分析了攻击成功的底层原因
- 实验效果：使用和不使用有目标特性的图训练GNN模型会导致模型参数和输出的不同，从而帮助对抗方推理特性是否存在

#### Enhanced Membership Inference Attacks against Machine Learning Models

- 研究问题：成员推理攻击，量化评估机器学习方法泄露了多少训练数据及原因
- 相关工作：
- 解决方案：提出了一个理解假设测试框架来统一解释已有工作，并设计新的推理攻击方法。提供了一个不可分辨博弈的模板，请提供了博弈中不同实例的攻击成功率的解释。
- 实验效果：方法试图将攻击的不确定性缩小到某个特定的数据节点的出现和消失上，并对不同类型的攻击进行了区别分析，展示了数据节点面对某种攻击脆弱的原因。

## 后门攻击

### S&P 2022

#### Spining Language Models: risks of propaganda-as-a-service and countermeasures

- 目标问题：发现了一种新的seq2seq模型的模型旋转威胁：可以导致模型修改输出倾向来在触发某些特定词支持选定的语义或观点的训练时间攻击。模型旋转将元后门引入模型，在保持上下文和标准精确度指标表现的前提下支持特定的元任务。模型旋转支持宣传即服务（此处宣传定义为有偏向的言论）。攻击者可以使用模型旋转定制模型来产生虚假信息或者将模型放入机器学习管道来污染下游任务。
- 解决方案：提出新的后门攻击技术，将元任务安插至seq2seq模型中，将需要的元任务输出反向传播至词嵌入空间作为点（“伪词”），然后使用伪词来偏移模型的整体输出分布。随后提出了一种黑盒且元任务独立的防御措施，给定候选触发词列表后可以检测出使用这些词进行了旋转的模型。
- 实验效果：在语言生成，总结和翻译三种模型上使用不同出发词和元任务测试了这种攻击的效果。结果证明模型旋转可以在保持精确度指标表现的情况下满足元任务要求，并且在供应链攻击场景下旋转的效果可以传播至下游任务。

#### BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning

- 目标问题：自监督学习模型上的后门攻击问题，让使用自监督学习编码器搭建的下游分类器继承后门的行为。
- 解决方案：提出了第一个针对自监督学习的后门攻击方法BadEncoder，将后门攻击描述为优化问题，并使用梯度递减方法解决，利用干净的编码器生成受污染的编码器。
- 实验效果：BadEncoder获得了很高的攻击成功率，同时保留了下游分类器的精确度。

#### Piccolo: exposing complex backdoors in NLP transformer models

- 目标问题：NLP模型上的后门检测问题。
- 相关工作：后门检测问题由于管道不连续和搜索空间大等NLP任务的特点而难以解决。现有的后门检测方法在面对较复杂的触发词和模型时效果不佳。
- 解决方案：提出了一种新的后门扫描技术，将模型转换为等价但可微的形式，然后使用优化方法来逆推出表示触发词相关性的词分布。方法使用新的词判别性分析来判断模型是否可以分辨类似的触发词的存在。
- 实验效果：使用大量NLP模型，复杂网络结构和不同的攻击类型进行测试，实验结果证明了方法的有效性。

### USENIX security 2023

#### [read] FreeEagle: detecting complex neural trojans in data-free cases

- 研究问题：复杂网络上的后门攻击问题。
- 相关工作：大多数现有方法都假设防御着可以获得一部分干净的验证样本或者有触发词的样本,在某些真实场景中并不成立
- 解决方案：提出了第一个不用数据的后门检测攻击FreeEagele，可以在不接触任何样本的前提下从DNN中检测复杂后门攻击
- 实验效果：实验证明可以有效抵御多种后门攻击，甚至超过某些SOTA的需要数据的后门检测方法。

#### A Data-free Backdoor Injection Approach in Neural Networks

- 研究问题：DNN模型上的后门攻击问题
- 相关工作：现有方法基本都需要接触原始的训练测试数据或者与主要任务相关的数据，以便植入后门
- 解决方案：提出了一种新的无需数据的后门植入攻击，使用与主要任务无关的数据，设计新的损失函数来用这些数据微调模型，并优化后门植入与主任务表现上的平衡。
- 实验效果：在多个深度学习场景和模型上进行了实验，结果证明了方法可以以接近100%的成功率植入后门，并在下游主要任务上保持可接受的表现。

### NDSS 2023

#### Backdoor Attacks agiainst dataset distillation

- 目标问题：数据蒸馏任务上的后门攻击问题。数据蒸馏是为了将大规模数据中的知识蒸馏到较小规模的数据集中，使在较小数据集上训练的模型有跟在大规模数据上训练相近的效果。
- 相关工作：目前的方法主要关注资源利用率和模型可用性上的取舍，很少关注数据安全问题。
- 解决方案：首次提出了图像领域在利用数据蒸馏模型得到的数据上进行训练的模型的后门攻击。提出了两种后门攻击方法NAIVEATTACK和DOORPING，NAIVEATTACK在数据蒸馏阶段向原始数据加入触发数据，DOORPING在整个蒸馏过程中迭代地更新触发数据。
- 实验效果：NAIVEATTACK在某些实验数据上获得了很好的攻击成功率(ASR)结果，DROOPING在所有数据上都获得了更好的ASR分数。进行消融实验分析了可能影响攻击效果的因素，最后实验了若干防御策略，证明提出的攻击方法可以规避这些防御措施。

#### The "Beatrix" Resurrections: robust backdoor detection via gram matrices

- 目标问题：后门攻击检测问题
- 相关工作：已有的后门攻击抵御方案通常假设被污染的样本共享同样的触发器，但近期的后门攻击说明在动态后门攻击中触发器也在随着输入而变化。
- 解决方案：提出了一种新的方法Beatrix，使用Gram matrix来捕捉表征的特征关联和高阶信息。通过学习普通数据中活跃模式的类别相关的统计信息，Beatrix可以通过捕捉活跃模式中的异常来识别污染数据。Beatrix进一步使用基于kernel的测试来在部队表征分布进行假设的情况下识别目标类别。
- 实验效果：实验效果证明在识别动态后门上达到了91.1%的F1分数，而目前的最优方法只能达到36.9%。

#### BEAGLE: forensics of Deep Learning backdoor attack for better defence

- 目标问题：针对基于DL的后门攻击的攻击检测问题。
- 解决方案：提出一种新的模型后门取证技术。通过给定的各种包含后门触发词的输入，方法可以自动将其分解为干净输入和对应的触发词，然后将触发词根据其特性聚类以自动进行分类总结。后门扫描器随后进行自动扫描在其他模型中找到相同类型的后门实例。
- 实验效果：在2532个预训练模型的10中流行攻击方式上与9个基准方法进行比较，结果证明方法得到贴近真实结果的触发词分解结果，扫描器可以发现其他基准无法发现的攻击。

### CCS 2022

#### ATTRITION: attacking static hardware trojan detection techniques using reinforcement learning

- 研究问题：对硬件的木马攻击的检测问题
- 相关工作：已有工作存在低检测成功率，高计算复杂度和需要大量测试模式的问题，并且广泛基于攻击是随机植入木马的假设，导致了检测方法表现出的虚假的精确度和安全性。目前没有对硬件木马进行系统性检测的方法
- 解决方案：提出了一个基于强化学习的自动化可扩展的攻击框架ATTRITION。
- 实验效果：ATTRITION绕开了两个木马检测方法类别下的8种检测方法，相较随机植入木马的方法在面对逻辑测试和侧信道技术的攻击成功率提升了47x和211x。

#### LoneNeuron: a highly-effective feature-domain neural trojan using invisible and polymorphic watermarks

- 研究问题：DNN的木马攻击问题
- 相关工作：当前大部分工作关注于将木马植入数据中，而植入代码和植入模型的方法才刚刚开始研究
- 解决方案：提出了一个新的模型木马植入方法LoneNeuron，响应转换为不可见的指定样本的多带像素域水印的特征域模式。因为独有的水印多态特性，同一个特征域的触发词可以被分配到像素域的多个水印中，进一步提升水印的随机性，隐蔽性和稳定性
- 实验效果：使用高度的攻击独特性，LoneNeuron达到了100%的攻击成功率。实验说明LoneNeuron可以绕过SOTA的木马检测，并且是ViTs的最有效攻击者。

#### EIFFeL: Ensuring Integrity for Federated Learning

- 研究问题：联邦学习的完整性问题，因为更新梯度的聚合是加密完成的，恶意植入的木马就无法被发现。
- 相关工作：
- 解决方案：形式化确定了保证更新隐私性和完整性的问题，并提出了一个新系统EIFFeL来加强更新梯度的安全聚合。EIFFeL是一个可以加强任意完整性检查并移除恶意梯度，同时不违反隐私性的通用框架。
- 实验效果：在100个client和10%的恶意植入环境下，EIFFeL可以以2.4s一次迭代的情况下训练一个MNIST分类器达到没有恶意植入一样的完整性。

## 对抗攻击

### S&P 2022

#### Bad Characters: imperceptible NLP Attacks

- 目标问题：针对基于文本的对抗样本攻击存在的保留语义信息和不可分辨性能力欠缺问题。
- 相关工作：已有工作主要关注视觉模型中人类和机器之间理解的差别，针对文本模型的对抗攻击难以保留语义信息和不可分辨性。
- 解决方案：探索了大量的可用于黑盒攻击文本模型的对抗样本。使用人肉眼无法分辨的针对编码的置换来操纵大量的NLP系统的输出。
- 实验效果：使用单一的不可分辨的编码注入（不可见字符的表示，同形异意，重新排序，删除），攻击者就可以显著降低被攻击模型的表现。使用超过三个注入时大部分模型会直接失效。说明攻击者可以在不对底层模型做出任何假设的情况下使用特定方法影响系统。基于文本的NLP系统需要细心的输入净化。

### USENIX security 2023

#### No more Reviewer 2: subverting automatic paper-reviewer assignment using adversarial learning

- 研究问题：论文审核分配系统，根据统计学的语言模型容易被手动操纵的问题。
- 相关工作：以往工作主要关注控制审核与评委碰撞问题，当前工作主要关注对抗学习方法
- 解决方案：提出一种攻击方式，通过修改给定论文来误导分配系统，从而选择它自己指定的审稿人。攻击基于一种新的优化策略，通过交替特征空间和问题空间来向论文添加不明显的变更。
- 实验效果：在真实的会议场景中进行分配模拟，结果说明可以在不接触分配系统的前提下选择和移除审稿人。修改的论文保持了合理性，并且难以人为分辨。

#### The space of adversarial Strategies

- 研究问题：对于对抗学习的理解问题，目前的对抗学习方法在威胁模型的假设和优化的定义上都各不相同。
- 相关工作：没有提到对抗的理论性工作，本文探索是否存在最优攻击方式
- 解决方案：提出了系统方法来描述最差情况的对抗学习。通过将攻击组成原子化为surface和travelers，对对抗机器学习的攻击进行可扩展的分解。在此基础上枚举组成部分创建了576种攻击方式，最后提出了帕累托组合攻击，一种理论上表现最优的攻击方式。
- 实验效果：在多个数据集和不同的模型构成的对抗策略空间（根据给定模型和领域时的攻击集合）上进行测试，实验结果表明攻击表现与环境信息（领域，模型稳定性）高度相关，攻击模型对攻击效率有较大影响。

#### Squint Hard Enough: attacking perceptual hashing with adversarial machine learning

- 研究问题：对实时内容扫描任务的感知哈希技术的对抗攻击问题。实时内容扫描用于识别违法内容，感知哈希技术根据文件生成摘要来与数据库中的违法内容进行模糊比对。将感知哈希匹配纳入客户端和端到端加密系统中的方式对感知哈希技术的对抗鲁棒性提出了更高要求（服务器端完全无法查看原内容）
- 相关工作：对感知哈希技术的研究很少从隐私和安全角度出发，服务器端的感知哈希仅在严格的机密性协议下有效。
- 解决方案：设计了对抗设定下针对哈希感知算法的威胁模型，并对两种算法进行了攻击。进一步按久了满足检测回避攻击的图像生成方法。
- 实验效果：可以有效生成目标的二次前向攻击，现有的感知哈希方法在检测回避攻击下并不鲁棒。

#### UnGANable: defending against GAN-based face manipulation

- 研究问题：基于生成对抗网络的面部操纵攻击的防御问题。GAN逆映射将目标图像映射到模型隐空间，以便生成指定方向的图像。
- 相关工作：检测方法只能被动检测面部图片是否被修改，扰乱方法模仿骨干网络，只对基于图片转换的操纵方式有效
- 解决方案：针对GAN逆映射的防御问题提出UnGANable防御系统，通过搜索目标图像附近的其他图像（遮罩图）来在发布后削弱GAN的逆映射过程。
- 实验效果：为两种sota逆映射技术设计了五种不同的防御措施，在两个基准数据集上的实验说明了有效性，并进一步探索了可能的绕过UnGANable的对抗方法。

#### Every Vote Counts: ranking-based training of federated learning to resist poisoning attacks

- 研究问题：联邦学习的无目标中毒攻击问题
- 相关工作：已有方法主要通过限制可接受更新的范围来防御中毒攻击，只考虑安全范围内的梯度更新从而限制对抗样本的选择。
- 解决方案：提出问题关键时client端的可选择的更新空间过大，因此提出了FRL框架，通过将更新空间从模型参数更新限制到模型参数级别的方式来限制选择空间。FRL clients根据本地训练数据对神经网络参数进行评分，server使用投票机制来聚合评分进行更新。
- 实验效果：实验证明了FRL的稳定性以及沟通高效性。

#### DiffSmooth: certifiably robust learning via diffusion models and local smoothing

- 研究问题：使用diffusion model在随机防御中获得更好更高效的光滑模型，提升光滑模型可验证鲁棒性的问题。
- 相关工作：
- 解决方案：理论证明从对抗中恢复的实例将以高概率位于相应原始实例的有界邻域内，去噪扩散概率模型（DDPM）的一次性去噪可以逼近连续时间扩散模型生成的后验分布的均值，从而在温和条件下逼近原始实例。在理论指导下设计了通用的可验证鲁棒对抗净化管道DiffSmooth，首先为输入加入随机高斯噪声方便验证，然后通过预训练扩散模型的逆步骤生成输入的去噪净化样本，最后为净化样本加入另一组噪声来生成局部光滑实例，并在平均置信度基础上进行预测。为所有输入重复以上步骤并选用主要投票作为最终的光滑预测。
- 实验效果：在不同数据集上验证了DiffSmooth相对于SOTA更高的可验证稳定性

#### Adversarial Training for Row-Binary Malware Classifiers

- 研究问题：可执行文件危害检测的机器学习模型的对抗训练问题。
- 相关工作：已有方法通过标准化可执行文件、随机遮罩一部分字节来进行识别，但标准化需要知道所有的IPR转换且计算成本巨大，随机遮罩会同时导致更多的误分类。
- 解决方案：首先大幅度提升了对抗样本生成的效率和规模，然后分析了不同的训练长度和针对不同攻击类型的效果。
- 实验效果：实验发现数据增强不会抵御SOTA攻击，但使用其他领域普通的梯度指导的方法会提升鲁棒性。在大多数情况下可以通过用同类型但低投入版本的攻击对抗训练来提升鲁棒性。通过使用某些类型的攻击可以提升针对其他类型攻击的鲁棒性。

### NDSS 2023

#### REaaS: enabling adversarially robust downstream classifiers via robust encoder as a service

- 目标问题：编码器即服务（服务提供商自行训练编码器并将其作为云服务API开放），下游分类器使用API得到输入数据，当输入数据具有扰动时下游分类器很容易受到对抗样本的攻击。研究服务商应当提供怎样的API，下游客户才能在最少的查询次数的前提下验证下游分类器的稳定性；以及运营商应当怎样预训练编码器，下游客户才能建造更加稳定的分类器。
- 解决方案：设计了两种API让下游客户在最小化查询次数的前提下验证分类器的稳定性，并发现使用谱范数正则化项与训练模型可以让下游客户建造更加稳定的分类器。
- 实验效果：

#### [read] Adversarial Robustness for tabular data through cost and utility awareness

- 目标问题：表格数据上模型的对抗鲁棒性问题。
- 相关工作：目前的对抗稳定性工作主要关注图像和文本领域的机器学习模型，没有考虑表格数据领域。这些模型无法捕捉到成本不明显的攻击，不同的攻击样本可以对实用性造成不同的影响，无法直接应用于表格数据。
- 解决方案：针对攻击能力和表格领域的攻击限制设计了新的成本和实用性敏感的威胁模型。提出了一种用于设计攻击和防御机制的框架，以保护模型对抗成本和实用性敏感的对抗样本。
- 实验效果：在三个数据集上证明方法的有效性。

#### RoVISQ: reduction of video service quality via adversatial attacks on deep learning-based video compression

- 目标问题：基于深度学习的视频压缩和下游分类系统的对抗攻击问题。
- 解决方案：提出攻击框架RoVISQ，操作视频压缩模型的码率-失真关系来达到增加网络带宽和/或降低终端用户视频质量的目的。设计了对下游视频分类服务的针对性和无针对性攻击的目标。提出了一个输入无关的扰动，可以同时扰动视频压缩和分类系统。是第一个可以抗压缩的扰动方法。
- 实验效果：在对抗训练，视频去噪和JPEG压缩等任务上进行测试，RoVISQ攻击会使峰值信噪比恶化高达5.6dB，使比特率增加～2.4倍，同时在下游分类器上实现超过90%的攻击成功率。

#### Securing Federated sensitive Topic Classification against poisoning Attacks

- 目标问题：针对敏感内容检测的污染攻击防御问题。污染攻击通过传播错误的模型更新来影响良性用户的识别准确性。
- 解决方案：提出了一种基于联邦学习的分布式分类器来识别包含敏感内容的URL，解决以往离线/集中式分类器的局限性。为抵御污染攻击，根据主观逻辑和基于残差的攻击检测开发了一种稳健聚合方案。
- 实验效果：理论分析，路径驱动模拟以及真实用户实验验证结果表明分类器可以高精度的检测敏感内容，快速学习新标签，并对污染攻击和输入不完美情况保持稳定。

### CCS 2022

#### Understanding Real-world Threats to Deep Learning Models in Android Apps

- 研究问题：深度学习模型的对抗攻击问题
- 相关工作：已有工作主要考虑有限的数据集合模型，在真实世界模型攻击上的有效性不甚明显。
- 解决方案：进行了第一个真实世界DNN模型对抗攻击的系统性研究，提出了一个真实世界模型数据集RWM。提出了一系列将现有对抗样本生成算法和真实世界模型适配的方法，包括从android应用自动提取模型，捕获模型输入输出，生成对抗样本并通过观察模型执行进行验证等。针对黑盒DL模型设计了一种基于语义的方法来搭建数据机，并用其训练数据集来进行基于转换的攻击
- 实验效果：在分析62583个真实世界应用的245个深度学习模型后，发现现有的对抗攻击算法只能直接攻击6.53%的模型，通过提出的数据集和方法，攻击成功率提升到了47.35%。

#### Order-Disorder: imitation adversarial attacks for black-box neural ranking models

- 研究问题：神经通路排序模型上的对抗攻击问题，神经文本评分模型继承了普通神经网络对于对抗攻击的脆弱性，这种脆弱性可能会被黑帽SEO。
- 相关工作：目前工作仅检测出了对于对抗的脆弱性，但没有解决。
- 解决方案：发现目标模型可以通过枚举关键的查询或训练数据然后在模仿模型上训练获得，使用模仿模型可以操作排序结果并将操作攻击转移到目标模型上。通过给目标函数新增下一句预测损失和语言模型流畅度约束来为目标模型添加伪装
- 实验效果：实验证明了攻击的有效性，伪装的有效性。

## 位翻转攻击

### USENIX security 2023

#### NeuroPots: realtime proactive defense against bit-flip attacks in neural networks

- 研究问题：位翻转攻击的防御问题，位翻转攻击通过将量化模型参数进行少量位翻转来降低模型精确度或者误分类特定输入，但因为可以发生在任何权重参数上，检测面过大，所以难以检测到。
- 相关工作：已有工作主要试图弥补DNN模型的脆弱性来抵抗位翻转攻击。
- 解决方案：提出了一种新的防御概念NeuroPots，通过将部分人工设计的脆弱神经引入DNN模型的方式来引诱攻击者进行攻击，从而缩小检测面，实现高效检测和模型恢复。设计了一种脆弱神经选择策略，并提出了两种将陷阱门嵌入DNN模型的方法，设计了一种基于校验和的方法来有效检测位翻转攻击，然后通过刷新陷阱门的方法来挽救模型精确度。
- 实验效果：实验证明方法可以有效检测攻击并低成本恢复被攻击模型。

#### Aegis: mitigating targeted bit-flip attacks against deep neural networks

- 研究问题：DNN上的位翻转攻击防御问题
- 相关工作：现有关注无目标场景的防御方法或者需要额外的可信应用，或者让模型在面对有目标攻击时更加脆弱。针对有目标攻击的防御方法需要更加隐蔽也更有意义，还有所欠缺。
- 解决方案：提出了一种新的缓和有目标BFA攻击的防御方法Aegis。根据已有攻击关注在特定关键层上进行翻转的观察结果，设计了一种动态退出机制来为隐层增加额外的内部分类器。此机制允许输入样本从不同层提前离开，有效打乱对抗攻击计划。同时通过在推理时随机选择分类起来增加自适应攻击的成本。进一步提出了一种鲁棒训练机制来在分类器训练过程中通过模拟BFA让分类器适应不同的攻击场景。
- 实验效果：在四个数据集和两个DNN结构上测试表明Agis可以有效缓解不同的SOTA有目标攻击，可以缓解5-10倍的成功率。

## 异常检测

### USENIX security 2023

#### PROGRAPHER: an anomaly detection system based on Provenance Graph Embedding

- 研究问题：对抗高级持续攻击（APT）的数据溯源（data provenance）技术可以捕获计算机系统或网络实体间的复杂关系，然后利用这些信息检测精确的APT攻击。但目前的方法在效率，精确度和粒度上没有做到很好的平衡。基于溯源图的方法会存在“依赖爆炸”的现象。
- 相关工作：基于签名，启发式或则已知攻击模式的溯源系统可以被绕过；使用单一溯源图的系统难以处理大量的日志和警告数据；基于时序快照的系统识别粒度过于粗糙，需要人工分析异常快照中的所有实体和交互。
- 解决方案：提出了一种溯源图上的新异常检测方法PROGRAPHER，在日志上提取时序快照并在其上构建溯源图进行检测。使用图嵌入学习整图嵌入,然后利用序列学习方法来获取图上的结构信息，最后从异常的快照上提取指标并汇报给分析员。
- 实验效果：在五个真实世界数据集上进行实验，结果证明可以高精确度检测标准攻击和APT攻击，超越现有最好的检测系统

#### Learning Normality is Enough: a software-based mitigation against inaudible voice attacks

- 研究问题：针对语音助手的无声攻击检测问题，使用听不到的声音将有害信息打入目标的声控设备。存在攻击模式多样，在不同设备上不同的问题。
- 相关工作：基于硬件的方法可以修复硬件问题，但需要定制周期较长。基于软件的方法通常是基于攻击信号的声学特性的有监督学习方法，需要受攻击设备且费时。不同设备上的声学特性不同，需要方法轻量化可迁移。
- 解决方案：根据异常检测设计无监督学习方法。根据良性声音模式相似的特点，通过学习良性模式来检测恶性声音（异常）。NormDetect将频谱特征映射到低维空间，进行相似度查血并将它们替换为标准特征嵌入以进行频谱重建，从而得到良性声音和攻击之间明显的重建误差。
- 实验效果：在24个设备上的383320个测试样本上的结果显示有平均99.48%的AUC和2.23%的EER，说明了方法的有效性。

### NDSS 2023

#### Anomaly Detection in the Open World: normality shift detection, explanation, and adaptation

- 目标问题：概念漂移问题是在机器学习模型部署后，数据的分布和特性随着时间的推移而变化的问题。异常检测方法因为在零阳性（没有异常数据）情况下进行训练，所以对于异常行为的漂移具有免疫性，但正常行为发生漂移时则会有很严重的影响。
- 相关工作：当前方法主要集中在异常行为和监督学习的概念漂移上，对于零阳性异常检测的正常行为漂移则没有研究。
- 解决方案：提出OWAD，一种在实践中检测、解释和适应正常性偏移的通用框架。OWAD通过无监督方式检测漂移，减少手动标记的开销，并通过分布级别的处理获得更好的适应表现。
- 实验效果：实验证明OWAD可以通过更少的标记实现更好的适应表现。案例研究分析了正常性偏移，并为部署安全应用程序提供建议。

## 流量分析

### USENIX security 2023

#### An input-agnostic hierarchical deep learning framework for traffic fingerprinting

- 研究问题：基于深度学习的流量指纹识别应用
- 相关工作：已有方法面临流量个性化和层级不敏感两个问题，需要手动为每个任务设计方法将流量特征转换为神经网络的固定维度输入，同时无法捕捉不同流量包之间的层级关系和相关性。
- 解决方案：设计了一种输入无关的层次化深度学习框架，可以层次化地将全面的异构流量特征抽象为可消化的同构向量，以支持现有的神经网络进行进一步分类。
- 实验效果：实验表明该框架仅需一个范例，支持异构流量输入，在流量识别任务中与SOTA相比表现了相当的性能。

### NDSS 2023

#### BARS: local robustness certification for deep learning based traffic analysis systems

- 目标问题：流量分析任务中的深度学习模型稳定性验证问题。
- 解决方案：针对基于深度学习的流量分析系统的特征高多样性，模型设计多样性和对抗操作环境三个特点，设计BARS，一个基于边界自适应随机平滑的通用稳定性验证框架。BARS使用收敛于分类边界的优化的平滑噪声来获得更严格的稳健性保证。首先设计分布式transformer来生成最优平滑噪声，然后为噪声形状和噪声尺度设计了特殊的分布式函数和两个基于梯度的搜索方法来优化平滑噪声。
- 实验效果：在三个基于DL的流量分析系统上进行测试，结果表明BARS可以得到更严格的稳定性保证。

#### Detecting Unknown Encrypted Malicious Traffic in Real Time via Flow Interaction Graph Analysis

- 目标问题：加密恶意流量检测问题

- 相关工作：已有工作主要位监督学习方法，依赖已有攻击的先验知识

- 解决方案：提出了一种基于机器学习的实时无监督恶意流量检测系统HyperVision，使用基于流量模式构建的小型内存图识别加密的恶意流量。内存图通过边特征捕捉字节流交互模式，而不是已知攻击的特征。开发了一个图学习方法来检测异常的流量交互模式，并建立了一个信息论模型来说明图里保留的信息已经达到了理论上限。

- 实验效果：在48种恶意加密流量攻击的92个数据集上进行实验，结果说明HyperVision明显超过了SOTA方法。

## 可解释性

### USENIX security 2023

#### [read] AIRS: explanation for deep reinforcement learning based security applications

- 研究问题：安全应用中的深度强化学习模型可解释性问题。
- 相关工作：已有的深度强化学习解释方法通常关注寻找重要特征，不适用于安全应用，在解释的精确性，有效性和模型调试能力上有所欠缺
- 解决方案：提出了一种解释基于深度强化学习的安全应用的通用框架AIRS。框架通过建模最终输出和DRL agent采用的关键步骤之间的关系来找到对于最终输出最重要的步骤。
- 实验效果：对AIRS的解释能力，精确性，稳定性和有效性进行测试，结果超过其他可解释方法，同时证明方法可以促进DRL模型抵消错误，帮助用于建立对模型决策的信任。

#### xNIDS: explaining deep learning-based network intrusion detection systems for active intrusion responses

- 研究问题：基于深度学习的网络入侵检测系统的可解释性问题。
- 相关工作：已有工作的检测结果与可执行操作之间有差距，无法主动对检测到的入侵采取行动。高错误成本也导致不能完全根据检测结果相应。根本问题是已有系统的可解释性不足。已有的可解释性方法无法处理历史输入和结构数据复杂的特征相关性，在检测系统上表现不佳。
- 解决方案：提出了通过解释入侵检测系统来促进入侵响应的新框架XNIDS，框架通过近似和采样历史输入以及捕获结构数据的特征相关性来实现高精度解释。随后基于解释结果生成可执行的防御规则。
- 实验效果：在四个SOTA检测系统上进行试验，结果说明XNIDS在精确度，稀疏性，完整性和稳定性上都超过现有方法，并且可以有效生成防御规则。

### CCS 2022

#### "Is your explanation stable?": a robustness evaluation framework for feature attribution

- 研究问题：模型可解释性问题，属性归因方法的改进
- 相关工作：大部分方法关注于模型解释的精确度，但现实世界的噪音可能导致相似的图片得到迥异的解释结果。模型解释方法对对抗攻击并不鲁棒，会给有恶意的扰动输入做出相同的解释。导致模型解释方法在实际应用上的局限
- 解决方案：提出MeTFA框架来量化不确定性并提升解释算法的稳定性。MeTFA包括两个功能，验证一个特征的重要程度并生成重要性地图，计算特征归因的可信程度并生成MeTFA光滑地图来提升解释的稳定性。
- 实验效果：MeTFA提升了解释的可视化质量，在保持解释的可信度的同时显著降低了不稳定性。MeTFA光滑解释可以有效提升可信度

#### [read] AI/ML for Network Security: the emperor has no clothes

- 研究问题：模型的可解释性问题，无法用足够的细节来描述模型阻挠用户信任并使用模型。
- 相关工作：现有方法难以在高精度捕获模型内部决策的同时保持可用性（规模够小方便人类理解）。
- 解决方案：结合高精度和低复杂度决策树来解释ML模型。提出了Trustee框架，使用ML模型和训练数据作为输入，生成高精度且易于理解的决策树和对应的可信报告。
- 实验效果：在三个模型描述性不足的常见实例上展示了Trustee的效果：捷径学习的证据，虚假相关性的存在和分布外样本的脆弱性。

## 模型遗忘

### NDSS 2023

#### [read] Machine Unlearning of Features and Labels

- 目标问题：模型遗忘问题，从机器学习模型中移除信息。
- 相关工作：目前的机器遗忘方法只能有效移除单个数据点，无法扩展到大量特征和标签需要移除的场景。
- 解决方案：提出了第一个遗忘特征和标签的方法，方法建立在影响函数的概念基础上，并通过模型参数的封闭形式更新来实现遗忘。方法支持回溯调整训练数据的影响，从而改正数据泄露和隐私问题。
- 实验效果：对于有强凸损失函数的学习模型，方法可以提供有理论保证的遗忘。对于非凸损失函数的学习模型，实操证明可以有效且比其他策略更高效地遗忘特征和标签。

### CCS 2022

#### [read] Graph Unlearning

- 研究问题：图上的机器遗忘问题，移除掉一些训练数据对机器学习模型的影响。
- 相关工作：SOTA机器遗忘方法SISA无法直接应用于图数据上，将训练数据分片的做法会严重损害图的结构信息，进而损害模型可用性。
- 解决方案：针对图数据设计机器遗忘框架GraphEraser，包含两个新的图划分算法和一个基于学习的聚合方法。
- 实验效果：在五个数据集上进行了实验，在小数据集和大数据集上分别取得了2.06x和35.94x的时间提升，获得了62.5%的F1值。

## 可信推理（MLaaS）

### NDSS 2023

#### [read] Fusion: efficient and secure inference resilient to malicious servers

- 目标问题：安全机器学习推理问题。
- 相关工作：已有方法假设服务器是半诚实的，即遵守协议但尝试推理获得额外信息，但真实世界中服务器可能是有恶意的。已有关注服务器可能偏离协议的方法没有验证模型的准确性，同时将服务端模型和客户端输入的隐私一并保存。
- 解决方案：提出Fusion，客户端将功用样本和自己的样本混合称为查询，作为多方计算的输入来共同进行安全推理。因为使用其质量模型或者偏离协议的服务端只能产生容易识别的结果，Fusion迫使服务端诚实运行。
- 实验效果：实验证明Fusion比已有的恶意安全推理协议快48.06被并少用30.90倍的通信。在ResNet50模型上的ImageNet水平的推理比半诚实的协议快1.18倍，且少用2.64倍的通信量。

#### REDsec: running encrypted discretized neural networks in seconds

- 目标问题：机器学习即服务（MLaaS）的敏感数据隐私保护问题
- 相关工作：已有工作主要使用完全同态加密进行机器学习计算。
- 解决方案：提出REDsec框架，通过使用三元神经网络来优化基于完全同态加密的私人机器学习推理。REDsec提出一个新的数据复用方式来首次实现完全同态加密重的整数和位域的桥接，从而实现整数和位域的高效加法和激活等运算。提出的方法在一个新的GPU加速库(REDcuFHE)中包含。
- 实验效果：在MNIST，CIFAR-10和ImageNet数据集上进行了推理实现，相较相关工作有表现提升。

### CCS 2022

#### Private and Reliable Neural Network Inference

- 研究问题：神经网络的可靠性和隐私保护问题
- 相关工作：可信神经网络和隐私保护神经网络目前基本没有联系起来。
- 解决方案：提出了第一个在可靠神经网络上进行隐私保护推理的系统，为搭建随机光滑的关键算法模块设计完全同态加密副本，难点在于完全同态加密所需的控制流限制。
- 实验效果：系统可以在没有明显延迟的情况下进行隐私保护推理。
