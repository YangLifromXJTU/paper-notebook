# CCS 2022

## Graph Unlearning

- 研究问题：图上的机器遗忘问题，移除掉一些训练数据对机器学习模型的影响。
- 相关工作：SOTA机器遗忘方法SISA无法直接应用于图数据上，将训练数据分片的做法会严重损害图的结构信息，进而损害模型可用性。
- 解决方案：针对图数据设计机器遗忘框架GraphEraser，包含两个新的图划分算法和一个基于学习的聚合方法。
- 实验效果：在五个数据集上进行了实验，在小数据集和大数据集上分别取得了2.06x和35.94x的时间提升，获得了62.5%的F1值。

## SSLGuard: a watermarking scheme for self-supervised learning pre-trained encoders

- 研究问题：预训练编码器的版权保护问题，攻击者会训练一个模型来模仿目标模型的行为。自学习编码器对模型窃取攻击非常脆弱。
- 相关工作：目前的版权保护工作主要集中在分类器上，对编码器的保护工作研究不多
- 解决方案：提出第一个面向预训练编码器的水印方法SSLGuard，为给定的预训练编码器植入水印并输出有水印的版本，浅层训练技术也用于保护水印免受模型窃取攻击的影响。
- 实验效果：实验证明SSLGuard在水印植入和验证上的有效性，以及在模型窃取和其他水印移除攻击（输入噪音，输出扰动，充血，模型修剪和微调）上的鲁棒性

## Understanding Real-world Threats to Deep Learning Models in Android Apps

- 研究问题：深度学习模型的对抗攻击问题
- 相关工作：已有工作主要考虑有限的数据集合模型，在真实世界模型攻击上的有效性不甚明显。
- 解决方案：进行了第一个真实世界DNN模型对抗攻击的系统性研究，提出了一个真实世界模型数据集RWM。提出了一系列将现有对抗样本生成算法和真实世界模型适配的方法，包括从android应用自动提取模型，捕获模型输入输出，生成对抗样本并通过观察模型执行进行验证等。针对黑盒DL模型设计了一种基于语义的方法来搭建数据机，并用其训练数据集来进行基于转换的攻击
- 实验效果：在分析62583个真实世界应用的245个深度学习模型后，发现现有的对抗攻击算法只能直接攻击6.53%的模型，通过提出的数据集和方法，攻击成功率提升到了47.35%。

## "Is your explanation stable?": a robustness evaluation framework for feature attribution

- 研究问题：模型可解释性问题，属性归因方法的改进
- 相关工作：大部分方法关注于模型解释的精确度，但现实世界的噪音可能导致相似的图片得到迥异的解释结果。模型解释方法对对抗攻击并不鲁棒，会给有恶意的扰动输入做出相同的解释。导致模型解释方法在实际应用上的局限
- 解决方案：提出MeTFA框架来量化不确定性并提升解释算法的稳定性。MeTFA包括两个功能，验证一个特征的重要程度并生成重要性地图，计算特征归因的可信程度并生成MeTFA光滑地图来提升解释的稳定性。
- 实验效果：MeTFA提升了解释的可视化质量，在保持解释的可信度的同时显著降低了不稳定性。MeTFA光滑解释可以有效提升可信度

## ATTRITION: attacking static hardware trojan detection techniques using reinforcement learning

- 研究问题：对硬件的木马攻击的检测问题
- 相关工作：已有工作存在低检测成功率，高计算复杂度和需要大量测试模式的问题，并且广泛基于攻击是随机植入木马的假设，导致了检测方法表现出的虚假的精确度和安全性。目前没有对硬件木马进行系统性检测的方法
- 解决方案：提出了一个基于强化学习的自动化可扩展的攻击框架ATTRITION。
- 实验效果：ATTRITION绕开了两个木马检测方法类别下的8种检测方法，相较随机植入木马的方法在面对逻辑测试和侧信道技术的攻击成功率提升了47x和211x。

## On the Privacy Risks of Cell-Based NAS Architecture

- 研究问题：神经网络结构搜索的隐私保护问题
- 相关工作：已有工作主要关注如何搜索性能更好的网络结构，忽视网络结构对于隐私攻击的鲁棒性。
- 解决方案：系统性的度量了NAS结构的隐私风险，并探索了基于结构单元搜索的架构的微观模式，衡量了结构单元模式如何影响架构的隐私风险
- 实验效果：根据实验结果解释了如何设计鲁棒的NAS架构，并且提供了一个理解NAS架构和其他隐私风险隐形联系的通用方法

## AI/ML for Network Security: the emperor has no clothes

- 研究问题：模型的可解释性问题，无法用足够的细节来描述模型阻挠用户信任并使用模型。
- 相关工作：现有方法难以在高精度捕获模型内部决策的同时保持可用性（规模够小方便人类理解）。
- 解决方案：结合高精度和低复杂度决策树来解释ML模型。提出了Trustee框架，使用ML模型和训练数据作为输入，生成高精度且易于理解的决策树和对应的可信报告。
- 实验效果：在三个模型描述性不足的常见实例上展示了Trustee的效果：捷径学习的证据，虚假相关性的存在和分布外样本的脆弱性。

## Are Attribute Inference Attacks Just Imputation?

- 研究问题：敏感值推理，一种属性推理攻击问题，攻击者知道部分训练数据并可接触由其训练出的模型，最终推理出这些训练数据未知的敏感信息，比一般属性推理攻击要求精度更高，属性值的敏感度也更高。
- 相关工作：
- 解决方案：将属性推理与数据归因在多种攻击者可获得训练数据的假设下进行了详尽的对比实验。
- 实验效果：目前的属性推理方法并不比只知道训练数据分布而不能接触模型的对抗方法好，黑盒属性推理难以学到任何不用模型就学不到的知识，但白盒攻击可以有效的识别某些不接触模型就识别不到的敏感属性值。

## Private and Reliable Neural Network Inference

- 研究问题：神经网络的可靠性和隐私保护问题
- 相关工作：可信神经网络和隐私保护神经网络目前基本没有联系起来。
- 解决方案：提出了第一个在可靠神经网络上进行隐私保护推理的系统，为搭建随机光滑的关键算法模块设计完全同态加密副本，难点在于完全同态加密所需的控制流限制。
- 实验效果：系统可以在没有明显延迟的情况下进行隐私保护推理。

## LPGNet: link private graph networks for node classification

- 研究问题：图神经网络上的数据窃取攻击，边窃取攻击。在不接触网络模型的前提下推理出现在训练图中的边是可行的。
- 相关工作：
- 解决方案：提出了新的网络架构LPGNet在有隐私敏感的边的图上训练图神经网络模型，通过新的图结构使用方法为边的使用提供差分隐私保证。
- 实验效果：实验结果说明LPGNet相较不使用边信息的隐私保护架构有更好的可用性，相较使用全图结构的GCN在面对边窃取攻击时更鲁棒。相较SOTA隐私保护GCN而言有更好的可用性和隐私保护的平衡。

## Order-Disorder: imitation adversarial attacks for black-box neural ranking models

- 研究问题：神经通路排序模型上的对抗攻击问题，神经文本评分模型继承了普通神经网络对于对抗攻击的脆弱性，这种脆弱性可能会被黑帽SEO。
- 相关工作：目前工作仅检测出了对于对抗的脆弱性，但没有解决。
- 解决方案：发现目标模型可以通过枚举关键的查询或训练数据然后在模仿模型上训练获得，使用模仿模型可以操作排序结果并将操作攻击转移到目标模型上。通过给目标函数新增下一句预测损失和语言模型流畅度约束来为目标模型添加伪装
- 实验效果：实验证明了攻击的有效性，伪装的有效性。

## StolenEncoder: stealing pre-trained encoders in self-supervised learning

- 研究问题：预训练编码器的版权保护问题。
- 相关工作：
- 解决方案：提出了第一个窃取预训练图像编码器的攻击方法StolenEncoder。
- 实验效果：在多个数据现实世界的预训练编码器上测试，证明窃取的编码器与目标编码器有着类似的功能。

## LoneNeuron: a highly-effective feature-domain neural trojan using invisible and polymorphic watermarks

- 研究问题：DNN的木马攻击问题
- 相关工作：当前大部分工作关注于将木马植入数据中，而植入代码和植入模型的方法才刚刚开始研究
- 解决方案：提出了一个新的模型木马植入方法LoneNeuron，响应转换为不可见的指定样本的多带像素域水印的特征域模式。因为独有的水印多态特性，同一个特征域的触发词可以被分配到像素域的多个水印中，进一步提升水印的随机性，隐蔽性和稳定性
- 实验效果：使用高度的攻击独特性，LoneNeuron达到了100%的攻击成功率。实验说明LoneNeuron可以绕过SOTA的木马检测，并且是ViTs的最有效攻击者。

## EIFFeL: Ensuring Integrity for Federated Learning

- 研究问题：联邦学习的完整性问题，因为更新梯度的聚合是加密完成的，恶意植入的木马就无法被发现。
- 相关工作：
- 解决方案：形式化确定了保证更新隐私性和完整性的问题，并提出了一个新系统EIFFeL来加强更新梯度的安全聚合。EIFFeL是一个可以加强任意完整性检查并移除恶意梯度，同时不违反隐私性的通用框架。
- 实验效果：在100个client和10%的恶意植入环境下，EIFFeL可以以2.4s一次迭代的情况下训练一个MNIST分类器达到没有恶意植入一样的完整性。

## Finding MNEMON: reviving memories of node embeddings

- 研究问题：图神经网络模型的数据窃取攻击问题
- 相关工作：当前工作主要关注隐藏图上信息或者理解图神经网络的安全性和隐私问题，没有关注上游模型和下游任务pipeline整合时的隐私风险。
- 解决方案：提出了新的模型未知时的图复原攻击，探索保存在图节点嵌入中的图结构信息。
- 实验效果：对抗攻击可以在仅接触节点嵌入矩阵而不接触图模型的情况下复原边的信息。

## Truth Serum: poisoning machine learning models to reveal their secrets

- 研究问题：数据窃取攻击问题
- 相关工作：
- 解决方案：提出了一类新的攻击方法，说明可以毒害数据的对抗攻击可以让在此数据上训练的模型泄露属于第三方的训练数据中的隐私信息。方法将完整性攻击和隐私攻击两者联合了起来。
- 实验效果：攻击方法可以在只毒害<0.1%的数据下将推理攻击的效果增大1到2个数量级。掌握了显著比例的训练数据的对抗攻击方可以使用无目标攻击，向所有其他用户的数据进行8x精度的推理。

## Group Peoperty Inference Attacks Against Graph Neural Networks

- 研究问题：图神经网络模型上的特性推理攻击，在与GNN接触过程中推理训练图上的敏感特性。
- 相关工作：已有工作主要探索图级别的特性（节点度数，图密度），不关注节点和边的群组的特性。
- 解决方案：首先根据对手知识类型将攻击模型分类，然后设计了六种攻击方式，。然后分析了攻击成功的底层原因
- 实验效果：使用和不使用有目标特性的图训练GNN模型会导致模型参数和输出的不同，从而帮助对抗方推理特性是否存在

## Enhanced Membership Inference Attacks against Machine Learning Models

- 研究问题：成员推理攻击，量化评估机器学习方法泄露了多少训练数据及原因
- 相关工作：
- 解决方案：提出了一个理解假设测试框架来统一解释已有工作，并设计新的推理攻击方法。提供了一个不可分辨博弈的模板，请提供了博弈中不同实例的攻击成功率的解释。
- 实验效果：方法试图将攻击的不确定性缩小到某个特定的数据节点的出现和消失上，并对不同类型的攻击进行了区别分析，展示了数据节点面对某种攻击脆弱的原因。
