# opening the black box of deep neural networks via information

## 主要结论

1. 大部分训练批次被用于将输入数据补全为有效的表示，而不是拟合训练标签

2. 表示补全过程从训练误差变小，随机梯度下降批次从快速下降到小训练误差，进入受训练误差限制的随机扩散时开始

3. 层的收敛非常接近信息瓶颈理论限制，并且从输入到任意隐层及隐层到输出层的映射满足信息瓶颈的self-consistent等式

4. 加入更多隐层时训练时间会急剧下降，因此隐层的最大优势是计算效率


