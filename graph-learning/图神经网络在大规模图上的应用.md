图神经网络在大规模图上的应用

研究问题是什么

本文关注的论文研究图神经网络在大规模图数据上的应用方式，具体而言是如何解决领域爆炸问题导致的高额计算和存储消耗，并在有上百万甚至更大规模网络上有效进行图神经网络的训练和预测。

为什么要研究这个问题

传统图挖掘方法以图结构为基本内容，随着机器学习和深度学习的发展，如何使图中的信息有效地转换成深度学习方法可以识别的输入成为了图挖掘领域的总要问题。随着DeepWalk【】和Node2Vec等网络嵌入工作的出现，图表示学习逐渐成为了解决上述问题的主要手段。图表示学习旨在为图及其中节点在嵌入空间中学习对应的向量表示，这些向量表示随后可以与其他机器学习方法相结合，应用于诸多下游任务，包括节点分类【】，链接预测【】，社团发现【】，图匹配等传统图挖掘人物以及推荐系统【】，问答系统【】和分子结构预测等其他领域任务。以图卷积网络【】和图注意力网络【】等为代表的图神经网络模型，可以有效的结合图中的结构信息和节点属性信息，相较主要考虑网络结构信息的网络嵌入方法有着更强的信息表达能力，从而成为了图表示学习中的主要方法。

经过近几年的发展，许多图神经网络已经具备比图卷积和图注意力操作更加复杂的神经网络结构，但整体而言（大部分）图神经网络仍然遵循信息传播结构。在每层网络中，图神经网络模型首先通过一个聚合函数从邻居节点中获得特征，然后在使用一个更新函数将邻居节点的特征与自身特征相结合，作为更新后的节点特征，公式化描述【】为

$$
a_v^{(k)}=AGGREGATE(\{h_u^{(k-1)}:u\in \mathcal{N}(v)) \}),\\
h_v^{(k)}=COMBINE^{(k)}(h_v^{(k-1)}, a_v^{(k)})
$$

其中$AGGREGATE(\cdot)$是聚合函数，$COMBINE(\cdot)$是更新函数，不同图神经网络中的聚合函数和更新函数各有不同。有一些提出的工作已经在考虑摆脱消息传递结构的限制，设计有更强表达能力的图神经网络，但目前大部分模型仍旧遵循此结构。

虽然已经有很多图神经网络相关的研究，但在大规模网络上训练并使用图神经网络模型依然存在很大挑战。因为在消息传递结构中网络层首先要从邻居节点获取特征，所以随着网络层数的增加，需要处理的节点数量会呈指数级增长，这个现象称为邻域爆炸。领域爆炸会导致训练图神经网络的计算资源和存储资源消耗随着网络规模的增大而快速增加，导致在大规模网络上无法进行训练。

【Cluster-GCN里面的邻域爆炸示意图】

近几年图神经网络在大规模网络上的应用问题逐渐引起了关注，有很多相关方法被提出。本文从解决领域爆炸的方法入手，对已有方法进行分类阐述，并对方法的优缺点进行描述，最后给出自己认为的潜在方向和总结。

有哪些工作在研究这个问题

从2017年GraphSAGE【】使用采样方式解决在大规模网络上的训练问题开始，已经有近30篇论文对大规模图数据上的图神经网络训练和应用方式进行了研究，根据解决邻域爆炸的方法不同，可以大致将已有方法分为基于采样的方法，基于随机游走的方法和其他方法。

基于采样的方法通过设计采样策略对领域节点进行采样的方式避免聚合整个网络的节点数据，将计算范围限定在相对较小的区间内，从而有效限制了计算和存储资源的剧烈消耗。Li等人【generalized guarantee】提出了对最多三层的GCN在进行半监督节点分类任务上使用图结构采样训练时的理论分析，描述了图结构采样对泛化性能和样本复杂度的影响。根据采样策略的不同，可以将基于采样的方法细分为节点级采样方法，层级采样方法和子图级采样方法。节点级采样方法以网络中节点为对象，在每次训练时为每个节点采样一定数量的领域节点进行聚合。GraphSAGE【】在进行批次训练时，根据上一层计算的节点均匀地采样一个固定大小的邻域节点，而不是使用全部节点。Chen等人【stichastic training】设计了一种基于变量控制的采样方法VR-GCN，利用每个节点的历史计算结果作为近似。在对节点当前训练循环的梯度进行蒙特卡洛采样得到采样结果后，与历史计算结果结合作为本轮的领域节点特征。zeng等人【decoupling the depth】将图神经网络的深度和感受域进行解耦，在单个节点的局部子图上学习对应的节点表征，从而限制节点的邻域范围。针对节点级采样方法需要为每个节点进行采样导致的效率低下问题，层级采样方法一次性为单个层采样所有需要更新的节点，效率相对更高，并且同样可以有效限制每层的计算和存储消耗。FastGCN【】设计一种重要性采样法，根据节点重要性分布独立采样每层网络需要处理的节点范围。Huang等人【Adaptive sampling】在FastGCN基础上设计了一种更有效的自适应采样方式，由上至下地构建神经网络，并根据前一层的节点一次性采样下一层需要处理的节点。为解决层级采样造成的稀疏连接问题，Zou等人【】设计了一种层间依赖的的重要性采样方法LADIES，选择前一层的采样节点的邻居节点构成二项图，并在其上计算节点重要性，最终根据重要性进行层级采样。子图级采样方法从网络中采样出若干子图，然后用子图代替完整的大规模网络进行训练，通过限制整体样本数量的方式降低计算和资源要求。LGCL【】通过从初始节点开始的广度优先遍历来采样若干子图，并将这些子图组成一个批次的数据进行训练。Cluster-GCN【】通过图聚类算法将网络分为多个紧密连接对子图，再在每个批次使用一个子图进行训练更新。节点级采样方法可以避免输入过大的批次数据，从而降低计算量，并缓解内存消耗。GraphSAINT【】在每轮训练开始前通过随机游走等方式采样出一批子图，每个批次利用其中一个子图进行训练。采样结果也被应用于修正前向传播和反向传播的结果，从而减少采样引入的偏差。MVS-GNN【】对采样方差进行分析，提出抽样方法的方差都可以分解为在前向阶段的嵌入近似方差和反向阶段的随机梯度方差，并设计了一个双层循环的训练方式。每轮训练首先随机采样一个大批次的样本，更新并计算梯度，随后在嵌套的内层循环中根据梯度在大样本中进行采样，对节点表征再做一次训练并对梯度进行再次更新。

基于随机游走的方法在图表示学习中得到了广泛运用。通过从目标节点开始的随机游走，可以获得若干条不完全相同的游走路径，其中节点的出现频率可以反映节点之间的结构紧密程度。尽管随机游走也可以视作为一种采样技术，但基于随机游走的方法往往并不在构建计算图或者训练过程中进行采样，而是将传播与更新步骤进行解耦。通过预处理方式完成邻居节点的传播过程，再单独进行更新函数和分类器的训练。首先利用与公式【】类似的PageRank矩阵预计算出特征的传播结果：

$$
P=\sum_{l=0}^Lw_lT^{(l)}=\sum_{l=0}^Lw_l(D^{r-1}AD^{-r})^l\cdot X
$$

，然后再在其上训练多层感知机等分类器：

$$
Y=\sigma(\Phi(W;P))
$$

，训练过程利用minibath等方法完成。由于传播过程在预计算中离线进行，最终训练时不需要考虑邻域节点的特征信息，所以可以每个节点单独作为一个样本进行训练，避免了训练中的存储消耗。

SGC【】首先提出通过移除非线性层和合并权重矩阵来降低计算复杂度，并通过预计算传播矩阵的方式完成传播过程。SIGN【】将传播矩阵进一步细化，使用多个线性传播算子$A_1,...,A_r$，每个算子独立对结果进行预测，并最终进行综合输出结果。PPNP和APPNP方法【】将预测放在传播之前，首先使用神经网络模型对每个节点进行预测，然后利用带重启的个性化PageRank矩阵对预测结果进行传播。通过在随机游走中允许节点游走回自身的概率限制传播的邻域范围，避免邻域爆炸。类似思路也出现在GCNII模型【simple and deep】中，GCNII模型通过在每层传播中使用初始残差和恒等映射来解决过平滑问题，但相较APPNP有更多的待训练参数。【Birirectional Propagation】通过随机游走方式对传播矩阵进行采样近似，提升计算效率，并通过反向传播所见采样方差。PPRGo算法【】也遵循了使用稀疏矩阵对随机游走矩阵进行近似的想法，并对PPNP进行了改进，设计了类似信息传播的近似矩阵预计算方法。

其他方法包括使用分布式训练【EXACT】,激活压缩【PIPEGCN】，信息记忆等。基于信息记忆的方法可以在不丢弃样本的情况下完成大规模网络上的训练，同时在近两年有相对较多的工作出现，因此本文主要对基于记忆的方法进行总结。

基于采样的方法虽然在许多数据集上表现出了和全图训练相媲美的能力，但是依然有丢失信息的风险。基于随机游走的方法在预计算传播矩阵时会有较高的计算消耗。基于信息记忆的方法可以在不进行采样和预计算的前提下完成大规模网络上的GNN训练，因此在近期获得了较多关注。通过对信息的记忆方式不同，基于信息记忆的方法可以大致分为直接记忆式方法和摘要记忆式方法。直接记忆式方法通过存储历史学习到的嵌入信息，然后在新一轮学习时进行利用的方式避免进行采样。在Chen等人【stochastic learning】的方法中已经出现过类似的应用，利用历史嵌入表征作为当前批次邻域节点的近似表征。IGLU【】方法利用懒惰更新技术，使用相邻两层间的表征自顶向下计算梯度更新权重，从而避免采样和邻域爆炸。GNNAutoScale【】通过把上一轮训练得到的嵌入结果保存下来，然后在这一轮训练时将批次外的相邻节点的嵌入用上一轮的结果顶替的方式进行更新，从而保证了在不丢弃任何数据的前提下达到常量大小的内存消耗。GraphFM【】在GNNAutoScale基础上通过动量更新的方式对历史嵌入进行更新，避免小批次训练导致的历史嵌入时间过久信息不足情况。直接记忆式方法不同，摘要记忆式方法并不直接保存节点的历史嵌入信息，而是通过将输入信息或训练结果综合成摘要信息的方式保存，从而拥有更低的存储消耗。Huang等人【graph coarsening】通过图粗化方式将原图进行压缩，缩小学习规模，然后在粗化后的图上进行训练。因为数据规模得到了缩小，甚至可以支持在粗化后的图上进行全图训练，所以没有必要进行采样。VQ-GNN【】通过学习和更新一小批量化参考向量的形式保存所有传递至训练批次的信息。训练时不进行采样，而是通过从量化参考向量包含的全局信息中提取邻域信息的方式进行更新，从而避免邻域爆炸问题。总之，基于信息记忆的方法可以在避免丢失信息的前提下解决邻域爆炸问题，但需要消耗一定的额外存储资源。

> 这里是未经总结的论文列表
> 
> 2022
> 
> ---
> 
> - [x] [ICML 2022] Generalization Guarantee of Training Graph Convolutional Networks with Graph Topology Sampling. [[paper](https://arxiv.org/abs/2207.03584)]：图卷积网络近期在图结构数据学习上获得了很大的成功。为了解决其因为递归嵌入邻居特征导致的可延展性问题，图结构采样技术被采用来降低训练GCN的存储和计算消耗，并且已经获得了和非采样方法媲美的效果。本文在半监督节点分类任务下，第一次提出了对最多三层的GCN在进行图结构采样训练时的理论分析。本文正式指出了一些GCN训练时可以减少泛化误差的必要条件。同时，本文方法解决了在不同层之间权重的非凸交互。本文明确描述了图结构和拓扑采样对泛化性能和样本复杂度的影响，并且通过数值试验验证了理论的正确性。
> 
> - [x] [ICLR 2022] PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication. [[paper](https://openreview.net/forum?id=kSwqMH0zn1F)] [[code](https://github.com/RICE-EIC/PipeGCN)]：图卷积网络是学习图结构数据的最先进的方法之一，训练大规模的GCN需要在多个加速器上进行分不训练，每个加速器保留一部分子图。分布式的GCN训练面对着传递节点特征和为每个GCN层在每次训练迭代时传递特征梯度的高昂开支，限制了可达到的训练效率和模型延展性。因此本文提出PipeGCN，一个简单有效的方法，可以通过子图划分内部计算输送划分间沟通的方式来隐藏沟通开销。利用管道进行GCN训练是不简单的，因为通信的节点特征和梯度可能滞留从而损害收敛性。关于具有滞留特征和特征梯度的GCN训练的收敛速度的分析很少。本文提供了理论上的收敛性分析，并且发现PipeGCN的收敛速度接近于vanilla分布式GCN训练的收敛速度，且没有任何滞留。本文还提出了一种平滑方法来进一步提高PipeGCN的收敛性。实验表明可以大大提高训练吞吐量
> 
> - [x] [ICML 2022] GraphFM: Improving Large-Scale GNN Training via Feature Momentum. [[paper](https://arxiv.org/abs/2206.07161)]：为大规模节点分类任务训练GNN是很困难的，其中一个挑战是在避免领域爆炸问题的前提下得到准确的隐层节点表示。本文提出了一种称为特征动量的**新技术**，使用一个动量步骤在更新特征表示时结合历史嵌入。本文提出两种方法，分别考虑批次内和批次外的数据，并为GraphFM-IB进行了收敛分析，为GraphFM-OB做了一些理论分析。GraphFM-IB可以有效缓解已有方法的邻域爆炸问题，GraphFM-OB获得了不错的实验效果。
> 
> - [x] [ICLR 2022] EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression. [[paper](https://openreview.net/forum?id=vkaMaq95_rX)] [[code](https://github.com/warai-0toko/Exact)]：在大规模网络上训练GNN因为高存储消耗很难，主要由于activations（比如节点嵌入）占用。已有工作主要集中于削减存储中的节点数量。同时，使用压缩激活图对GNN进行训练的探索较少。主要由于在常见的图学习工具包中缺少必要的工具。本文提供了一个优化的GPU实现，支持使用激活压缩对GNN进行训练。基于此本文提出了一个内存高效的训练框架EXACT，首次使用压缩激活来训练GNN，并系统地分析了内存节省，时间开销，精度下降之间的权衡。EXACT可以将激活的内存占用减少多达32倍，时间开销减少10-25%，而准确率仅下降0.2-0.5%。在实践中，EXACT可以将在ogbn-products上训练三层全批次GraphSAGE的硬件要求从128GB GPU降到 48GB GPU。
> 
> - [x] [ICLR 2022] IGLU: Efficient GCN Training Via Lazy Updates.：
> 
> 2021
> 
> ---
> 
> - [x] [NeurIPS 2021] Decoupling the Depth and Scope of Graph Neural Networks. [[paper](https://openreview.net/forum?id=_IY3_4psXuf)] [[code](https://github.com/facebookresearch/shaDow_GNN)]：本文提出了一种设计原则来解耦 GNN 的深度和范围——为了生成目标实体（即节点或边）的表示，首先提取一个局部子图作为有界大小范围，然后在其上使用一个任意深度的GNN。 正确提取的子图由少量关键邻居组成，同时排除不相关的邻居。 GNN，无论它有多深，都会将局部邻域平滑为信息表示，而不是将全局图过度平滑为“白噪声”。 从理论上讲，解耦从图信号处理（GCN）、函数逼近（GraphSAGE）和拓扑学习（GIN）的角度提高了GNN的表达能力。 根据经验，在七个图（最多 1.1 亿个节点）和六个主干 GNN 架构上，本文的设计实现了显着的精度提高，计算和硬件成本降低了数量级。
> 
> - [NeurIPS 2021] VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization. [[paper](https://arxiv.org/abs/2110.14363)] [[code](https://github.com/devnkong/VQ-GNN)]：基于采样的方法很难应用于每层利用多跳或者全局上下文的GNN，对于不同的任务和数据集表现出不稳定的性能，并且不能加速模型推理。本文提出了一种原则性且根本不同的方法VQ-GNN，使用矢量量化在不影响性能的情况下扩展任何基于卷积的GNN。可以在每个GNN层中使用VQ学习和更新少量全局节点表示的量化参考向量来有效地保留传递给小批量节点的所有信息。使用VQ表示与卷积矩阵的低秩版本相结合，避免了GNN的邻居爆炸问题。
> 
> - [x] [ICLR 2021] Combining Label Propagation and Simple Models Out-performs Graph Neural Networks. [[paper](https://arxiv.org/abs/2010.13993)] [[code](https://github.com/CUAI/CorrectAndSmooth)]：使用标签传播加上简单的MLP模型可以达到当前最优的GNN的效果。
> 
> - [x] [KDD 2021] Scaling Up Graph Neural Networks Via Graph Coarsening. [[paper](https://arxiv.org/pdf/2106.05150.pdf)] [[code](https://github.com/szzhang17/Scaling-Up-Graph-Neural-Networks-Via-Graph-Coarsening)]：利用图粗化（提取超图）解决在大规模网络学习时节点的接受域太大导致的随机优化策略低效的问题。使用图粗化缩小学习规模，然后小规模图上学习分类器权重。
> 
> - [x] [ICML 2021] GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings. [[paper](https://arxiv.org/abs/2106.05609)] [[code](https://github.com/rusty1s/pyg_autoscale)]：通过把上一轮训练得到的嵌入结果保存下来，然后在这一轮训练时将batch外的相邻节点的嵌入用上一轮的结果顶替的方式进行。从而保证了在不丢弃任何数据的前提下达到常量大小的内存消耗。
> 
> 2020
> 
> ---
> 
> - [x] [ICLR 2020] GraphSAINT: Graph Sampling Based Inductive Learning Method. [[paper](https://arxiv.org/abs/1907.04931)] [[code](https://github.com/GraphSAINT/GraphSAINT)]：GraphSAINT通过采样训练图而不是点或边的方式构建最小训练批次。每次迭代都通过采样的子图构建一个完整的GCN，从而保证在所有层中都有固定数量的结构良好的节点。本文提出从前向和反向传播中解耦采样的方法，从而将GraphSAINT应用于多个结构变体中。
> 
> - [x] [KDD 2020] Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks. [[paper](https://arxiv.org/abs/2006.13866)] [[code](https://github.com/CongWeilin/mvs_gcn)]：现有的采样方法大多基于图结构信息，忽略了优化的动态性，导致估计随机梯度的方差很大。在非常大的图中，高方差问题可能非常明显，这会导致收敛速度慢和泛化能力差。本文从理论上分析了抽样方法的方差，并表明，由于经验风险的复合结构，任何抽样方法的方差都可以分解为在前向阶段的*嵌入近似方差*和反向阶段的*随机梯度方差*，需要减轻这两种类型的方差以获得更快的收敛速度。本文提出了一种解耦方差减少策略，该策略采用（近似）梯度信息来自适应地对具有最小方差的节点进行采样，并显式减少嵌入近似引入的方差。本文从理论上和经验上表明，与现有方法相比，即使小批量较小，所提出的方法也具有更快的收敛速度和更好的泛化能力。
> 
> - [x] [KDD 2020] Scaling Graph Neural Networks with Approximate PageRank. [[paper](https://arxiv.org/abs/2007.01570)] [[TensorFlow](https://github.com/TUM-DAML/pprgo_tensorflow)] [[PyTorch](https://github.com/TUM-DAML/pprgo_pytorch)] [[web](https://www.in.tum.de/daml/pprgo/)]：许多已有的可扩展GNN方法依赖于昂贵的消息传递过程来传播特征。本文提出PPRGo模型，利用GNN中信息扩散的有效近似来显著加速GNN同时保持最先进的预测性能。PPRGo具有可扩展性，并且可以并行处理大型数据集。本文证明了在分布式和单机训练环境中的有效性。
> 
> - [x] [ICML Workshop 2020] SIGN: Scalable Inception Graph Networks. [[paper](https://arxiv.org/abs/2004.11198)] [[code](https://github.com/twitter-research/sign)]：本文提出了一种新的，高效且可扩展的深度图学习架构，通过使用不同大小的图卷积滤波器来避免图采样，滤波器可以进行高效的与计算，从而实现极快的训练和推理。本文架构支持使用不同的局部图算子来适配任务，实验证明在大规模网络上可以取得最先进结果
> 
> - [x] [ICML 2020] Simple and Deep Graph Convolutional Networks. [[paper](https://arxiv.org/abs/2007.02133)] [[code](https://github.com/chennnM/GCNII)]：解决图卷积网络的过平滑问题，过平滑问题由拉普拉斯矩阵的k阶近似的固定系数导致，会导致k层图卷积网络的输出逼近固定分布。通过引入残差网络和点对点映射的方式来解决，残差网络保证输出保留了一定比例的原始信号，点对点映射保证网络学习下限是原始信号。提出了将图卷积网络的激活层去除的做法，使大规模网络的邻接矩阵乘法可以提前进行线下计算，最终只用学习一个分类器网络。
> 
> - [x] [NeurIPS 2020] Scalable Graph Neural Networks via Bidirectional Propagation. [[paper](https://arxiv.org/abs/2010.15421)] [[code](https://github.com/chennnM/GBP)]：通过随机游走方式进行蒙特卡洛采样近似高阶邻接矩阵，然后通过双向传播减少蒙特卡洛采样的方差，最后结合蒙特卡洛和双向传播的结果
> 
> 2019
> 
> ---
> 
> - [x] [ICLR 2019] Predict then Propagate: Graph Neural Networks meet Personalized PageRank. [[paper](https://arxiv.org/abs/1810.05997)] [[code](https://github.com/benedekrozemberczki/APPNP)]：为了对节点进行分类，信息传播方法只考虑距离传播几步之遥的节点，并且这个利用的邻域的大小很难扩展。 本文利用图卷积网络（GCN）和 PageRank 之间的关系推导出基于个性化 PageRank 的改进传播方案，并利用这种传播过程来构建一个简单的模型，即神经预测的个性化传播 (PPNP) 及其快速近似值 APPNP。 本文的模型的训练时间与以前的模型相当或更快，其参数数量与以前的模型相同或更低。 它利用一个大的、可调整的邻域进行分类，并且可以很容易地与任何神经网络结合。
> 
> - [x] [KDD 2019] Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks. [[paper](https://arxiv.org/abs/1905.07953)] [[TensorFlow](https://github.com/google-research/google-research/tree/34444253e9f57cd03364bc4e50057a5abe9bcf17/cluster_gcn)] [[PyTorch](https://github.com/benedekrozemberczki/ClusterGCN)]：当前基于SGD的方法要么面临高计算成本，要么面临保持整个图和每个节点的嵌入在内存中的高存储成本。本文提出Cluster-GCN，利用图聚类结构，并适用于SGD训练。方法在每一步从与聚类算法识别的密集子图中采样节点集合，从而提高计算和存储效率。实验证明可以在没有太多时间和内存开销的情况下训练更深的GCN，从而提高预测精度。
> 
> - [x] [ICML 2019] Simplifying Graph Convolution Networks. [[paper](https://arxiv.org/abs/1902.07153)] [[code](https://github.com/Tiiiger/SGC)]：GCN可能包含一些不必要的复杂度和冗余计算，本文通过移除非线性层和合并权重矩阵来削减运行复杂度。本文通过理论分析说明模型等同于一个固定的低通滤波器接一个线性分类层，实验证明简化操作在许多任务上并没有影响精确度，并且可以应用于大型数据集。
> 
> - [x] [NeurIPS 2019] Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks. [[paper](https://arxiv.org/abs/1911.07323)] [[code](https://github.com/acbull/LADIES)]：全批次训练要求在每一GCN层都计算所有节点的表示，导致高计算和存储消耗。节点级的采样方法递归地采样固定数量的邻居节点，因此计算消耗受指数级增加的邻居节点数量影响。层级的采样忽略了邻居依赖，因此受稀疏连接影响。本文提出了一个新采样方法，根据上层的采样节点，选择相邻节点构成二项子图，并在其上计算节点重要性，然后再根据重要性采样固定数量节点。最后每层都进行一遍来构建整体计算图，从理论和实验上证明了方法的有效性。
> 
> 2018
> 
> ---
> 
> + [x] [ICLR 2018] FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling. [[paper](https://arxiv.org/abs/1801.10247)] [[code](https://github.com/matenure/FastGCN)]：GCN原本是设计为在训练和测试数据都有的情况下学习的。领域爆炸问题导致难以在大规模和高密度网络上训练。为了放宽测试数据同时可用的要求，本文将图卷积解释为概率测量下嵌入函数的积分变换，这种解释允许使用蒙特卡洛方法来一致地估计积分，本文由此提出FastGCN。通过重要性采样，GastGCN不仅对训练有效，对推理也有很好的泛化性。
> 
> + [x] [KDD 2018] Large-Scale Learnable Graph Convolutional Networks. [[paper](https://arxiv.org/abs/1808.03965)] [[code](https://github.com/divelab/lgcn)]：CNN计算需要固定数量的有序单元，但在普通图中邻居节点无序且数量不固定。本文解决这个问题，提出了可学习的图卷积层，图卷积层根据排序值为每个特征自动选择一个固定数量的邻居节点，从而将图数据转换为1d的网格数据，然后在其上使用常规卷积操作。为了使模型可以在大规模网络上训练，本文提出了一个子图训练方法来削减训练存储和计算资源消耗。
> 
> + [x] [ICML 2018] Stochastic Training of Graph Convolutional Networks with Variance Reduction. [[paper](https://arxiv.org/abs/1710.10568)] [[code](https://github.com/thu-ml/stochastic_gcn)]：GCN通过迭代地获取邻居节点信息来计算节点表示，导致感受域的规模随着层数增加指数级增大。已有通过采样削减感受域的方法无法保证收敛，并且每个节点的感受域依然有上百个节点。本文提出了基于控制变量的方法，允许对任意小的邻居大小进行采样。并且证明了算法收敛到GCN的局部最优值。实验结果证明方法与每个节点仅使用两个邻居的精确算法有相似的收敛性，并且运行时间仅为原来采样方法的七分之一。
> 
> + [x] [NeurIPS 2018] Adaptive Sampling Towards Fast Graph Representation Learning. [[paper](https://arxiv.org/abs/1809.05343)] [[code](https://github.com/huangwb/AS-GCN)]：GCN已经是学习图节点表示的重要工具。将GCN应用于大规模网络上的一个主要挑战是可延展性问题，再多层之间由于领域爆炸导致的高计算和存储消耗。本文使用一种可适应的层级采样方法来加速GCN训练。通过从上到下逐层搭建网络的方式，本文根据顶层网络对底层网络进行采样，采样的邻居节点对不同的父节点之间共享，由于采样固定规模，所以爆炸现象得以避免。本文提出一种新方法，通过应用skip连接来促进消息在远距离节点上传播。实验证明了方法有效性，并同时有更快的收敛速度。
> 
> 2017
> 
> ---
> 
> + [x] [NIPS 2017] Inductive Representation Learning on Large Graphs. [[paper](https://arxiv.org/abs/1706.02216)] [[code](https://github.com/williamleif/GraphSAGE)]：GraphSAGE，最开始通过采样方式（除了DeepWalk和Node2Vec）来在大规模网络上训练GNN的模型

你为什么要去研究这个问题
