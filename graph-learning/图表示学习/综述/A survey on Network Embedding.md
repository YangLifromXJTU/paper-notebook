# Graph Embedding综述

>为第二个点努力，追赶时代潮流

网络嵌入（Network Embedding）将网络中的节点映射成低维向量表示并且有效保留网络结构。  
在搞这个的时候首先要摒弃原来的关于“边”的概念，点还是那个点，但边不是那个边了。

网络表示学习，首先就得明白这个网络表示是什么意思，就我个人理解，网络表示就是我们是用什么方式来说明这个网络的。

原来是用点和边的形式，对每一个点，我们用了一个向量或者说边的列表或者说邻接矩阵的一行来表示它在这个网络中的关系，也就是它和哪些节点相连。这就是说我们是用**边的形式**来表示这个网络的。

网络嵌入就是用一个向量来表示节点在网络中的关系。需要明白的是这个向量和网络结构没有必然联系，也就是说是把网络映射到了一个新的空间中。在这个空间中没有边，只有点，关系用点和点之间的距离表示。

## Graph Embedding是要解决什么问题

原来表示网络的方式是用边，也就是邻接矩阵。在网络规模大了之后这个数据量爆炸，而且传统的根据边的方式导致很多进行网络分析的方法需要有一个循环或者迭代的过程，因为要顺着边去找其他的节点，通过每个节点能接触到的只有和它相邻的节点，其他的节点就需要迭代或者循环，就很费时间。

其次用边表示的网络也无法灵活使用，因为对一个节点的改变会影响其邻居节点的表示。但是用嵌入的方式整成一个向量之后就可以并行化了，因为任意两个节点之间的关系可以由这两个节点直接得到，节点的改变也不会对其他节点的向量表示产生影响。

最后就是用边来表示的网络没法使用机器学习方法，但是用嵌入向量的方式就可以了，原因还是上面这个。因为机器学习方法需要被处理的数据可以表示成独立的向量，而用边或者邻接矩阵表示的并不是独立的。

## Graph Embedding是要干什么

简单来说，graph embedding利用节点在嵌入空间中的距离远近来表示节点在原网络中的某个角度上的相似程度，这个相似程度可以是距离，可以是社团从属关系，可以是邻居的构成。

Graph Embedding感觉起到了两种作用

+ 第一，graph embedding可以起到一个降维的作用，通过将节点表示成低维向量的形式，可以有效地缩小需要处理的数据维度
+ 第二，graph embedding可以把每个节点变成一个独立的个体*我总感觉这条有大用*

但这个东西并不是用来搞社团发现的，这个必须说明，这个算是一个数据预处理的技术。

那要这么说，就要保证两点

+ 第一，用边表示的图和用向量表示的图得是一个图，就是这两种表示要可以互相转化，embedding是从边转换成向量，转换要有效就必须要保证得到的结果可以再转换回去
+ 第二，在边表示的图中成立的关系在向量表示的图中也得成立，用边表示的图中有团，有三角，这种关系在向量空间中也要成立。

## Graph Embedding技术

以下分类根据graph embedding要在嵌入空间中保留什么信息产生

+ 保留结构和网络特点的network embedding
  >保留节点和边的信息的，保留邻居信息的，保留高维邻居信息的，即根据某些结构上的特点来决定哪些节点在映射后的空间上比较接近。
  >还有比如三角接近度（network transitivity），网络平衡性之类的特性
  >还有一个研究方向是如何在属性级别对齐嵌入空间和网络空间
+ 使用额外信息的network embedding
  >使用节点内容，节点标签，点和边的属性，节点类型
+ 保留进阶信息的network embedding
  >根据不同的的具体任务（级联预测，异常检测，网络对其，协作预测...）设计具体的嵌入空间

常用的方法包括

+ 矩阵分解
    根据网络的邻接矩阵等矩阵，进行奇异值分解或者非负矩阵分解，得到矩阵的低维表示，达到降维的目的
+ 随机游走
    利用随机游走的方法得到节点前后会出现的节点，然后利用类似自然语言处理的词向量嵌入的方式得到每个节点的向量嵌入
+ 深度神经网络
    利用深度神经网络学习从网络空间到嵌入空间的映射函数，这个映射函数可以是非线性的，复杂的。
    在知道具体的嵌入任务的时候也可以设定具体的目标函数，执行端到端的嵌入

## 传统Graph Embedding技术

+ Isomap首先提出利用KNN等方法构建邻域图`G`，然后根据`G`可以计算任意两个节点之间的距离，从而得到网络中任意两个节点之间的距离矩阵。最后可以用多维缩放方法得到节点的向量，目的是使节点在嵌入空间上的距离和原图中的距离尽量相近。
+ Isomap的问题在于计算节点之间的距离具有很高的时间复杂度。局部线性嵌入（LLE）用于避免估计很远的节点之间的距离。LLE使用一个$W_{ij}$来表示节点`j`对节点`i`的贡献，然后得到可以最小化节点间的距离的`W`，然后在嵌入空间中也利用这个矩阵，得到节点的向量表示，使距离最小化。
+ 拉普拉斯特征映射（LE）首先构建邻域图，然后利用heat kernel确定任意两个节点之间的权重，最后使嵌入向量之间的差的和等于拉普拉斯矩阵分解的迹。

$$
\sum_{i,j}||u_i-u_j||^2W_{ij} = tr(U^TLU)
$$

这些方法主要的目的都集中在图重建上，即通过这些方法映射得到的向量，可以通过计算距离等方式重新在图空间将图构建出来。

## 保留结构和网络特点的Network Embedding

### 保留结构的Network Embedding

+ DeepWalk用于学习网络中的节点表示，保留节点的邻居信息。DeepWalk基于观察结果，即随机游走下网络中节点出现的次数分布和自然语言中单词出现的分布很接近。DeepWalk首先根据有截断的随机游走生成一些节点序列，然后根据Skip-Gram模型，最大化邻居节点出现在游走序列里面的概率。最后使用异构softmax得到嵌入向量。
+ Node2Vec发现DeepWalk不能抓到连接模式的多样性。Node2Vec使用了更宽松的邻域定义以及2阶的随机游走来取样邻居节点。Node2Vec可以将属于同一个社团的节点映射到更近的位置，也可以使角色相近的节点的嵌入向量相似。
+ LINE用于进行大规模网络潜入，可以保留一阶和二阶的距离信息。一阶距离是节点的相连关系，二阶距离是两个节点之间的结构相似程度。在嵌入空间里，一阶距离可以使用两个节点之间的联合概率分布度量

  $$
  p_1(v_i,v_j)=\frac{1}{1+exp(-u_i^Tu_j)}
  $$

  二阶距离可以使用节点j的向量由节点i生成的概率来度量

  $$
  p_2(v_j|v_i)=\frac{exp(u_j^Tu_i)}{\sum_kexp(u_k^Tu_i)}
  $$
  。基本思路是结构相近的节点具有相似的向量表示。通过分别最小化两个分布得KL散度和经验分布，就可以得到节点的表示。
+ GraRep和LINE相似，不过是把2阶扩展到了k阶，k阶可以用邻接矩阵的k次方来求得，矩阵中的元素代表两个节点之间距离为k的路径的数目。对每一阶的损失函数，使用奇异值分解得到对应的向量表示，然后再将所有向量集中（concentrate）就可以得到节点最后的全局表征。
+ 模块化非负矩阵分解（M-NMF）可以保留一阶距离，二阶距离和社团结构。使用非负矩阵分解得到节点间的相似度矩阵。社团结构用模块度最大化得到。基于节点的表征和社团的表征相似则节点很有可能属于社团的假设，使用了一个辅助社团表示矩阵来得到节点考虑了社团结构的表征。

以上的方法基本都用了浅模型，就是模型结构简单，且都假设网络空间和嵌入空间的对应关系是线性的。

+ SDNE提出了一个用于进行嵌入的深度模型，用于解决高非线性，保留结构和（邻接矩阵的）稀疏问题。使用多层的深度自编码器进行表征学习，保留节点的邻居信息。
+ Cao等人提出了一个捕获带权重的图结构的节点嵌入方法。使用了类似PageRank的随即冲浪模型。在随即冲浪模型的基础上，节点的表征可以通过结合转移概率矩阵来初始化得到。然后计算PPMI矩阵，最后使用堆叠去噪自编码器学习隐向量表示。
+ Chen等人提出了一个通用的网络嵌入框架。框架包含三个部分，节点距离函数$h(.)$，节点相似程度计算函数$g(.)$，以及和嵌入模板的距离函数$d(.,.)$。

还有一些方法用于学习整个图的表示，而不是每个节点的表示。
>后面补

### 保留结构特点的Network Embedding

+ Ou等人通过隐相似性分量保留非传递性。非传递性即三个节点中两两相似的关系并不能得到第三对节点相似。使用了一个线性映射矩阵计和来得到$M$哈希表格，每对节点可以得到$M$个相似度。然后最终的相似度由这些相似度聚合得到。最终将聚合相似度近似为语义相似度：如果两个节点的语义相似度交大，则哈希表中至少有一个相似度较大。
+ HOPE考虑了有向网络的不对称传递性。通过将四个度量方式结合到一个通用的公式里。然后使用奇异值分解得到低维表示，奇异值分解问题可以进一步发展为广义SVD问题，降低HOPE的时间复杂度。
+ SiNE用于对有标记网络进行嵌入，同时考虑了正向和负向边。负向边的节点的相似度应当比正向边的相似度低。SiNE使用了一个深度模型，包括两个使用非线性函数的深度网络，用来学习表征和保留网络的结构特性。关键问题是解决嵌入空间和网络空间之间的不对称性。

## 保留额外信息的Network Embedding

### 使用节点内容的Network Embedding

+ MMDW是利用节点的标签信息的半监督network embedding方法。MMDW使用基于DeepWalk的矩阵分解，采用支持向量机，并结合标签信息以找到最佳的分类边界。通过同时优化支持向量机的最大余量（max-margin）分类器和基于矩阵分解的DeepWalk，以学习具有更大判别能力的节点表征。
+ 是
