# 图的研究

> 要从什么角度来说呢，类别上有图的生成和挖掘，类型上有复杂网络，属性网络，异构网络，动态网络，还有处理单个网络和一系列网络的
> 还是要以问题为导向，看文章主要解决的是什么问题，每一类里面再按照图的类型进行划分，再下面再细分具体的问题，研究方法之类

## 生成图的研究

### 单个网络

#### 复杂网络

+ [ ] scalable deep generative modeling for sparse graphs：没太看懂，但是只考虑了生成图中有边的部分，从而降低时间复杂度。利用二叉树的方式为每个节点生成邻接向量，利用lstm估计在已有状态下节点会不会有左右子节点，最终的叶子节点就是根节点的相邻节点。剩下的没看懂

#### 属性网络

#### 异构网络

#### 动态网络

### 多个网络

#### 复杂网络

#### 属性网络

##### 图卷积变分自编码器-图变分对抗网络

Conditional Structure Generation through Graph Variational Generative Adversarial Network: 提出了条件结构生成问题，并使用图卷积神经网络构建了图变分生成对抗网络，用来生成具有灵活的属性-结构条件和排列不变性的属性网络

#### 异构网络

#### 动态网络

## 挖掘图的研究

### 单个网络

#### 复杂网络

##### 最小生成树发现

+ [ ] A fast minimum spanning tree algorithm based on K-means: 最小生成树发现，先用kmeans将网络分成若干个小网络，然后在小网络上发现最小生成树，最后把这些最小生成树连在一起

##### 社团发现

+ [ ] The Local Closure Coefficient_ A New Perspective On Network Clustering：定义了一个新的聚类紧凑程度指标，将二阶路径的头节点从中间节点换成两边节点
> 或许要看一下换过来的作用是什么，感觉还没看到指标的价值，或许可以和那个利用图神经网络进行神经网络结构探索的工作关联一下，因为那个用的是原来的clustering coefficient来描述搜索空间的
+ [ ] Community Detection using Diffusion Information：没太看懂，感觉看着挺像标签传播的，每个时间点只能观测到部分节点，节点的社团归属条件概率由相邻节点的归属来决定，预先指定了若干个cascade，然后观测节点在每个时间点出现在这个cascade，按照出现的向量表示来计算两个节点之间边的权重，最后用这个权重来决定节点社团归属
> 看了个大概，不知道这数据都是怎么获得的
+ [ ] Detecting Community Structures in Networks by Label Propagation with Prediction of Percolation Transition：将percolation translation和标签传播结合，延迟出现巨型社团的时刻
> 不懂percolation transition，感觉价值有限
+ [ ] Local spectral Clustering for overlapping community detection：通过对已知社团的节点进行随机游走对网络进行采样，得到一个子图，然后首先假设所有节点均匀属于每一社团，通过在子图的拉普拉斯矩阵上计算1到$l$跳邻居的数目来更新节点属于每个社团的概率，对概率向量进行拼接后求正交基，并通过与拉氏矩阵相乘的方式循环更新正交基直至收敛，得到最后重叠社团的划分结果。
> 一搞正交基我就懵了，感觉有点图嵌入那味了
+ [ ] near linear time algorithm to detect community structures in large-scale networks：经典LPA算法
+ [ ] 

##### 图信号处理

1. 综述
    + Graph signal process part I:

##### 网络表示学习

1. 图对抗网络
   + [ ] GraphGAN: graph representation learning with generative adversarial nets: 利用对抗生成网络结合图表示学习中的生成式模型和辨别式模型。GraphGAN假设所有节点的连接遵循一个先验分布，边是这个分布的已观测样本。方法包含两个模型，生成器用于拟合真实的链接分布概率，判别器用于区分well-connected和ill-connected的节点对，计算节点之间存在边的可能性。方法利用嵌入好的向量，并使用sigmoid函数利用嵌入向量计算节点之间的连通可能性。
   > 需要看代码是如何实现的
   + [ ] Learning deep network representations with adversarially regularized autoencoders: 大部分网络嵌入模型根据采样得到的节点序列学习嵌入向量，以使低维向量保留局部特征或者全局重构能力。但是因为采样序列的稀疏性，学习到的表示向量很难用于模型泛化。使用对抗正则自编码器同时考虑局部特征和全局重构约束。联合对立封装在对抗训练过程中，以规避显示先验分布的要求，从而获得更好的泛化性能。首先使用随机游走对节点进行采样，然后利用LSTM作为自编码器的编码器和解码器。利用自编码器学到的向量为正样本，从先验分布中利用生成器采样得到负样本，最后输入判别器进行分辨，以两个分布的EMD距离的双重性是作为距离度量。

2. 图自编码器
   + 

#### 属性网络

+ [ ]How neural networks extrapolate_from feedforward to graph neural networks: 神经网络如何进行推断，如何推断可视范围之外的样本。
> 综述文章，还没看

##### 网络表示学习

1. 图对抗网络

    + [ ] Deep graph infomax: 利用对抗神经网络解决以采样为基础的网络表示学习方法对于随机游走目标的依赖，并可用于迁移式和归纳式学习。利用图卷积自编码器为每个节点学习一个patch表征。同时以最大化局部互信息为自编码器学习目的，利用一个readout function将patch表征聚合为图级别的表征。同时通过负采样从图中获取负样本，然后在鉴别网络部分分别衡量正负样本的patch表征属于图级别表征对的概率，最后以最小化这个概率为目标进行训练。

    > 其实不太明白，卷积网络部分应该跟随机游走没什么关系，意思是重建邻接矩阵的学习方法对于结点的距离信息过于看重了吗？

    + [ ] Adversarial network embedding: 利用对抗生成网络解决图表示学习中对噪声数据敏感的问题。首先对数据进行预处理，产生更加稠密的输入特征矩阵，避免过拟合问题。结构保留模型利用已有的图表示学习方法学习节点表示向量，本文使用了DeepWalk的生成式变体，为每个节点随机采样多条同等长度的序列，游走概率由邻接矩阵中的权重决定，然后为每个节点训练一个目标表示和内容表示。对抗学习模型包含一个生成器和一个鉴别器，生成器将输入的高维特征映射为低维向量，使嵌入向量贴近先验分布，鉴别器将正例从嵌入向量中分辨出来。文章引入一个预设的先验分布，并假设所有节点向量都由这个分布产生。

    > 第一是这个随机游走是怎么处理节点特征的，第二是为所有节点特征预设同一个先验分布是不是合适，因为不同社团中的节点特征可能由不同的分布产生

    + GANE: A Generative Adversarial Network Embedding:
    + [ ] Adversarially regularized graph autoencoder for graph embedding: 大部分图嵌入方法着重于保留拓扑结构或者最小化重建误差，但忽略了隐层向量的数据分布，导致较差的嵌入结果。提出了一个新的嵌入框架，将网络的拓扑结构和文本信息编码为紧凑表示，解码器用于重建网络结构。隐层表示通过对抗训练策略约束为遵循先验分布。对抗训练策略用于分辨隐层向量是来自于真实分布还是图编码器。通过两层的图卷积自编码器将网络邻接矩阵和特征矩阵嵌入到隐层空间中，然后利用解码器重构邻接矩阵。使用多层感知器作为判别器，通过最小化二值判别器的交叉熵损失，嵌入向量可以在训练过程中被正则化。

2. 图自编码器
    + [ ] Variation autoencoder based network representation learning for classification: 原有工作只考虑网络表示学习的部分方面，比如边结构，节点信息或者部分整合(?)，这篇工作将网络结构和节点文本特征结合起来。使用Content2vec模型为节点文本生成属性特征，然后和网络的邻接矩阵拼接在一起作为变分自编码器的输入统一训练。模型中利用全连接层做降维映射，最终得到四段嵌入向量，分别作为文本信息和结构信息分布的均值和方差。然后从先验分布（高斯分布）中采样两个值作为扰动，通过线性函数结合均值和方差得到文本和结构的嵌入向量，再将两者拼接得到编码层最终习得的向量。最终通过解码层对输入进行还原，最小化输入输出向量之间的交叉熵和文本结构向量的KL散度。
    > 不太明白KL散度里面的$q(z_{ik}|x_i)$和$p(z_{ik})$是什么
    + [ ] Accelerated Attributed Network Embedding：用节点属性计算节点间的相似度，然后要求嵌入向量同时满足结构相似度和属性相似度，最终利用数学方式将学习方式进行分解，用分布式方式进行加速
    > 特点是用嵌入向量的欧几里得距离而不是余弦距离来衡量结构相似程度
    + [ ] Co-embedding Attributed Networks: 同时对结构和特征矩阵进行嵌入，利用全连接网络嵌入特征矩阵，利用图卷积网络嵌入邻接矩阵，双方均加入扰动项，然后分别重构特征矩阵和邻接矩阵
    >有个不太明白的地方，学两个表征干什么呢，这俩嵌入分别是干什么的，主要这个属性嵌入能干什么

3. 图卷积网络
   + [ ] simple and deep graph convolutional networks：解决图卷积网络的过平滑问题，过平滑问题由拉普拉斯矩阵的k阶近似的固定系数导致，会导致k层图卷积网络的输出逼近固定分布。通过引入残差网络和点对点映射的方式来解决，残差网络保证输出保留了一定比例的原始信号，点对点映射保证网络学习下限是原始信号。
    > 这种搞理论的不太懂，这好像也没有什么改进的地方了，好像只能试试往其他模型进行迁移，或者使用更加复杂的激活函数，比如像脉冲网络那种，可不可以避免过平滑，这个就是如何在数学上进行证明了，话说他那个证明好像没有考虑激活函数
   + [ ] convolutional kernel networks for graph structured data: 在图结构上的图卷积核网络
    > 还不懂图核是什么，先看了综述再说吧

4. 网络嵌入
    + [ ] context-aware network embedding for relation modeling：分别学习节点的结构嵌入和属性嵌入，在属性嵌入部分有注意力网络的雏形，针对不同的节点对学习不同的嵌入向量，最后将结构嵌入和属性嵌入做拼接作为最终的嵌入，在损失函数部分同时考虑结构间，属性间和结构-属性的相似程度
    + [ ] fast network embedding enhancement via high order proximity approximation：利用高阶结构的雏形模型，通过邻接矩阵相乘的方式得到二阶相邻关系，然后再分别计算节点嵌入和上下文关系嵌入（与deepwalk相关）
    > 这个工作后面由去掉激活函数的图卷积网络进一步发展，那个好像可以再做进一步的扩展，把多层的学习结果进行拼接最后再做一次滤波？

5. 图神经网络
    + [ ] generative pre-training of graph neural networks：一个新的GNN预训练方法，按照给定的顺序输入节点，根据节点的结构嵌入预测属性，根据属性嵌入预测结构，然后优化根据给定的图预测下一个节点输入时的图的结构和属性的能力
    > 后面结构预测那里没看懂，算是一种新的训练方式，就是不管GNN的模型结构是什么，根据这个方法可以得到一个无监督的很好的表征结果
    + [ ] graph neural networks with feature-wise linear modulation：利用一个超网络来计算以边类型划分的节点之间的转移矩阵，从而把目标节点的信息也纳入考虑，与GAT不同的是使用了转移矩阵和wh的元素乘而不是拼接。
    + [ ] on graph neural networks versus graph augmented mlps：用了多个处理后的邻接矩阵得到下一层的嵌入，再把所有嵌入拼接后过网络得到最终的下一层嵌入。
    > 这个嵌入方法很诡异，感觉创新点在理论那，这不就是把图残差网络做了多层拼接么？
    + [ ] gmnn: graph markov neural network: 利用一个GNN使用节点特征作为输入学习特征和标签之间的相关性，再利用另一个GNN使用节点标签作为特征学习标签之间的相关性，然后使用EM算法进行优化
    + [ ] the surprising power of graph neural networks with random node initialization：证明使用随机初始化的节点特征也可以学到universal的节点表示
    > 这个universal是什么意思

6. 图结构学习
    + [ ] deep graph structure learning for robust representations_a survey: 图结构学习综述，不但学习节点表示，同时学习优化的图结构，包括学习图结构的度量函数，调整图结构的权重，以及学习图结构的生成模型三种方式。
    + [ ] cluster-aware graph neural networks for unsupervised graph representation learning：利用GCN学习节点表示，然后利用kmeans进行聚类，同时利用MLP进行分类，优化分类和聚类结果的交叉熵，同时根据kmeans聚类结果，将同属同一社团概率过小的节点之间的边去除，再为属于同一社团概率够高的节点之间添加边。
    + [ ] unifying graph convolutional neural networks and label propagation: 将网络结构权重也作为可学习变量，先学习优化标签传播结果的结构权重，再根据结构权重学习节点表示，两者联合优化
##### 事实验证

+ [ ] fine-grained verification with kernel graph attention network: 用了一个核注意力机制来挖掘节点信息，然后对证据图上的节点进行分析，对于给定的序列来判断最终的事实是否成立
> 没太看懂，不太懂所谓的证据图（evidence graph是什么，应该怎么理解）
+ [ ] graph-based evidence aggregating and reasoning for fact verification
> 又是一篇，对于所描述的图结构和定义的问题都不是很明白

##### 节点分类

1. 对抗学习
   + [ ] Adversarial attacks on neural networks for graph data: 关注在二值属性网络上的节点分类问题，设计一个对抗模型，希望找到一个改变幅度低于一定的阈值的混淆网络，通过更改有限节点的特征和结构，使卷积网络对目标节点的分类结果与在原网络中的结果“距离”尽可能远。利用混淆网络和原网络的节点度数的概率分布，使用统计双样本测试来衡量混淆网络和原网络是否来自同一分布，对原网络的更改是否过于明显。在特征网络上利用随机游走，以从源特征节点到目标特征节点的随机游走概率来衡量两个特征共同出现是否是过于明显。在原图上计算出替代模型，然后在改变过于明显之前，依次从可攻击节点中选择进行结构和属性的混淆，并分别计算混淆和学习之后对目标结点的分类结果的“距离”，然后选择对距离改变最明显的混淆方式生成新图，最后返回改变最有效的混淆图。
   > （感觉已经有人做过了）这个可以当作是对抗学习网络的generator部分？不断生成图结构上几乎不可观测但是会改变学习结果的新图，然后通过判别器进行辨别？
   + [ ] Combining label propagation and simple models outperforms graph neural network：利用简单的多层感知机学习节点分类结果，然后计算和真实标签之间的误差，并传播这个误差，利用传播后的误差对节点的标签进行修正，最后修正后的标签标签，稳定后的标签作为节点分类结果


##### 社团发现
   + [ ] Discovering Communities and Anomalies in Attributed Graphs_Interactive Visual Exploration and Summarization: visual exploration and summarization部分没有看，前面community discovering部分通过定义了一个新的标准normality来衡量社团结构和属性的一致性，通过节点属性的元素积来判断节点属性的相似度，并用一个权重向量来关注更主要的属性，最后提出了一个算法来发现normality最大的社团划分
   + [ ] Semantic Community Identification in Large Attribute Networks：比较明显的通过网络嵌入来做社团分类，为每个社团也初始化了一个属性向量，要求节点属性和所属的社团属性要尽可能相似，同时节点的社团归属矩阵可以重构出网络邻接矩阵。
   > 有点像gci的矩阵版本

#### 社交网络

##### 网络表示学习

+ [ ] transnet_translation-based network representation learning for social relation extraction：一部分社交网络的边上有多个标签，利用这些标签预测网络中的其他边上的标签，通过源节点-边-目标节点的距离计算的方式提高节点嵌入的效果，通过自编码器方式学习边的嵌入，并优化正节点-边-节点关系的嵌入向量的距离

#### 异构网络

###### 网络表示学习

1. 图卷积对抗网络
   + [ ] Heterogeneous deep graph infomax: 
   + [ ] relation structure-aware heterogeneous information network embedding：将网络中的关系划分为隶属关系(AR)和交互关系(IR)，然后使用不同的损失函数来衡量不同关系下节点的相似程度，最后两边一块优化

#### 动态网络

##### collaboration network

1. 网络嵌入
网络拓扑结构随时间变化

   + 特征值分解

    Attributed network embedding for learning in a dynamic environment: 
    High-order proximity preserved embedding for dynamic networks
    Error-bounded SVD restart on dynamic networks

   + skip-gram

    Dynamic network embedding: an extended approach for skip-gram based network embedding
    Continuous-time dynamic network embeddings

   + 自编码器

    Deep embedding method for dynamic graphs
    dyngraph2vec: capturing network dynamics using dynamic graph representation learning

   + 图卷积

    Representation learning over dynamic graphs

   + 其他方法

    Dynamic network embedding by modeling tradic closure process
    Streaming graph neural networks
    Dyn2vec: exploiting dynamic behavior using difference networks-based node mebeddings for classification

##### telephonecall network

网络上节点之间直接相互影响

1. 网络嵌入

Embedding Temporal Network via Neighborhood Formation:

#### 时空网络

##### 网络表示学习

1. 图卷积对抗网络
    + Spatio-temporal deep graph infomax: 

### 多个网络

#### 复杂网络

#### 属性网络

##### 图相似度计算

+ [ ] SimGNN_A Neural Network Approach to Fast Graph Similarity Computation：在利用GCN得到图的节点表示之后分别对两个图上节点间的和整个图的嵌入向量进行比较，最终将两个相似度向量拼接后经过全连接网络得到两个图的相似程度。
> 这个会不会改成无监督的，比如改成对抗学习或者那种优化互信息的方式
> 怎么得到整个图的嵌入的还没看，两个图的嵌入的相似程度计算方法像是用了个bilinear矩阵，然后结合了拼接的全连接层
+ [ ] graph optimal transport fot cross-domain alignment：与上一个结构类似，根据相似性构建网络，然后对节点之间和节点对之间的相似性进行计算得到相似性cost，再根据wd算法得到转移矩阵，最后利用转移矩阵和相似性cost矩阵计算匹配程度，作为损失函数的一部分强调匹配程度。
> 后面的分布那里没有太看懂，和全图的比较好像又不太一样，粒度更细
#### 异构网络

#### 动态网络
