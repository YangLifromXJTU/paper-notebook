# 图的研究

## 图生成

### 序列神经网络

#### 复杂网络

+ [x] **scalable deep generative modeling for sparse graphs**：没太看懂，但是只考虑了生成图中有边的部分，从而降低时间复杂度。利用二叉树的方式为每个节点生成邻接向量，利用lstm估计在已有状态下节点会不会有左右子节点，最终的叶子节点就是根节点的相邻节点。剩下的没看懂

## 条件结构生成

### 图变分对抗网络

#### 属性网络

+ [x] **Conditional Structure Generation through Graph Variational Generative Adversarial Network**: 提出了条件结构生成问题，并使用图卷积神经网络构建了图变分生成对抗网络，用来生成具有灵活的属性-结构条件和排列不变性的属性网络

## 最小生成树发现

### k-means

#### 复杂网络

+ [x] **A fast minimum spanning tree algorithm based on K-means**: 最小生成树发现，先用kmeans将网络分成若干个小网络，然后在小网络上发现最小生成树，最后把这些最小生成树连在一起

## 社团发现(节点分类)

### 综述

+ [x] **clustering attributed graphs_models,measures and methods**：属性网络社团发现综述

### 新指标

#### 复杂网络

+ [x] **The Local Closure Coefficient_ A New Perspective On Network Clustering**：定义了一个新的聚类紧凑程度指标，将二阶路径的头节点从中间节点换成两边节点

  > 或许要看一下换过来的作用是什么，感觉还没看到指标的价值，或许可以和那个利用图神经网络进行神经网络结构探索的工作关联一下，因为那个用的是原来的clustering coefficient来描述搜索空间的

+ [x] **Discovering Communities and Anomalies in Attributed Graphs_Interactive Visual Exploration and Summarization**: visual exploration and summarization部分没有看，前面community discovering部分通过定义了一个新的标准normality来衡量社团结构和属性的一致性，通过节点属性的元素积来判断节点属性的相似度，并用一个权重向量来关注更主要的属性，最后提出了一个算法来发现normality最大的社团划分

  > 这个$I=\sum_{i\in C,j\in C,i\neq j}(A_{ij}-\frac{k_ik_j}{2m})sim_\bold{w}(\bold{x}_i,\bold{x}_j|\bold{w})$ 可以代替modularity吗

  >  但其实$A_{ij}-\frac{k_ik_j}{2m}$是不变的，所以其实优化的就是同个社团内两个节点的属性相似程度，那么这个和优化邻接矩阵的区别不大？

+ [x] **Leader-aware Community Detection in Social Networks**：定义每个节点的领导力，根据领导力生成依赖关系，最终根据依赖关系得到生成树作为社团结果

### 模块度方法

#### 复杂网络

+ [x] **Fast Unfolding of Communities in Large Networks**：经典louvain方法，计算节点加入一个社团后对模块度的改变，然后改变节点的社团归属，再把同一个社团节点融合成一个整体做成一个超图

+ [x] **fast algorithm for detecting community structure in networks**：经典FN方法，第一次提出模块度，也是逐渐选择模块度增加最多的合并方式融合社团

+ [x] **finding and evaluating community structure in networks**：经典GN算法，自顶向下根据边介数划分社团，逐渐删除边介数最大的边，并重新计算剩余的边介数

  > 边介数：边在最短路径上的节点对数

+ [x] **finding local community structure in networks**：定义了局部模块度，然后逐渐将局部模块度提升最大的节点加入社团，因为只考虑局部结构，所以只有一个标签。

+ [x] **community structure in social and biological networks**：和GN算法一样？

### 启发式算法

#### 复杂网络

+ [x] **GA-Net_A_Genetic_Algorithm_for_Community_Detection**：利用遗传算法进行社团发现，初始化若干个基因，每个基因代表原图中对应节点的一个边，然后进行交叉变异，最后得到模块度最大的基因作为社团发现结果

### 标签传播

#### 复杂网络

+ [x] **融入节点重要性和标签影响力的标签传播社区发现算法**：根据节点重要性对节点进行排序，然后根据标签影响值进行标签传播。

+ [x] **Detecting Community Structures in Networks by Label Propagation with Prediction of Percolation Transition**：将percolation translation和标签传播结合，延迟出现巨型社团的时刻

  > 不懂percolation transition，感觉价值有限

+ [x] **near linear time algorithm to detect community structures in large-scale networks**：经典LPA算法

+ [x] **a kind of community detecting algorithm based on modularized label propagation**：先根据节点度数传播，然后计算传播后的模块度，如果增加了就更新，没增加就维持原状。

+ [x] **memory based label propagation algorithm for community detection in social networks**：重复执行多次标签传播，保存每次稳定后的传播结果，然后从中选择出现次数最多的。

+ [x] **MLPA_Detecting Overlapping Communities by Multi-Label Propagation Approach**：经典重叠社团发现算法MLPA，计算两个节点之间的强度，根据强度选择节点标签，根据阈值对标签进行截断，并给每个节点一个记忆组件存储指定数量的社团标签

#### 动态网络

+ [ ] **Community Detection using Diffusion Information**：没太看懂，感觉看着挺像标签传播的，每个时间点只能观测到部分节点，节点的社团归属条件概率由相邻节点的归属来决定，预先指定了若干个cascade，然后观测节点在每个时间点出现在这个cascade，按照出现的向量表示来计算两个节点之间边的权重，最后用这个权重来决定节点社团归属

### 谱域方法

#### 复杂网络

+ [x] **Local spectral Clustering for overlapping community detection**：通过对已知社团的节点进行随机游走对网络进行采样，得到一个子图，然后首先假设所有节点均匀属于每一社团，通过在子图的拉普拉斯矩阵上计算1到$l$跳邻居的数目来更新节点属于每个社团的概率，对概率向量进行拼接后求正交基，并通过与拉氏矩阵相乘的方式循环更新正交基直至收敛，得到最后重叠社团的划分结果。

  > 一搞正交基我就懵了，感觉有点图嵌入那味了

### 矩阵（张量）分解

#### 复杂网络

+ [x] **Overlapping Community Detection via Constrained PARAFAC A Divide and Conquer Approach**：构建自我网络，然后通过张量分解来确定社团归属

  > 简单说，没看懂

### 网络表示学习方法

#### 属性网络

+ [x] **community detection in networks with node attributes**：经典CESNA算法，有点网络表示学习雏形，根据社团归属重建邻接矩阵和节点特征

+ [x] **Semantic Community Identification in Large Attribute Networks**：比较明显的通过网络嵌入来做社团分类，为每个社团也初始化了一个属性向量，要求节点属性和所属的社团属性要尽可能相似，同时节点的社团归属矩阵可以重构出网络邻接矩阵。

  > 有点像gci的矩阵版本，那可不可以把每个属性看作是一个信号，然后节点的连接关系是由其中某几个信号所决定的，用注意力机制来衡量节点对之间起作用的属性。

+ [x] **Combining label propagation and simple models outperforms graph neural network**：利用简单的多层感知机学习节点分类结果，然后计算和真实标签之间的误差，并传播这个误差，利用传播后的误差对节点的标签进行修正，最后修正后的标签标签，稳定后的标签作为节点分类结果

+ [x] **gmnn: graph markov neural network**: 利用一个GNN使用节点特征作为输入学习特征和标签之间的相关性，再利用另一个GNN使用节点标签作为特征学习标签之间的相关性，然后使用EM算法进行优化

+ [x] **A Non-negative Symmetric Encoder-Decoder Approach forCommunity Detection**：将节点归属矩阵作为编码器和解码器权重，学到的类似社团特征，要求解码器输出的邻接矩阵和输入尽可能相似，也要求解码器反向编码的社团特征和编码器的结果尽可能相似

## 网络表示学习

### 综述

+ [x] **Graph Embedding Techniques, Applications, and Performance_A Survey**：
+ [x] **A survey on network embedding**：将网络嵌入方法分成保留结构信息的方法，保留额外信息的方法，保留进阶信息（端到端的）的方法

### 生成对抗网络

#### 基础GAN

+ [x] **Generative adversarial nets**：经典GAN

+ [x] **Conditional generative adverarial nets**：条件GAN，将先验条件分布同时输入生成器和判别器

  > 方法就半页，这就是大佬吗

#### 复杂网络

节点表示随机生成，然后学习进行优化

+ [x] **GraphGAN: graph representation learning with generative adversarial nets**: 利用对抗生成网络结合图表示学习中的生成式模型和辨别式模型。GraphGAN假设所有节点的连接遵循一个先验分布，边是这个分布的已观测样本。方法包含两个模型，生成器用于拟合真实的链接分布概率，判别器用于区分well-connected和ill-connected的节点对，计算节点之间存在边的可能性。方法利用嵌入好的向量，使用bfs后的生成树的路径计算节点连接概率当作生成器进行抽样，并使用sigmoid函数利用嵌入向量计算节点之间的连通可能性。

  > 没用神经网络，就硬计算，使用了bfs树来保证对图结构敏感

+ [x] **Learning deep network representations with adversarially regularized autoencoders**: 大部分网络嵌入模型根据采样得到的节点序列学习嵌入向量，以使低维向量保留局部特征或者全局重构能力。但是因为采样序列的稀疏性，学习到的表示向量很难用于模型泛化。使用对抗正则自编码器同时考虑局部特征和全局重构约束。联合对立封装在对抗训练过程中，以规避显示先验分布的要求，从而获得更好的泛化性能。首先使用随机游走对节点进行采样，然后利用LSTM作为自编码器的编码器和解码器。利用自编码器学到的向量为正样本，从先验分布中利用生成器采样得到负样本，最后输入判别器进行分辨，以两个分布的EMD距离的双重性是作为距离度量。

+ [ ] GANE: A Generative Adversarial Network Embedding:


#### 属性网络

+ [x] **Adversarial network embedding**: 利用对抗生成网络解决图表示学习中对噪声数据敏感的问题。首先对数据进行预处理，产生更加稠密的输入特征矩阵，避免过拟合问题。结构保留模型利用已有的图表示学习方法学习节点表示向量，本文使用了DeepWalk的生成式变体，为每个节点随机采样多条同等长度的序列，游走概率由邻接矩阵中的权重决定，然后为每个节点训练一个目标表示和内容表示。对抗学习模型包含一个生成器和一个鉴别器，生成器将输入的高维特征映射为低维向量，使嵌入向量贴近先验分布，鉴别器将正例从嵌入向量中分辨出来。文章引入一个预设的先验分布，并假设所有节点向量都由这个分布产生。

  > 第一是这个随机游走是怎么处理节点特征的，第二是为所有节点特征预设同一个先验分布是不是合适，因为不同社团中的节点特征可能由不同的分布产生，第三是内容表示和目标表示应该用哪个进行分辨

+ [ ] **learning graph embedding with adversarial training methods**：自编码器和对抗网络结合方法，先后优化重构损失和判别损失

#### 异构网络

+ [x] Adversarial learning on Heterogeneous information networks：通过节点和关系嵌入相乘的方式得到高斯分布的均值，在高斯分布中采样并使用MLP得到生成的负样本表示，然后利用一个解码器来计算相连概率进行区分。

  > 特点是负样本生成方式，原来都是直接负采样的

### 自编码器方法

#### 复杂网络

+ [x] LINE: Large-scale Information Network Embedding：经典LINE，但其实没太弄明白二阶距离是怎么体现的，可以看作以随机生成作为生成器，共现概率作为解码器的自编码器网络

+ [x] **Deep neural network for learning graph representations**：首先用了随机冲浪模型进行次啊样然后计算PPMI矩阵，然后利用一个自编码器嵌入PPMI矩阵，得到嵌入向量

+ [x] **Adversarially regularized graph autoencoder for graph embedding**: 大部分图嵌入方法着重于保留拓扑结构或者最小化重建误差，但忽略了隐层向量的数据分布，导致较差的嵌入结果。提出了一个新的嵌入框架，将网络的拓扑结构和文本信息编码为紧凑表示，解码器用于重建网络结构。隐层表示通过对抗训练策略约束为遵循先验分布。对抗训练策略用于分辨隐层向量是来自于真实分布还是图编码器。通过两层的图卷积自编码器将网络邻接矩阵和特征矩阵嵌入到隐层空间中，然后利用解码器重构邻接矩阵。使用多层感知器作为判别器，通过最小化二值判别器的交叉熵损失，嵌入向量可以在训练过程中被正则化。

  > 能不能利用一下连接预测的学习方法来重构邻接矩阵，从而将对抗网络和自编码器网络结合在一起？对抗的精髓是要有一个先验的结果，通过对抗使嵌入结果更加贴合这个结果，提升编码器的部分
  >
  > 我感觉这个方法更重要的一个问题是，他要求嵌入结果都符合一个先验分布，但如果网络本来就不符合呢？这俩学习目标会不会冲突？

#### 社交网络

+ [x] **transnet_translation-based network representation learning for social relation extraction**：一部分社交网络的边上有多个标签，利用这些标签预测网络中的其他边上的标签，通过源节点-边-目标节点的距离计算的方式提高节点嵌入的效果，通过自编码器方式学习边的嵌入，并优化正节点-边-节点关系的嵌入向量的距离

  > 用了知识图谱的嵌入方式？

+ [x] **An End-to-End Framework for Learning Multiple Conditional Network Representations of Social Network**：根据不同的行为类别将节点表示进行区分，然后分别利用注意力机制进行学习，最后得到的结果分类衡量距离

  > 一个端到端的方法，但也可以用自编码器的方式进行无监督学习，端到端这里其实还没看懂

#### 属性网络

+ [x] **Variation autoencoder based network representation learning for classification**: 原有工作只考虑网络表示学习的部分方面，比如边结构，节点信息或者部分整合(?)，这篇工作将网络结构和节点文本特征结合起来。使用Content2vec模型为节点文本生成属性特征，然后和网络的邻接矩阵拼接在一起作为变分自编码器的输入统一训练。模型中利用全连接层做降维映射，最终得到四段嵌入向量，分别作为文本信息和结构信息分布的均值和方差。然后从先验分布（高斯分布）中采样两个值作为扰动，通过线性函数结合均值和方差得到文本和结构的嵌入向量，再将两者拼接得到编码层最终习得的向量。最终通过解码层对输入进行还原，最小化输入输出向量之间的交叉熵和文本结构向量的KL散度。

+ [x] **Accelerated Attributed Network Embedding**：用节点属性计算节点间的相似度，然后要求嵌入向量同时满足结构相似度和属性相似度，最终利用数学方式将学习方式进行分解，用分布式方式进行加速

  > 结构邻接矩阵和属性邻接矩阵

+ [x] **Co-embedding Attributed Networks**: 同时对结构和特征矩阵进行嵌入，利用全连接网络嵌入特征矩阵，利用图卷积网络嵌入邻接矩阵，双方均加入扰动项，然后分别重构特征矩阵和邻接矩阵

  > 加了重构节点是否拥有这个特征的信息的损失函数的图卷积自编码器

+ [x] **a variational autoencoder for directed acyclic graphs**: 有向无环图上的变分自编码器，根据结构顺序使用GRU学习节点表示，用GRU解码重构有向无环图，根据输入顺序度量重构每一步得到的节点类型和连边相似程度

  > 感觉在nas和强化学习中有用处？可以根据已有节点进行推理，预测将要使用的节点是什么样子的，可以缩小搜索范围，甚至启发生成新的节点/新的网络层？，nas好像已经有的贝叶斯网络结构学习方法，这个应该也结合了图？

### 图卷积网络

#### 属性网络

+ [x] **Large-Scale Learnable Graph Convolutional Networks**：和其他gcn不同的是在特征层面进行采样，从相邻节点中每个特征采样前k个最大的，然后直接利用cnn进行降维，最后将特征拼接后通过全联接层进行判断。

+ [x] **simple and deep graph convolutional networks**：解决图卷积网络的过平滑问题，过平滑问题由拉普拉斯矩阵的k阶近似的固定系数导致，会导致k层图卷积网络的输出逼近固定分布。通过引入残差网络和点对点映射的方式来解决，残差网络保证输出保留了一定比例的原始信号，点对点映射保证网络学习下限是原始信号。

  > 这种搞理论的不太懂，这好像也没有什么改进的地方了，好像只能试试往其他模型进行迁移，或者使用更加复杂的激活函数，比如像脉冲网络那种，可不可以避免过平滑，这个就是如何在数学上进行证明了，话说他那个证明好像没有考虑激活函数

+ [x] **convolutional kernel networks for graph structured data**: 在图结构上的图卷积核网络

  > 还不懂图核是什么，先看了综述再说吧

### 注意力方法

#### 属性网络

+ [x] **graph attention networks**：经典GAT

+ [x] **graph neural networks with feature-wise linear modulation**：利用一个超网络来计算以边类型划分的节点之间的转移矩阵，从而把目标节点的信息也纳入考虑，计算目标节点在指定边类型下的权重并与wh进行元素乘，并在后将另一个同样方式计算出的权重作为偏置项加入。

  > 与GAT不同的是使用了转移矩阵和wh的元素乘而不是拼接

### 其他神经网络方法

#### 属性网络

+ [x] **context-aware network embedding for relation modeling**：分别学习节点的结构嵌入和属性嵌入，在属性嵌入部分有注意力网络的雏形，针对不同的节点对学习不同的嵌入向量，最后将结构嵌入和属性嵌入做拼接作为最终的嵌入，在损失函数部分同时考虑结构间，属性间和结构-属性的相似程度

  > 但好像没说最后得到的嵌入向量是什么？就光得到了节点对的嵌入向量，那不成了一个N$\times$N$\times$d 的张量了？

+ [x] **on graph neural networks versus graph augmented mlps**：用了多个处理后的邻接矩阵得到下一层的嵌入，再把所有嵌入拼接后过网络得到最终的下一层嵌入。

  > 这个嵌入方法很诡异，感觉创新点在理论那，这不就是把图残差网络做了多层拼接么？

+ [x] **the surprising power of graph neural networks with random node initialization**：证明使用随机初始化的节点特征也可以学到通用的节点表示


#### 异构网络

+ [x] **relation structure-aware heterogeneous information network embedding**：将网络中的关系划分为隶属关系(AR)和交互关系(IR)，然后使用不同的损失函数来衡量不同关系下节点的相似程度，最后两边一块优化

### 随机游走

+ [x] Efficient Estimation of Word Representations in Vector Space：经典word2vec

#### 复杂网络

+ [x] **deepwalk_online learning of social representations**：经典deepwalk，代码来看就是随机游走+word2vec。

+ [x] **fast network embedding enhancement via high order proximity approximation**：利用高阶结构的雏形模型，通过邻接矩阵相乘的方式得到高阶相邻关系，然后再分别计算节点嵌入和上下文关系嵌入。进一步优化了学到的嵌入结果

### 对比学习

#### 属性网络

+ [x] **Deep graph infomax**: 利用对抗神经网络解决以采样为基础的网络表示学习方法对于随机游走目标的依赖，并可用于迁移式和归纳式学习。利用图卷积自编码器为每个节点学习一个patch表征。同时以最大化局部互信息为自编码器学习目的，利用一个readout function将patch表征聚合为图级别的表征。同时通过负采样从图中获取负样本，然后在鉴别网络部分分别衡量正负样本的patch表征属于图级别表征对的概率，最后以最小化这个概率为目标进行训练。

#### 异构网络

+ [x] Heterogeneous deep graph infomax: 根据不同的元路径随机游走的方式生成邻接矩阵并学习对应的表示，针对邻接矩阵进行负采样学到负样本的表示，然后用一个语义级别的将不同表示合并，利用readout的到正负图的整体表示，最后用判别器区分正负图。

### 预训练

+ [x] generative pre-training of graph neural networks：一个新的GNN预训练方法，按照给定的顺序输入节点，根据节点的结构嵌入预测属性，根据属性和已有结构预测剩余结构，然后优化根据给定的图预测下一个节点输入时的图的结构和属性的能力

  > 对于一个GNN模型有没有包含足够的信息的判定方式是能不能根据已有的结构和属性信息去预测下一个节点的结构和属性，这个借鉴了图生成思路

## 图结构学习

### 综述

+ [x] **deep graph structure learning for robust representations_a survey**: 图结构学习综述，不但学习节点表示，同时学习优化的图结构，包括学习图结构的度量函数，调整图结构的权重，以及学习图结构的生成模型三种方式。

### 调整结构权重

#### 属性网络

+ [x] **cluster-aware graph neural networks for unsupervised graph representation learning**：利用GCN学习节点表示，然后利用kmeans进行聚类，同时利用MLP进行分类，优化分类和聚类结果的交叉熵，同时根据kmeans聚类结果，将同属同一社团概率过小的节点之间的边去除，再为属于同一社团概率够高的节点之间添加边。

+ [x] **unifying graph convolutional neural networks and label propagation**: 将网络结构权重也作为可学习变量，先学习优化标签传播结果的结构权重，再根据结构权重学习节点表示，两者联合优化

  > 总感觉这里的方式其实和GAT很像，都是自适应地学习每条边的权重，这个表现更好是因为调整方式的感受范围更大吗？

## 链接预测

### 图神经网络

#### 属性网络

+ [x] <span id="imc-gnn">**Indctive matrix completion based on graph neural networks**</span> : 首先根据购买矩阵从用户-物品二项网络中提取用户-物品对为基础的$1$跳子网络，然后用gnn学习节点表示，拼接用户-物品表示得到子图表示，最后利用多层感知机对边权重进行回归学习。
+ [x] **link prediction based on graph neural networks**：很像[用gnn补全用户-物品矩阵那个研究](#imc-gnn)，采样获得连边节点附近的图，然后给每个节点标签，形成节点信息矩阵，最后利用gnn学习表示，并进行连接预测（文章中用了dgcnn，直接作为图分类问题进行预测了）

## 事实验证

### 图注意力网络

#### 属性网络

+ [x] fine-grained verification with kernel graph attention network: 用了一个核注意力机制来挖掘节点信息，然后对证据图上的节点进行分析，对于给定的序列来判断最终的事实是否成立

  > 没太看懂，不太懂所谓的证据图（evidence graph是什么，应该怎么理解）

+ [x] graph-based evidence aggregating and reasoning for fact verification

  > 又是一篇，对于所描述的图结构和定义的问题都不是很明白

## 图攻击

### 对抗学习

#### 属性网络

+ [x] **Adversarial attacks on neural networks for graph data**： 关注在二值属性网络上的节点分类问题，设计一个对抗模型，希望找到一个改变幅度低于一定的阈值的混淆网络，通过更改有限节点的特征和结构，使卷积网络对目标节点的分类结果与在原网络中的结果“距离”尽可能远。利用混淆网络和原网络的节点度数的概率分布，使用统计双样本测试来衡量混淆网络和原网络是否来自同一分布，对原网络的更改是否过于明显。在特征网络上利用随机游走，以从源特征节点到目标特征节点的随机游走概率来衡量两个特征共同出现是否是过于明显。在原图上计算出替代模型，然后在改变过于明显之前，依次从可攻击节点中选择进行结构和属性的混淆，并分别计算混淆和学习之后对目标结点的分类结果的“距离”，然后选择对距离改变最明显的混淆方式生成新图，最后返回改变最有效的混淆图。

## 图分类

### 图神经网络

#### 属性网络

+ [x] An End-to-End deep leraning architecture for graph classification: 多层图卷积学到节点表示，再把多层学习结果横向拼接，然后先在行内排序再从最后一列向前字典排序节点，选择前k个节点，不够的用全零向量补齐，最后使用1-d卷积层和Max pooling层和全连接层获得图分类结果

  > 后半截非常神仙，为什么学习到的表示经过这样排序加卷积就可以得到很好的结果，首先是为什么就可以这样排序。

+ [x] pooling regularized graph neural network for fmri biomarker analysis：图卷积$+$ topK池化进行图分类（对脑成像进行分类），设计了多种损失函数，衡量前k个节点和后面节点之间的相似程度，要求前k个评分尽可能相似且尽量大，后面评分尽可能相似且尽量小，同一社团内评分尽可能相似。

  > 总感觉脑成像这里的社团发现问题和传统社团发现问题不同，这里主要考量的好像是不同社团间的交互强度

## 图相似度计算

### 图神经网络

#### 属性网络

+ [x] **SimGNN_A Neural Network Approach to Fast Graph Similarity Computation**：在利用GCN得到图的节点表示之后分别对两个图上节点间的和整个图的嵌入向量进行比较，最终将两个相似度向量拼接后经过全连接网络得到两个图的相似程度。

  > 这个会不会改成无监督的，比如改成对抗学习或者那种优化互信息的方式
  > 怎么得到整个图的嵌入的还没看，两个图的嵌入的相似程度计算方法像是用了个bilinear矩阵，然后结合了拼接的全连接层

+ [x] **graph optimal transport fot cross-domain alignment**：与上一个结构类似，根据相似性构建网络，然后对节点之间和节点对之间的相似性进行计算得到相似性cost，再根据wd算法得到转移矩阵，最后利用转移矩阵和相似性cost矩阵计算匹配程度，作为损失函数的一部分强调匹配程度。

  > 后面的分布那里没有太看懂，和全图的比较好像又不太一样，粒度更细