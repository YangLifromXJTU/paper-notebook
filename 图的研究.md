# 图的研究

## 图生成

### 传统方法

#### 复杂网络

+ [x] **benchmark graphs for testing community detection algorithms**：经典LFR算法，没咋看懂光用包了，需要看一下

### 序列神经网络

#### 复杂网络

+ [x] **scalable deep generative modeling for sparse graphs**：没太看懂，但是只考虑了生成图中有边的部分，从而降低时间复杂度。利用二叉树的方式为每个节点生成邻接向量，利用lstm估计在已有状态下节点会不会有左右子节点，最终的叶子节点就是根节点的相邻节点。剩下的没看懂

### 对抗生成网络

#### 复杂网络

+ [ ] **NetGAN_Generating Graphs via Random Walks**：利用一个LSTM作为编码器逐渐生成随机游走序列，每步生成节点采样分布和状态表示，判别器用于分别生成的随机游走和真实的随机游走

  > 看个半懂

## 条件结构生成

### 图变分对抗网络

#### 属性网络

+ [x] **Conditional Structure Generation through Graph Variational Generative Adversarial Network**: 提出了条件结构生成问题，并使用图卷积神经网络构建了图变分生成对抗网络，用来生成具有灵活的属性-结构条件和排列不变性的属性网络

## 最小生成树发现

### k-means

#### 复杂网络

+ [x] **A fast minimum spanning tree algorithm based on K-means**: 最小生成树发现，先用kmeans将网络分成若干个小网络，然后在小网络上发现最小生成树，最后把这些最小生成树连在一起

## 最大团发现



## 弱关系群组发现

### 传统方法

#### 复杂网络

+ [ ] **forming online support groups for internet and behavior related addictions**：在社交网络上寻找互助小组，通过在加权图的补图上寻找极大团的方式寻找满足规模下限的权重最大的群组

+ [x] **On finding socially tenuous groups for online social networks**：构造k三角度量，贪心寻找k三角最少的群组，并证明了找到的是最优群组之一

+ [ ] **Finding tenuous groups in social networks**：利用k边数量衡量关系强度，并用贪心算法寻找弱关系群组

#### 属性网络

+ [ ] **On Automatic Formation of Effective Therapy Groups in Social Networks**：寻找弱关系群组，且群组中每个属性都至少有两个节点拥有。

  > 这个属性衡量方式其实很少见

## 社团发现(节点分类)

### 综述

+ [x] **clustering attributed graphs_models,measures and methods**：属性网络社团发现综述

### 新指标

#### 复杂网络

+ [x] **The Local Closure Coefficient_ A New Perspective On Network Clustering**：定义了一个新的聚类紧凑程度指标，将二阶路径的头节点从中间节点换成两边节点

  > 或许要看一下换过来的作用是什么，感觉还没看到指标的价值，或许可以和那个利用图神经网络进行神经网络结构探索的工作关联一下，因为那个用的是原来的clustering coefficient来描述搜索空间的

+ [x] **Discovering Communities and Anomalies in Attributed Graphs_Interactive Visual Exploration and Summarization**: visual exploration and summarization部分没有看，前面community discovering部分通过定义了一个新的标准normality来衡量社团结构和属性的一致性，通过节点属性的元素积来判断节点属性的相似度，并用一个权重向量来关注更主要的属性，最后提出了一个算法来发现normality最大的社团划分

  > 这个$I=\sum_{i\in C,j\in C,i\neq j}(A_{ij}-\frac{k_ik_j}{2m})sim_\bold{w}(\bold{x}_i,\bold{x}_j|\bold{w})$ 可以代替modularity吗

  >  但其实$A_{ij}-\frac{k_ik_j}{2m}$是不变的，所以其实优化的就是同个社团内两个节点的属性相似程度，那么这个和优化邻接矩阵的区别不大？

+ [x] **Leader-aware Community Detection in Social Networks**：定义每个节点的领导力，根据领导力生成依赖关系，最终根据依赖关系得到生成树作为社团结果

#### 属性网络

+ [x] **Node attribute-enhanced community detection in complex networks**：计算节点间属性相似度并选择最相似的k个构建k近邻网络，并利用k近邻网络对原网络进行补全，然后在网络上计算节点间的距离，并计算局部密度，和全局密度的距离，并结合pagerank计算每个节点是knn算法中心节点的概率，最后利用knn得到社团划分。

  > 感觉所有的方法都有一个问题，就是为什么要这么算，为什么要玩这种花活，以及为什么还就有作用了

### 模块度方法

#### 复杂网络

+ [x] **Fast Unfolding of Communities in Large Networks**：经典louvain方法，计算节点加入一个社团后对模块度的改变，然后改变节点的社团归属，再把同一个社团节点融合成一个整体做成一个超图

+ [x] **fast algorithm for detecting community structure in networks**：经典FN方法，第一次提出模块度，也是逐渐选择模块度增加最多的合并方式融合社团

+ [x] **finding and evaluating community structure in networks**：经典GN算法，自顶向下根据边介数划分社团，逐渐删除边介数最大的边，并重新计算剩余的边介数

  > 边介数：边在最短路径上的节点对数

+ [x] **finding local community structure in networks**：定义了局部模块度，然后逐渐将局部模块度提升最大的节点加入社团，因为只考虑局部结构，所以只有一个标签。

+ [x] **community structure in social and biological networks**：和GN算法一样？

### 启发式算法

#### 复杂网络

+ [x] **GA-Net:A Genetic Algorithm for Community Detection**：利用遗传算法进行社团发现，初始化若干个基因，每个基因代表原图中对应节点的一个边，然后进行交叉变异，最后得到模块度最大的基因作为社团发现结果

### 标签传播

#### 复杂网络

+ [x] **融入节点重要性和标签影响力的标签传播社区发现算法**：根据节点重要性对节点进行排序，然后根据标签影响值进行标签传播。

+ [x] **Detecting Community Structures in Networks by Label Propagation with Prediction of Percolation Transition**：将percolation translation和标签传播结合，延迟出现巨型社团的时刻

  > 不懂percolation transition，感觉价值有限

+ [x] **near linear time algorithm to detect community structures in large-scale networks**：经典LPA算法

+ [x] **a kind of community detecting algorithm based on modularized label propagation**：先根据节点度数传播，然后计算传播后的模块度，如果增加了就更新，没增加就维持原状。

+ [x] **memory based label propagation algorithm for community detection in social networks**：重复执行多次标签传播，保存每次稳定后的传播结果，然后从中选择出现次数最多的。

+ [x] **MLPA_Detecting Overlapping Communities by Multi-Label Propagation Approach**：经典重叠社团发现算法MLPA，计算两个节点之间的强度，根据强度选择节点标签，根据阈值对标签进行截断，并给每个节点一个记忆组件存储指定数量的社团标签

+ [ ] **a core leader based label propagation algorithm for community detection**：使用leader rank算法计算每个节点的影响度，然后根据影响度寻找最大的若干节点作为领导节点，最终从领导节点开始逐渐传播标签直至标签收敛，得到社团结果。不再给每个节点一个标签，减少标签量，提升计算效率

+ [ ] **a new label propagation with dams**：计算边介数，在边介数最高的一部分边上设置阻碍限制标签传播范围，然后执行标签传播，重复过程多次，然后根据节点共现频率构建新图，并在新图上以连通子图作为核来划分社团

+ [x] **a link strength based label propagation algorithm for community detection**：用k近邻代替直接邻居来衡量边强度，然后根据边强度选择标签

+ [ ] **a new multiple label propagation algorithm for linked open data**

  > 方法其实很直观，就是根据边权重计算节点标签，但公式是真看不懂，也不清楚他要干什么，关联多图？社团发现？

+ [ ] 

#### 动态网络

+ [ ] **Community Detection using Diffusion Information**：没太看懂，感觉看着挺像标签传播的，每个时间点只能观测到部分节点，节点的社团归属条件概率由相邻节点的归属来决定，预先指定了若干个cascade，然后观测节点在每个时间点出现在这个cascade，按照出现的向量表示来计算两个节点之间边的权重，最后用这个权重来决定节点社团归属

### 谱域方法

#### 复杂网络

+ [x] **Local spectral Clustering for overlapping community detection**：通过对已知社团的节点进行随机游走对网络进行采样，得到一个子图，然后首先假设所有节点均匀属于每一社团，通过在子图的拉普拉斯矩阵上计算1到$l$跳邻居的数目来更新节点属于每个社团的概率，对概率向量进行拼接后求正交基，并通过与拉氏矩阵相乘的方式循环更新正交基直至收敛，得到最后重叠社团的划分结果。

  > 一搞正交基我就懵了，感觉有点图嵌入那味了

### 矩阵（张量）分解

#### 复杂网络

+ [x] **Overlapping Community Detection via Constrained PARAFAC A Divide and Conquer Approach**：构建自我网络，然后通过张量分解来确定社团归属

  > 简单说，没看懂

### 图神经网络方法

#### 属性网络

+ [x] **community detection in networks with node attributes**：经典CESNA算法，有点网络表示学习雏形，根据社团归属重建邻接矩阵和节点特征

+ [x] **Semantic Community Identification in Large Attribute Networks**：比较明显的通过网络嵌入来做社团分类，为每个社团也初始化了一个属性向量，要求节点属性和所属的社团属性要尽可能相似，同时节点的社团归属矩阵可以重构出网络邻接矩阵。

  > 那可不可以把每个属性看作是一个信号，然后节点的连接关系是由其中某几个信号所决定的，用注意力机制来衡量节点对之间起作用的属性。


+ [ ] **A Unified Framework for Community Detection and Network Representation Learning**：在随机游走的基础上计算每个节点属于社团的概率，以及社团和游走序列的紧密程度。统计方式计算节点属于社团的概率，统计方式或向量方式计算社团和序列的紧密程度，然后计算节点在序列中属于社团的概率。重复多次直至社团收敛，然后根据社团的后验概率更新节点标签，并联合最大化滑动窗口内节点的共现概率和拥有相同社团标签的概率，最终社团表示聚合得到节点表示，然后和节点向量拼接得到最终表示。

  > 这个方法学到的就有社团信息

+ [x] **Combining label propagation and simple models outperforms graph neural network**：利用简单的多层感知机学习节点分类结果，然后计算和真实标签之间的误差，并传播这个误差，利用传播后的误差对节点的标签进行修正，最后修正后的标签标签，稳定后的标签作为节点分类结果

+ [x] **Semi-supervised learning on graphs with generative adversarial nets**：生成器用于生成在中心部分density gap的样本，分类器用于给每个节点分类，以及根据中间结果挑出假样本，进行对抗学习后分类器是要学到不同标签分布之间的边界

+ [x] **gmnn: graph markov neural network**: 利用一个GNN使用节点特征作为输入学习特征和标签之间的相关性，再利用另一个GNN使用节点标签作为特征学习标签之间的相关性，然后使用EM算法进行优化

+ [x] **A Non-negative Symmetric Encoder-Decoder Approach forCommunity Detection**：将节点归属矩阵作为编码器和解码器权重，学到的类似社团特征，要求解码器输出的邻接矩阵和输入尽可能相似，也要求解码器反向编码的社团特征和编码器的结果尽可能相似

+ [x] **Overlapping community detection with graph neural networks**：将节点嵌入到社团数维度，并假定社团与节点结构遵循伯努利分布，根据伯努利分布重构邻接矩阵。通过每次采样正样本和负样本的方式支持批量训练，并提升效率。

  > 这个批量训练的方式很有意思，可以拿来用下
  
+ [x] **Graph Agreement models for semi-supervised learning**：学习一个认知模型，判断两个节点是否有同一个标签，然后根据这个概率进行学习。认知模型进行编码，聚合和概率预测三部分。首先将认知模型训练到收敛，然后利用认知模型结果训练图的分类模型，然后根据分类模型结果选择置信度前列的节点赋予标签，然后重复过程固定次数，或直至所有节点都被打了标签。

  > you figure out a simple idea, then found out that somebody already published it with a better version one year ago.

#### 复杂网络

+ [x] **deep autoencoder-like nonnegative matrix factorization for community detection**：将邻接矩阵表示成多个非负矩阵分解的形式，然后将编码器也当做解码器，反向返回要求重构的邻接矩阵尽可能像。
+ [x] **learning deep representations for graph clustering**：经典GraphEncoder，自编码器嵌入相似度矩阵，然后使用kmeans进行聚类

+ [x] **COSINE: Community-Preserving Social Networks Embedding from Information Diffusion Cascades**：首先固定节点嵌入，分别计算社团的均值方差和多项分布，更新节点的社团归属，然后固定社团参数，在每个社团中根据嵌入估计未感染节点的感染时间，然后根据预估的时间更新节点的嵌入，重复直到收敛。

## 网络表示学习

### 综述

+ [x] **Graph Embedding Techniques, Applications, and Performance_A Survey**：
+ [x] **A survey on network embedding**：将网络嵌入方法分成保留结构信息的方法，保留额外信息的方法，保留进阶信息（端到端的）的方法
+ [x] **Graph Learning: A Survey**：

### 生成对抗网络

#### 基础GAN

+ [x] **Generative adversarial nets**：经典GAN

+ [x] **Conditional generative adverarial nets**：条件GAN，将先验条件分布同时输入生成器和判别器

  > 方法就半页，这就是大佬吗

#### 复杂网络

节点表示随机生成，然后学习进行优化

+ [x] **GraphGAN: graph representation learning with generative adversarial nets**: 利用对抗生成网络结合图表示学习中的生成式模型和辨别式模型。GraphGAN假设所有节点的连接遵循一个先验分布，边是这个分布的已观测样本。方法包含两个模型，生成器用于拟合真实的链接分布概率，判别器用于区分well-connected和ill-connected的节点对，计算节点之间存在边的可能性。方法利用嵌入好的向量，使用bfs后的生成树的路径计算节点连接概率当作生成器进行抽样，并使用sigmoid函数利用嵌入向量计算节点之间的连通可能性。

  > 没用神经网络，就硬计算，使用了bfs树来保证对图结构敏感

+ [x] **Learning deep network representations with adversarially regularized autoencoders**: 大部分网络嵌入模型根据采样得到的节点序列学习嵌入向量，以使低维向量保留局部特征或者全局重构能力。但是因为采样序列的稀疏性，学习到的表示向量很难用于模型泛化。使用对抗正则自编码器同时考虑局部特征和全局重构约束。联合对立封装在对抗训练过程中，以规避显示先验分布的要求，从而获得更好的泛化性能。首先使用随机游走对节点进行采样，然后利用LSTM作为自编码器的编码器和解码器。利用自编码器学到的向量为正样本，从先验分布中利用生成器采样得到负样本，最后输入判别器进行分辨，以两个分布的EMD距离的双重性是作为距离度量。

+ [ ] GANE: A Generative Adversarial Network Embedding:

+ [x] **Adversarial network embedding**: 利用对抗生成网络解决图表示学习中对噪声数据敏感的问题。首先对数据进行预处理，产生更加稠密的输入特征矩阵，避免过拟合问题。结构保留模型利用已有的图表示学习方法学习节点表示向量，本文使用了DeepWalk的生成式变体，为每个节点随机采样多条同等长度的序列，游走概率由邻接矩阵中的权重决定，然后为每个节点训练一个目标表示和内容表示。对抗学习模型包含一个生成器和一个鉴别器，生成器将输入的高维特征映射为低维向量，使嵌入向量贴近先验分布，鉴别器将正例从嵌入向量中分辨出来。文章引入一个预设的先验分布，并假设所有节点向量都由这个分布产生。

  > 为所有节点特征预设同一个先验分布是不是合适，因为不同社团中的节点特征可能由不同的分布产生，第三是内容表示和目标表示应该用哪个进行分辨

#### 属性网络

+ [ ] **learning graph embedding with adversarial training methods**：自编码器和对抗网络结合方法，先后优化重构损失和判别损失
+ [x] **MolGAN_An implicit generative model for small molecular graphs**：一个生成器，一个判别器，一个反馈器，反馈器结合强化网络来优化生成器的性能（由额外软件计算，向指定方向优化）
+ [x] **Adversarial  data augementation for graph neural networks**: 根据每步的梯度更新扰乱参数，并使用批量计算梯度后更新的方式加速更新

#### 异构网络

+ [x] Adversarial learning on Heterogeneous information networks：通过节点和关系嵌入相乘的方式得到高斯分布的均值，在高斯分布中采样并使用MLP得到生成的负样本表示，然后利用一个解码器来计算相连概率进行区分。

  > 特点是负样本生成方式，原来都是直接负采样的

### 自编码器方法

#### 复杂网络

+ [x] LINE: Large-scale Information Network Embedding：经典LINE，但其实没太弄明白二阶距离是怎么体现的，可以看作以随机生成作为生成器，共现概率作为解码器的自编码器网络

+ [x] **Deep neural network for learning graph representations**：首先用了随机冲浪模型进行采样然后计算PPMI矩阵，然后利用一个自编码器嵌入PPMI矩阵，得到嵌入向量

+ [x] **Adversarially regularized graph autoencoder for graph embedding**: 大部分图嵌入方法着重于保留拓扑结构或者最小化重建误差，但忽略了隐层向量的数据分布，导致较差的嵌入结果。提出了一个新的嵌入框架，将网络的拓扑结构和文本信息编码为紧凑表示，解码器用于重建网络结构。隐层表示通过对抗训练策略约束为遵循先验分布。对抗训练策略用于分辨隐层向量是来自于真实分布还是图编码器。通过两层的图卷积自编码器将网络邻接矩阵和特征矩阵嵌入到隐层空间中，然后利用解码器重构邻接矩阵。使用多层感知器作为判别器，通过最小化二值判别器的交叉熵损失，嵌入向量可以在训练过程中被正则化。

  > 能不能利用一下连接预测的学习方法来重构邻接矩阵，从而将对抗网络和自编码器网络结合在一起？对抗的精髓是要有一个先验的结果，通过对抗使嵌入结果更加贴合这个结果，提升编码器的部分
  >
  > 我感觉这个方法更重要的一个问题是，他要求嵌入结果都符合一个先验分布，但如果网络本来就不符合呢？这俩学习目标会不会冲突？
  
+ [x] **Deep variational network embedding in Wasserstein space**：可以理解为将对角为0的拉普拉斯矩阵作为输入，利用编码器学习节点表示，然后根据采样的三元组优化嵌入空间的一阶距离，并通过重构拉氏矩阵的方式优化二阶距离，因为嵌入的结果是一个高斯分布，所以两个距离都是用Wasserstein距离来衡量的

  > 在重构后的矩阵上才能用$||x-y||_2^2$的方式计算W距离，在分布上要用$||\mu_1-\mu_2||_2^2+||\varSigma_1^{1/2}-\varSigma_2^{1/2}||_F^2$的方式来计算

#### 社交网络

+ [x] **transnet_translation-based network representation learning for social relation extraction**：一部分社交网络的边上有多个标签，利用这些标签预测网络中的其他边上的标签，通过源节点-边-目标节点的距离计算的方式提高节点嵌入的效果，通过自编码器方式学习边的嵌入，并优化正节点-边-节点关系的嵌入向量的距离

  > 用了知识图谱的嵌入方式？

+ [x] **An End-to-End Framework for Learning Multiple Conditional Network Representations of Social Network**：根据不同的行为类别将节点表示进行区分，然后分别利用注意力机制进行学习，最后得到的结果分类衡量距离

  > 一个端到端的方法，但也可以用自编码器的方式进行无监督学习，端到端这里其实还没看懂

#### 属性网络

+ [x] **Variation autoencoder based network representation learning for classification**: 原有工作只考虑网络表示学习的部分方面，比如边结构，节点信息或者部分整合(?)，这篇工作将网络结构和节点文本特征结合起来。使用Content2vec模型为节点文本生成属性特征，然后和网络的邻接矩阵拼接在一起作为变分自编码器的输入统一训练。模型中利用全连接层做降维映射，最终得到四段嵌入向量，分别作为文本信息和结构信息分布的均值和方差。然后从先验分布（高斯分布）中采样两个值作为扰动，通过线性函数结合均值和方差得到文本和结构的嵌入向量，再将两者拼接得到编码层最终习得的向量。最终通过解码层对输入进行还原，最小化输入输出向量之间的交叉熵和文本结构向量的KL散度。

+ [x] **Accelerated Attributed Network Embedding**：用节点属性计算节点间的相似度，然后要求嵌入向量同时满足结构相似度和属性相似度，最终利用数学方式将学习方式进行分解，用分布式方式进行加速

  > 结构邻接矩阵和属性邻接矩阵

+ [x] **Co-embedding Attributed Networks**: 同时对结构和特征矩阵进行嵌入，利用全连接网络嵌入特征矩阵，利用图卷积网络嵌入邻接矩阵，双方均加入扰动项，然后分别重构特征矩阵和邻接矩阵

  > 加了重构节点是否拥有这个特征的信息的损失函数的图卷积自编码器

+ [x] **a variational autoencoder for directed acyclic graphs**: 有向无环图上的变分自编码器，根据结构顺序使用GRU学习节点表示，用GRU解码重构有向无环图，根据输入顺序度量重构每一步得到的节点类型和连边相似程度

  > 感觉在nas和强化学习中有用处？可以根据已有节点进行推理，预测将要使用的节点是什么样子的，可以缩小搜索范围，甚至启发生成新的节点/新的网络层？，nas好像已经有的贝叶斯网络结构学习方法，这个应该也结合了图？

### 图卷积网络

#### 属性网络

+ [x] **Large-Scale Learnable Graph Convolutional Networks**：和其他gcn不同的是在特征层面进行采样，从相邻节点中每个特征采样前k个最大的，然后直接利用cnn进行降维，最后将特征拼接后通过全联接层进行判断。

+ [x] **simple and deep graph convolutional networks**：解决图卷积网络的过平滑问题，过平滑问题由拉普拉斯矩阵的k阶近似的固定系数导致，会导致k层图卷积网络的输出逼近固定分布。通过引入残差网络和点对点映射的方式来解决，残差网络保证输出保留了一定比例的原始信号，点对点映射保证网络学习下限是原始信号。

  > 这种搞理论的不太懂，这好像也没有什么改进的地方了，好像只能试试往其他模型进行迁移，或者使用更加复杂的激活函数，比如像脉冲网络那种，可不可以避免过平滑，这个就是如何在数学上进行证明了，话说他那个证明好像没有考虑激活函数

+ [x] **convolutional kernel networks for graph structured data**: 在图结构上的图卷积核网络

  > 还不懂图核是什么，先看了综述再说吧
  
+ [x] **AM-GCN: Adaptive Multi-channel graph Convolutional Networks**：根据节点特征生成相似度邻接矩阵，然后利用一个GCN学一个表征，在原来的图结构上用一个GCN学一个表征，最后相似度邻接矩阵和图结构共享一个GCN学一个表征，最后利用注意力网络将这三个表征进行聚合得到最后的表征。

  > 结构图是考虑结构信息，特征图是考虑特征信息，中间共享的这个是考虑结构和属性的相关性，很好玩
  
+ [x] **Disentangled Graph Convolutional Networks**：希望最后的表示由k个独立的成分构成，分成分学习节点表示，然后计算节点对间不同成分的节点表示相似度来衡量节点间是根据这个成分相连的概率，然后根据概率更新成分表示

  > 这个学习是逐节点来的，根据每个节点来衡量其他节点是根据哪个成分相连的，所有节点的成分参数共享

+ [x] **GraphAIR: graph representation learning with neighborhood aggregation and interaction**：首先证明了邻居之间交互项的系数很小，导致gcn效果不好。然后提出了一个模型，用了类似多头注意力的方式在每一层独立的学两个聚合表征，然后元素乘作为节点之间的交互特征。然后类似残差网络，通过将传统的聚合特征和新的交互特征分别通过激活函数后相加的方式得到最后的输出特征

  > 这样可以解决它所提出的系数过小问题吗

+ [x] **Robust graph convolutional networks against adversarial attacks**：将节点嵌入为高斯分布，然后从其中抽样得到向量进行重构，抽样采用了注意力机制，损失函数加入了正则项使学到的分布贴近于正态分布。

+ [x] **Simple spectral graph convolution**: 去掉了图卷积层之间的激活函数，并利用了残差连接

+ [x] **Simplifying Graph Convolutional Networks**: 去掉了图卷积层之间的激活函数 

#### 社交网络

+ [x] **DeepInf: social influence prediction with Deep Learning**：首先获取每个节点的局部子图，然后对节点进行嵌入，与特征向量和动作及在子图中的身份等信息进行拼接，最后利用图卷积等网络进行判断

  > 这个拼接方式很有意思

### 注意力方法

#### 属性网络

+ [x] **graph attention networks**：经典GAT

+ [x] **graph neural networks with feature-wise linear modulation**：利用一个超网络来计算以边类型划分的节点之间的转移矩阵，从而把目标节点的信息也纳入考虑，计算目标节点在指定边类型下的权重并与wh进行元素乘，并在后将另一个同样方式计算出的权重作为偏置项加入。

  > 与GAT不同的是使用了转移矩阵和wh的元素乘而不是拼接
  
+ [x] **scalable and adaptive graph neural networks with self-label-enhanced training**：特征模型每一层在做完聚合后不加激活函数直接传往下一层，每层的输出最后分别经过一个MLP，然后使用注意力机制进行选择，最终再过一个MLP得到节点表示。标签模型每一层使用标签传播得到节点标签，然后将置信度高于阈值的节点标签也算作预设的，最终通过MLP得到标签嵌入后与节点表示相加得到最终的表示。

  > 这里标签的没搞懂，文中说了标签嵌入，是独热编码还是嵌入向量，向量的话怎么传播呢

#### 异构网络（知识图谱）

+ [x] **Heterogeneous graph attention networks**：使用元路径衡量相邻节点，对不同的元路径使用不同的映射得到不同的节点嵌入，然后以元路径为类别计算注意力系数，使用注意力机制聚合同一元路径内的相邻节点，并在得到的表示上使用注意力机制学习不同元路径的重要性，最终进行加权聚合得到节点最终的表示

  > 可以看做按元路径划分不同的邻接矩阵，不同点在于将节点嵌入对不同元路径做了嵌入，以及对不同元路径的权重进行了学习
  
+ [x] **Disentangle-based continual graph representation learning**：关注在动态知识图谱上的持续图表示学习问题。每个关系都被假设由K个独立的组件构成，使用一个注意力机制来学习对应的边是由对应的组件构成的概率，然后再使用最可能的n个组件来表示。这样就对每条边的成分做了限定，随后使用正常的知识图谱和图嵌入方式得到图表示，在下一批三元组到来时，还是先进行解耦，得到每个关系的前n个组件，然后只将新加入三元组的相邻节点中同一关系并且拥有同样组件的老三元组加入进来一块进行学习，从而完成高效的持续学习目的。

### Transformer

#### 属性网络

+ [ ] **A generation of transformer networks to graphs**：使用拉氏矩阵的特征向量来作为位置编码，并使用目标节点作为query，相邻节点作为key和value，并使用了batch归一化代替了层归一化，并在计算注意力权重的时候加入了边特征的影响

  > 这样可不可以得到一个新范式？
  
+ [ ] **Graph-bert: Only attention is needed for learning graph representations**：用随机游走计算节点相似度，然后采样前k个最可能的，用了随机嵌入，wl标签嵌入，相似度大小顺序嵌入，相似子图节点在原图中的距离嵌入，然后把四种嵌入相加输入bert，并结合残差机制计算新表示。最终可以在特征重构和结构重构任务上与训练，然后在聚类和分类任务上进行微调。

  > 后三种用的同一种嵌入方式，就是里面的自变量不同，是wl标签，相似度顺序索引和节点距离

  > 这种先通过不同方式生成对应信息的嵌入，然后再通过编码器进行编码的方式是否就已经足够捕捉网络中的信息了？那要这么说的话，通过同样的方式，然后直接使用MLP是不是也可以捕捉？这种方法可以保证转换不变性吗

### 自动机器学习

#### 复杂网络

+ [x] **AutoNE: Hyperparameter Optimization for Massive Network Embedding**：先通过随机游走获得若干个子图，然后分别在每个子图上分别进行试验。随后利用一个元学习器将子图上学到的结果扩散到整个网络上。算法分两步，首先获得子图，在子图上实验不同参数，并计算对应的奖励值，然后扩充已有样本（经验）库并更新核函数的参数，第二阶段选择前在最优解，在全图实验并记录奖励值，最后优化核函数。最终输出奖励值最大的超参数。

  > 元学习器使用一个核函数计算不同子图-超参数对的相似程度，预设相似的子图上相似的参数下网络嵌入算法的效果是差不多的。随后假设奖励函数的符合均值为0，方差为子图-超参数对相似度矩阵的高斯分布，最终针对给定的测试样本和观测到的奖励值，新的测试样本的奖励值符合高斯分布，并选择置信上限最高的参数。
  > 我看不懂，但我大受震撼（滑稽

### 其他神经网络方法

#### 属性网络

+ [x] **context-aware network embedding for relation modeling**：分别学习节点的结构嵌入和属性嵌入，在属性嵌入部分有注意力网络的雏形，针对不同的节点对学习不同的嵌入向量，最后将结构嵌入和属性嵌入做拼接作为最终的嵌入，在损失函数部分同时考虑结构间，属性间和结构-属性的相似程度

  > 但好像没说最后得到的嵌入向量是什么？就光得到了节点对的嵌入向量，那不成了一个N$\times$N$\times$d 的张量了？

+ [x] **on graph neural networks versus graph augmented mlps**：用了多个处理后的邻接矩阵得到下一层的嵌入，再把所有嵌入拼接后过网络得到最终的下一层嵌入。

  > 这个嵌入方法很诡异，感觉创新点在理论那，这不就是把图残差网络做了多层拼接么？

+ [x] **the surprising power of graph neural networks with random node initialization**：证明使用随机初始化的节点特征也可以学到通用的节点表示

+ [ ] **Deep loopy neural network model for graph structured data representation learning**：感觉大头在如何进行参数更新，模型就是同一层的网络不仅从前一层的神经元中获得特征，也从同一层的其他神经元中获得特征

  > 这里感觉无形中弱化了网络的结构信息，计算方式是很像全联接层的，感觉应该是在更新的时候通过生成树的方式来将网络的部分结构信息包括其中

+ [x] **Cognitive Graph for multi-hop reading comprehension at scale**：使用bert进行实体识别，然后将同一句中出现的实体间进行连接，构建关系网络，并在网络上利用GNN进行表示学习，最后利用一个评分函数确定答案实体

  > 严格来说不应该放在这里，属于gnn的应用

#### 异构网络

+ [x] **relation structure-aware heterogeneous information network embedding**：将网络中的关系划分为隶属关系(AR)和交互关系(IR)，然后使用不同的损失函数来衡量不同关系下节点的相似程度，最后两边一块优化

### 随机游走

+ [x] **Efficient Estimation of Word Representations in Vector Space**：经典word2vec

#### 复杂网络

+ [x] **deepwalk: online learning of social representations**：经典deepwalk，代码来看就是随机游走+word2vec。
+ [x] **fast network embedding enhancement via high order proximity approximation**：利用高阶结构的雏形模型，通过邻接矩阵相乘的方式得到高阶相邻关系，然后再分别计算节点嵌入和上下文关系嵌入。进一步优化了学到的嵌入结果

### 矩阵分解

#### 复杂网络

+ [x] **Billion-scale network embedding with iterative random projection**：从先验高斯分布中采样得到随机映射矩阵，然后根据随机映射(Gram Schmidt 过程)得到对角映射矩阵，再通过迭代式的和邻接矩阵相乘并最终将每次相乘结果累加的方式得到最终嵌入，方法可以通过分别计算每一行的方式进行并行学习，从而提升效果。也可以通过只计算新加入节点的方式进行动态更新。

#### 属性网络

+ [x] **Binarized Attributed Network Embedding**：根据WL图核合并特征和结构矩阵的到距离矩阵，然后通过矩阵分解的方式得到二值化的节点嵌入

  > 有时候真的很不懂，为什么要搞这些花的，搞二值化的节点嵌入为什么要先搞个距离矩阵出来，这两者有什么关系

### 对比学习

#### 属性网络

+ [x] **Deep graph infomax**: 利用对抗神经网络解决以采样为基础的网络表示学习方法对于随机游走目标的依赖，并可用于迁移式和归纳式学习。利用图卷积自编码器为每个节点学习一个patch表征。同时以最大化局部互信息为自编码器学习目的，利用一个readout function将patch表征聚合为图级别的表征。同时通过负采样从图中获取负样本，然后在鉴别网络部分分别衡量正负样本的patch表征属于图级别表征对的概率，最后以最小化这个概率为目标进行训练。

#### 异构网络

+ [x] **Heterogeneous deep graph infomax**： 根据不同的元路径随机游走的方式生成邻接矩阵并学习对应的表示，针对邻接矩阵进行负采样学到负样本的表示，然后用一个语义级别的将不同表示合并，利用readout的到正负图的整体表示，最后用判别器区分正负图。

## 预训练

（划分方式与其他不同，因为这个研究的对象是训练过程）

#### 重构式

+ [x] **generative pre-training of graph neural networks**：一个新的GNN预训练方法，按照给定的顺序输入节点，根据节点的结构嵌入预测属性，根据属性和已有结构预测剩余结构，然后优化根据给定的图预测下一个节点输入时的图的结构和属性的能力

  > 对于一个GNN模型有没有包含足够的信息的判定方式是能不能根据已有的结构和属性信息去预测下一个节点的结构和属性，这个借鉴了图生成思路

#### 对抗式

+ [x] **graph adversarial training: dynamically regularizing based on graph structure**：通过给网络中节点特征加入一个扰动破坏和周围节点的流畅性，生成器用于生成尽可能和原网络相似的扰动项，判别器用于分辨加了和没加扰动项的节点

## GNN训练

### 训练方法

+ [x] **Training graph neural networks with 1000 Layers**：提出了将特征划分不同通道并迭代式更新的可逆连接，共享参数的权重绑定，以及平衡态模型（equilibrium model）来改进GNN的内存和效率。

  > 这个平衡态模型不太理解，是如何保证可以找到固定点的呢。

## GNN表达能力

> 这里的分类还需要调整

### 其他算法等价

> 这种证明和其他方法等价的方式和证明NP难很像，那又有谁来证明等价的问题呢

#### WL算法

+ [x] **How powerful are graph neural networks**：提出在图分类问题上GNN的表达能力与WL测试相同，并说明只有聚合函数是单射的时候才能具有相同的性能，并据此提出了GIN网络，使用MLP来拟合每层的聚合函数。

+ [x] **weisfeiler and leman go neural-higher-order graph neural networks**：根据k-WL方法设计遵循k-WL方法的图神经网络，以k集为单位学习表示

  > 这个主要用来进行图分类的，k=1的时候就是普通GNN，感觉有点像图池化，但是在邻域的设计上不同，或者基于motif的学习？

#### LOCAL模型

+ [x] **What Graph neural network cannot learn-depth vs width**：证明GNN等价于LOCAL模型，并以此证明GNN的消息传递机制是图灵完备的，可以计算任何图灵可计算函数。并通过和加强版的CONGEST模型等价，证明在给定的深度和宽度下，GNN无法识别某些子图结构以及解决最小且等优化问题。通过说明GNN和LOCAL算法等价，证明只要满足1）有足够的层；2）层有足够的宽度；3）节点之间相互独立；4）每层的消息传递和特征更新函数是图灵完备的，GNN就是图灵普适的。并且证明了处理多个问题时宽度与深度乘积的下界

  > 这篇很硬

#### 关系逻辑模型

+ [ ] **Learning with molecules beyond graph neural networks**：

### 其他计算方式

#### 同构子图数

+ [x] **Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting**：根据预定的子图结构统计每个节点出现的同构子图，以及在其中的角色，并以数量作为节点的特征，然后利用消息传递网络进行学习，最终学到的表示有超越WL测试的能力。

## GNN可解释性

主要提供后验的解释，哪条边/节点/特征更加重要，哪种网络模式对指定类型的影响更大

### 个体级别的解释

研究根据输入确定的解释性，识别针对具体任务的图中的重要特征，根据对于重要性分数的计算方式进行具体分类

#### 基于梯度/特征的方法

使用梯度或者特征值来表示不同输入特征的重要性，基于梯度的方法计算目标预测对于输入特征的梯度，梯度越大说明重要性越强。基于特征值的方法将隐层特征映射回输入空间来衡量重要性。

+ [x] [Deep inside convolutional networks: Visualising image classification models and saliency maps](https://arxiv.org/abs/1312.6034.pdf)：固定学习到的网络参数，计算输出概率和输入特征之间的梯度，得到类别显著性映射，用来反映图中每个像素对于特定类别识别结果的重要性

+ [x] [Smoothgrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825.pdf)：基于梯度的方法，随机从高斯分布中采样多个值，然后加在原输入上，并求梯度的平均值作为输入的重要性，用于降低噪声影响

  > 其实没太明白怎么加随机采样然后再计算梯度，说是从输入的领域中随机采样，然后计算平均敏感性映射

+ [x] [Learning deep features for discriminative localization](http://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html)：研究均值池化层如何使CNN可以捕捉局部特征，生成一个类别活动映射来表示CNN用于识别特定类别的图像区域。因为最后卷积层经过全局池化再接一个全连接层接softmax来输出类别，所以全联接层的权重代表了特定卷积单元的重要性，可以推出在$\sum_kw_k^cf_k(x,y)$代表了图片位置$(x,y)$对预测结果$c$的重要性

+ [x] [Grad-cam: Visual explanations from deep networks via gradient-based localization](http://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html)：计算输出预测与特征图中每个元素的导数，并用全局均值作为系数，在与特征图进行加权后使用relu函数得到特征图中每个元素对于分类结果的影响程度。

+ [x] [Explainability techniques for graph convolutional networks](https://arxiv.org/abs/1905.13686)：提出了基于梯度和基于分解的方法。SA直接使用梯度的开方来衡量输入特征的重要性，Guided BP之反向传播正梯度并将负梯度归零。LRP将输出信号分解为输入特征的组合，并将组合权重作为重要性分数。

+ [x] [Explainability methods for graph convolutional neural networks](http://openaccess.thecvf.com/content_CVPR_2019/html/Pope_Explainability_Methods_for_Graph_Convolutional_Neural_Networks_CVPR_2019_paper.html)：将CAM和Grad-CAM引入了图卷积网络，计算的东西变成了每列特征值，最终得到节点每个特征对于指定分类的重要程度，进而得到每个节点的重要程度。

#### 基于转换的方法

通过观察打乱输入后输出结果的变化来判断重要性，如果重要的特征没动的话最终结果应当相似。用一个遮罩层来遮盖重要特征，生成一个新图，然后将新图也送入同一GNN，根据最终得到的结果更新遮罩层。

> 这个适用于大型网络吗，遮罩层存的进内存么

+ [x] [**Real time image saliency for black box classifiers**](https://arxiv.org/abs/1705.07857)：后面没有全看懂，但是方法是初始化一个遮罩层，然后遮罩掉一部分输入特征，并和另一个图的反向遮罩结果相加，最终通过优化遮罩层得到对于特征重要性的解释。被遮掉的就是不重要的特征。

+ [ ] [**Interpreting image classifiers by generating discrete masks**](https://ieeexplore.ieee.org/abstract/document/9214476/)：采用GAN结构，利用要解释的GN N做判别器，生成器用来得到遮罩层，使用一个概率图来采样得到离散遮罩层，然后得到输入图，用GNN学习输出判断结果。使用policy gradient来更新生成器，最终生成器得到的就是可以保留重要信息的遮罩层，可以过滤出重要的输入特征

  > 概率图采样得到遮罩层的方法忘了

+ [x] **learning to explain: an information-theoretic perspective on model interpretation**：使用Gumbel-softmax来生成遮罩层，从而使遮罩层变化可以被学习，然后使用一个神经网络来拟合与标签的条件分布，并使其结果和原特征的结果尽可能相似。

+ [x] [**GnnExplainer: Generating explanations for graph neural networks**](https://www.ncbi.nlm.nih.gov/pmc/articles/pmc7138248/)：使用遮罩层遮掉一部分边和节点特征，然后使学到的结果尽可能和原始的结果相近，结构遮罩层直接学习，特征遮罩层使用重参数方法进行梯度更新

#### 基于分解的方法

通过将任务指定的指标逐层反向分解，得到特征的重要性分数

#### 基于替代的方法

首先在样本处采样相邻节点，然后在采样的图中使用一个简单的可解释模型来解释原本的预测

### 模型级别的解释

研究与特定输入无关的对于GNN的解释。解释图模式对对应类的重要程度

> 可以理解为这个GNN对于这个类的判断方式就是这个图模式

#### 递减方式

+ [x] **Generative Casual expalantions for graph neural networks**:使用一个模型来生成遮罩层，然后再利用遮罩层遮掉一部分边得到要解释的GNN的损失，最终优化这个损失，通过遮罩层过滤出的网络就是对分类结果贡献最大的网络。

+ [x] **Parameterized explainer for graph neural networks**：用一个MLP来判断图中每条边的权重，然后去掉重要性比较低的边，再把剩下的图输入待解释的GNN，并使预测结果尽可能接近输入原始图时的结果。在训练好后剩下的MLP权重就可以解释GNN到底偏好哪类结构。对于一个待解释的图，只要输入边嵌入，就可以得到在对应的类中待解释的GNN会关注哪些边

+ [ ] **PGM-Explainer_probabilistic graphical model explanations for graph neural networks**：首先使用扰动特征的方式得到一堆采样子图，在对采样子图进行筛选降低分析数量后，使用剩余的采样子图得到最终的解释贝叶斯网络，解释模型判断指定节点在特定motif中的角色的原因

  > 后面的概率部分没太细看，要研究的话需要细看看懂

#### 递增方式

+ [x] **XGNN: Towards model-level explanations of graph neural networks**：使用一个生成器，每次在图中增加一条边，首先使用一个图卷机聚合节点特征，然后使用一个MLP来预测节点是新加边的起始节点的概率，然后再用一个MLP预测终止节点的概率，并利用图分类的预测结果作为reward，根据奖励更新生成器参数。最终生成得到的图结构就是对得到对应分类标签作用最大的子图结构

## 图结构学习

### 综述

+ [x] **deep graph structure learning for robust representations_a survey**: 图结构学习综述，不但学习节点表示，同时学习优化的图结构，包括学习图结构的度量函数，调整图结构的权重，以及学习图结构的生成模型三种方式。

### 调整结构权重

#### 复杂网络

+ [x] **Network enhancement as general method to denoise weighted biological networks**: 去除网络中包含噪声的边，加强清晰的边，利用边权重占所有相邻边的权重的占比来衡量边是不是噪声，并将这个权重传播出去。

#### 属性网络

+ [x] **cluster-aware graph neural networks for unsupervised graph representation learning**：利用GCN学习节点表示，然后利用kmeans进行聚类，同时利用MLP进行分类，优化分类和聚类结果的交叉熵，同时根据kmeans聚类结果，将同属同一社团概率过小的节点之间的边去除，再为属于同一社团概率够高的节点之间添加边。

+ [x] **unifying graph convolutional neural networks and label propagation**: 将网络结构权重也作为可学习变量，先学习优化标签传播结果的结构权重，再根据结构权重学习节点表示，两者联合优化

  > 总感觉这里的方式其实和GAT很像，都是自适应地学习每条边的权重，这个表现更好是因为调整方式的感受范围更大吗？

## 链接预测

### 图神经网络

#### 属性网络

+ [x] <span id="imc-gnn">**Indctive matrix completion based on graph neural networks**</span> : 首先根据购买矩阵从用户-物品二项网络中提取用户-物品对为基础的$1$跳子网络，然后用gnn学习节点表示，拼接用户-物品表示得到子图表示，最后利用多层感知机对边权重进行回归学习。
+ [x] **link prediction based on graph neural networks**：很像[用gnn补全用户-物品矩阵那个研究](#imc-gnn)，采样获得连边节点附近的图，然后给每个节点标签，形成节点信息矩阵，最后利用gnn学习表示，并进行连接预测（文章中用了dgcnn，直接作为图分类问题进行预测了）

#### 时空社交网络

+ [x] **Social Link inference via multi-view matching network from patio-temporal trajectories**：利用余弦相似度和最大池化计算位置序列的相似性，并通过拼接得到位置匹配结果，利用lstm以及元素乘计算时间序列之间的相似性，利用gat计算社交关系相似性，并将三个模块的匹配向量拼接计算两者之间存在连边的概率，最终联合优化预测损失和lstm学习损失

  > 在这个基础上可以做些什么呢

## 事实验证

### 图注意力网络

#### 属性网络

+ [x] **fine-grained verification with kernel graph attention network**: 用了一个核注意力机制来挖掘节点信息，然后对证据图上的节点进行分析，对于给定的序列来判断最终的事实是否成立

  > 没太看懂，不太懂所谓的证据图（evidence graph是什么，应该怎么理解）

+ [x] **graph-based evidence aggregating and reasoning for fact verification**

  > 又是一篇，对于所描述的图结构和定义的问题都不是很明白

## 名称消岐

+ [x] **On graph-based disambiguation**：区分同一个姓名的不同人。首先根据两人的合著关系构建网络，边权是两人合住的论文集合，然后根据两个节点之间不同路径的数量计算两个节点之间的相似程度，最后根据相似程度使用聚类方法进行聚类，每个类中要区分的名字只有一个。

## 图攻击

### 对抗学习

#### 属性网络

+ [x] **Adversarial attacks on neural networks for graph data**： 关注在二值属性网络上的节点分类问题，设计一个对抗模型，希望找到一个改变幅度低于一定的阈值的混淆网络，通过更改有限节点的特征和结构，使卷积网络对目标节点的分类结果与在原网络中的结果“距离”尽可能远。利用混淆网络和原网络的节点度数的概率分布，使用统计双样本测试来衡量混淆网络和原网络是否来自同一分布，对原网络的更改是否过于明显。在特征网络上利用随机游走，以从源特征节点到目标特征节点的随机游走概率来衡量两个特征共同出现是否是过于明显。在原图上计算出替代模型，然后在改变过于明显之前，依次从可攻击节点中选择进行结构和属性的混淆，并分别计算混淆和学习之后对目标结点的分类结果的“距离”，然后选择对距离改变最明显的混淆方式生成新图，最后返回改变最有效的混淆图。

## 图分类

### 图神经网络

#### 属性网络

+ [x] **An End-to-End deep leraning architecture for graph classification**: 多层图卷积学到节点表示，再把多层学习结果横向拼接，然后先在行内排序再从最后一列向前字典排序节点，选择前k个节点，不够的用全零向量补齐，最后使用1-d卷积层和Max pooling层和全连接层获得图分类结果

  > 后半截非常神仙，为什么学习到的表示经过这样排序加卷积就可以得到很好的结果，首先是为什么就可以这样排序。

+ [x] **pooling regularized graph neural network for fmri biomarker analysis**：图卷积$+$​ topK池化进行图分类（对脑成像进行分类），设计了多种损失函数，衡量前k个节点和后面节点之间的相似程度，要求前k个评分尽可能相似且尽量大，后面评分尽可能相似且尽量小，同一社团内评分尽可能相似。

  > 总感觉脑成像这里的社团发现问题和传统社团发现问题不同，这里主要考量的好像是不同社团间的交互强度

## 图相似度计算

### 图神经网络

#### 属性网络

+ [x] **SimGNN_A Neural Network Approach to Fast Graph Similarity Computation**：在利用GCN得到图的节点表示之后分别对两个图上节点间的和整个图的嵌入向量进行比较，最终将两个相似度向量拼接后经过全连接网络得到两个图的相似程度。

  > 这个会不会改成无监督的，比如改成对抗学习或者那种优化互信息的方式
  > 怎么得到整个图的嵌入的还没看，两个图的嵌入的相似程度计算方法像是用了个bilinear矩阵，然后结合了拼接的全连接层

+ [x] **graph optimal transport fot cross-domain alignment**：与上一个结构类似，根据相似性构建网络，然后对节点之间和节点对之间的相似性进行计算得到相似性cost，再根据wd算法得到转移矩阵，最后利用转移矩阵和相似性cost矩阵计算匹配程度，作为损失函数的一部分强调匹配程度。

  > 后面的分布那里没有太看懂，和全图的比较好像又不太一样，粒度更细

## 其他

+ [x] **Exploring Network Structure-Dynamics-and Function using Networkx**：networkx使用说明书

## 子图匹配

+ [ ] **privacy preserving subgraph matching on large graphs in cloud**：异构属性网络上的子图匹配问题，同时考虑了隐私保护要求，首先构造了一个自同构图，即整个网络可以被分为k个互为同构图的块，在给定了一个查询图后，返回在网络中匹配到的图。

  > 还没有看完